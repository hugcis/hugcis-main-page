[{"body":"Edit 2022-04-20: I did not see it right away, but there was a significant spike in the logs due to this post hitting the front page of Hacker News! I\u0026rsquo;m glad more people are interested in this. See the discussion here\nSome people expressed interest in my org roam notes workflow. Setting this up has been an interesting project, teaching me about Emacs, org-mode and a little about web frontend. This post will describe my current workflow in as much details as possible.\nHopefully, you\u0026rsquo;ll learn to create an automated build script to publish your notes on the web and convert your org-roam note network into a graph like the one below.\nNote graph screenshot For the interactive version, see my notes.\nThe notes For the past years I have been trying to get into the habit of writing down new pieces of knowledge into notes. I am committed to this idea of keeping track of what I read and learn about. It can also be nice to come back to some of these notes to remind myself about a subject or make connections with something I learned.\nWhy org-roam? When searching for the right way to organize and keep track of these notes, I ended up hearing about org-roam. I went through this great blog post by Jethro Kuan which details why he started org-roam.\nThe \u0026ldquo;roam\u0026rdquo; in org-roam refers to Roam Research, a standalone note-taking tool that people seem to praise. The tool is closed source for now and not cheap, making some people seek free alternatives. Org-roam seems like a great alternative, but not for everybody. Being an org-mode based tool makes it ideal for Emacs users, but may involve a steep learning curve for other users. If you\u0026rsquo;ve never heard about org-mode, you may want to read this introduction. The tool is incredibly useful, but it takes time to get used to it.\nI haven\u0026rsquo;t thoroughly researched other alternatives, but Athens seems to fill the gap: it\u0026rsquo;s open-source, it looks user-friendly and not tied to an editor.\nPersonally, I prefer to stick with org-roam for three main reasons:\nI am already a heavy user of Emacs and org-mode and a lot of features and Emacs tools integrate well with org-roam I can export the files to my site (I detail how in Publishing) The source files are plain text (with org-mode markup). This means that even if Emacs and org-mode and all related software were to disappear, or more probably, if I decide to manage my notes with something else, I can still export all the content and translate it into whichever format I want If you are interested in Emacs and would like to see my setup, I publish my dotfiles. You can also read the detailed and commented version of my org-mode configuration, or jump directly to the org-roam configuration.\nOrganizing knowledge in org-roam The goal of org-roam is to create a web of notes linked between each other. By linking notes in a non-hierarchical way, one can make connections between topics and remember them without going through the hassle of creating a hierarchical tag system where it\u0026rsquo;s never clear if a note should belong to tag \u0026ldquo;A\u0026rdquo; or its parent \u0026ldquo;B\u0026rdquo;, etc.\nThere are plenty of different techniques for taking notes with a tool like this, but I don\u0026rsquo;t follow any particular one.\nThis is what an org-roam note looks like as raw source (Emacs makes this much prettier in practice):\n:PROPERTIES: :ID: 9fd080fa-a9a1-4d85-a6bf-682b41a67b72 :ROAM_ALIASES: \u0026#34;ML\u0026#34; :END: #+TITLE: Machine learning - tags :: [[id:1b23dc1d-c276-4f95-866a-ad8e37dd544f][Artificial Intelligence]] Machine learning is about constructing algorithms that can approximate complex functions from observations of input/output pairs. Machine learning is related to [[id:ce8b2ebe-f42e-4e4b-b0af-2a4563ef9fc0][Statistics]] since its goal is to make predictions based on data. Examples of such functions include: - [[id:33ebc467-8c6c-43bc-b8b4-ba1971c75a37][Image classification]] - Time-series prediction - [[id:e72f23c2-d3a7-47df-87a6-5de09f820ecf][Language modeling]] The first 5 lines are a header containing the title of the note, an alias (an alternative name I can use to search for the note within org-roam) and an ID (automatically generated with org-id).\nThe rest is org-mode markup with some links to other notes (they also have unique IDs).\nPublishing Sharing knowledge as much as possible is essential. I also think that it\u0026rsquo;s even better to share the learning process and the steps to acquiring that knowledge because this is what will help your brain make the right connections and remember what you wrote.\nMy notes are a constant work in progress. I will change them, enrich them or delete them as I learn. Let this be a disclaimer too: I might write notes that are not true or the sole result of my uninformed opinion.\nOther people might have something to say about these thoughts and some have contacted me to discuss it. Interesting discussions have started from these notes.\nExporting the notes The first step to publishing is to export the org-roam notes to the language of the web: HTML.\nSimple HTML The simplest way is to use org\u0026rsquo;s embedded HTML export function (for example you can call M-x org-html-export-to-html when the note is open). For the note above, this will give you this simple HTML result:\nNote exported to plain HTML Plain HTML + styling is more than enough to get a decent site with all your notes.\nHugo I use a static site generator called Hugo for my website, so I chose to go down the slightly more complicated path of exporting the notes to Hugo\u0026rsquo;s Markdown format and using Hugo to generate the final HTML. This allows for more flexibility and the possibility to automatically and generate complex pages like this one.\nIf you have ox-hugo installed, exporting a note is as simple as calling M-x org-hugo-export-wim-to-md on your note. You should read the ox-hugo documentation to configure your site source path and other useful parameters.\nI automated the process with a few Elisp functions that I put in a Gist. The last function loops through a file containing a note path on each line. For each line, it exports the note and deletes the line from the file.\nThe graph A great advantage of org-roam is that it stores all its useful information in a SQLite database. Don\u0026rsquo;t worry if you deleted it! Org-roam can rebuild it from the notes and all the links inside them. The database caches a lot of useful data but everything is still in the notes.\nThe database has a table for the nodes and a table for the links, so there isn\u0026rsquo;t much work to do to get a graph!\nRead the org-roam DB My org-roam database is at ~/.emacs.d/org-roam.db. Here is the code that extracts all the information to build my note graph from the DB with 2 SQL queries and constructs a Python graph with the networkx library.\nSQL experts might scream at this.. these queries were the first that came to mind but there might be a better way of doing this! I\u0026rsquo;m dealing with small databases so performance isn\u0026rsquo;t a problem.\nimport networkx as nx import pathlib import sqlite3 def to_rellink(inp: str) -\u0026gt; str: return pathlib.Path(inp).stem def build_graph() -\u0026gt; nx.Digraph: \u0026#34;\u0026#34;\u0026#34;Build a graph from the org-roam database.\u0026#34;\u0026#34;\u0026#34; graph = nx.DiGraph() home = pathlib.Path.home() conn = sqlite3.connect(home / \u0026#34;.emacs.d\u0026#34; / \u0026#34;org-roam.db\u0026#34;) # Query all nodes first nodes = conn.execute(\u0026#34;SELECT file, id, title FROM nodes WHERE level = 0;\u0026#34;) # A double JOIN to get all nodes that are connected by a link links = conn.execute(\u0026#34;SELECT n1.id, nodes.id FROM ((nodes AS n1) \u0026#34; \u0026#34;JOIN links ON n1.id = links.source) \u0026#34; \u0026#34;JOIN (nodes AS n2) ON links.dest = nodes.id \u0026#34; \u0026#34;WHERE links.type = \u0026#39;\\\u0026#34;id\\\u0026#34;\u0026#39;;\u0026#34;) # Populate the graph graph.add_nodes_from((n[1], { \u0026#34;label\u0026#34;: n[2].strip(\u0026#34;\\\u0026#34;\u0026#34;), \u0026#34;tooltip\u0026#34;: n[2].strip(\u0026#34;\\\u0026#34;\u0026#34;), \u0026#34;lnk\u0026#34;: to_rellink(n[0]).lower(), \u0026#34;id\u0026#34;: n[1].strip(\u0026#34;\\\u0026#34;\u0026#34;) }) for n in nodes) graph.add_edges_from(n for n in links if n[0] in graph.nodes and n[1] in graph.nodes) conn.close() return graph Some graph analysis This second script starts by constructing the graph with the function above. Then, it applies 3 graph algorithms:\nCompute the PageRank score for each vertex and store it as the centrality attribute. This will be used to size the nodes Do a clustering of the nodes into N_COM clusters and store the community in the vertex attribute communityLabel Add N_MISSING \u0026ldquo;predicted\u0026rdquo; links to enable discovery of new hidden connections between nodes. This uses networkx\u0026rsquo;s implementation of Soundarajan and Hopcroft\u0026rsquo;s method for link prediction import itertools import json import sys import networkx as nx import networkx.algorithms.link_analysis.pagerank_alg as pag import networkx.algorithms.community as com from networkx.drawing.nx_pydot import read_dot from networkx.readwrite import json_graph from build_graph_from_org_roam_db import build_graph N_COM = 7 # Desired number of communities N_MISSING = 20 # Number of predicted missing links MAX_NODES = 200 # Number of nodes in the final graph def compute_centrality(dot_graph: nx.DiGraph) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Add a `centrality` attribute to each node with its PageRank score. \u0026#34;\u0026#34;\u0026#34; simp_graph = nx.Graph(dot_graph) central = pag.pagerank(simp_graph) min_cent = min(central.values()) central = {i: central[i] - min_cent for i in central} max_cent = max(central.values()) central = {i: central[i] / max_cent for i in central} nx.set_node_attributes(dot_graph, central, \u0026#34;centrality\u0026#34;) sorted_cent = sorted(dot_graph, key=lambda x: dot_graph.nodes[x][\u0026#34;centrality\u0026#34;]) for n in sorted_cent[:-MAX_NODES]: dot_graph.remove_node(n) def compute_communities(dot_graph: nx.DiGraph, n_com: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Add a `communityLabel` attribute to each node according to their computed community. \u0026#34;\u0026#34;\u0026#34; simp_graph = nx.Graph(dot_graph) communities = com.girvan_newman(simp_graph) labels = [tuple(sorted(c) for c in unities) for unities in itertools.islice(communities, n_com - 1, n_com)][0] label_dict = {l_key: i for i in range(len(labels)) for l_key in labels[i]} nx.set_node_attributes(dot_graph, label_dict, \u0026#34;communityLabel\u0026#34;) def add_missing_links(dot_graph: nx.DiGraph, n_missing: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Add some missing links to the graph by using top ranking inexisting links by ressource allocation index. \u0026#34;\u0026#34;\u0026#34; simp_graph = nx.Graph(dot_graph) preds = nx.ra_index_soundarajan_hopcroft(simp_graph, community=\u0026#34;communityLabel\u0026#34;) new = sorted(preds, key=lambda x: -x[2])[:n_missing] for link in new: sys.stderr.write(f\u0026#34;Predicted edge {link[0]} {link[1]}\\n\u0026#34;) dot_graph.add_edge(link[0], link[1], predicted=link[2]) if __name__ == \u0026#34;__main__\u0026#34;: sys.stderr.write(\u0026#34;Reading graph...\u0026#34;) DOT_GRAPH = build_graph() compute_centrality(DOT_GRAPH) compute_communities(DOT_GRAPH, N_COM) add_missing_links(DOT_GRAPH, N_MISSING) sys.stderr.write(\u0026#34;Done\\n\u0026#34;) JS_GRAPH = json_graph.node_link_data(DOT_GRAPH) sys.stdout.write(json.dumps(JS_GRAPH)) The script writes the graph in JSON format to stdout with networkx\u0026rsquo;s JSON export function. It looks something like this (you can find the actual graph represented on my notes page here):\n{ \u0026#34;directed\u0026#34; : true, \u0026#34;graph\u0026#34; : {}, \u0026#34;links\u0026#34; : [ { \u0026#34;source\u0026#34; : \u0026#34;012f1087-bae9-4bbf-b468-114169782b5b\u0026#34;, \u0026#34;target\u0026#34; : \u0026#34;2fc873e6-d61d-43e3-9dd1-635f54b4114f\u0026#34;, \u0026#34;predicted\u0026#34; : 0.676190476190476, }, { \u0026#34;source\u0026#34; : \u0026#34;012f1087-bae9-4bbf-b468-114169782b5b\u0026#34;, \u0026#34;target\u0026#34; : \u0026#34;89569187-fcdc-4ebf-8608-75bfb129e8c2\u0026#34; }, ... ], \u0026#34;nodes\u0026#34;: [ { \u0026#34;centrality\u0026#34; : 0.0848734602948687, \u0026#34;communityLabel\u0026#34; : 0, \u0026#34;id\u0026#34; : \u0026#34;012f1087-bae9-4bbf-b468-114169782b5b\u0026#34;, \u0026#34;label\u0026#34; : \u0026#34;CPPN\u0026#34;, \u0026#34;lnk\u0026#34; : \u0026#34;cppn\u0026#34;, \u0026#34;tooltip\u0026#34; : \u0026#34;CPPN\u0026#34; }, { \u0026#34;centrality\u0026#34; : 0.0593788434729073, \u0026#34;communityLabel\u0026#34; : 0, \u0026#34;id\u0026#34; : \u0026#34;b495ae71-9ed6-4ef1-8917-45b490bca56f\u0026#34;, \u0026#34;label\u0026#34; : \u0026#34;Neural tangent kernel\u0026#34;, \u0026#34;lnk\u0026#34; : \u0026#34;neural_tangent_kernel\u0026#34;, \u0026#34;tooltip\u0026#34; : \u0026#34;Neural tangent kernel\u0026#34; }, ... ] } Some frontend with D3.js You will find the JavaScript code for handling the D3 graph construction here. To create the graph I started from this D3 Observable tutorial on making force-directed graph. Getting something decent-looking is straightforward with D3.\nThe main SVG source where the graph is drawn is defined in the \u0026lt;div\u0026gt; with id main-graph here. There are two components in there: a filter definition and the main rectangle where the graph is drawn.\nThe filter is shown below. I use it for the tooltips when you point a vertex with your mouse. The tooltips are composed of two elements: the text and a background, that is a rectangle bounding box around the text with the text cut out (it\u0026rsquo;s what the filter does).\n\u0026lt;defs\u0026gt; \u0026lt;filter x=\u0026#34;0\u0026#34; y=\u0026#34;0\u0026#34; width=\u0026#34;1\u0026#34; height=\u0026#34;1\u0026#34; id=\u0026#34;solid\u0026#34;\u0026gt; \u0026lt;feFlood flood-color=\u0026#34;#f7f7f7\u0026#34; flood-opacity=\u0026#34;0.9\u0026#34;/\u0026gt; \u0026lt;feComposite in=\u0026#34;SourceGraphic\u0026#34; operator=\u0026#34;xor\u0026#34; /\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/defs\u0026gt; Conclusion That\u0026rsquo;s all I have for you. I realize this might look like a complicated bunch of scripts, but this setup works well for me now. Jethro Kuans\u0026rsquo;s dotfiles and Braindump source were a great source of inspiration. It notably helped for the Elisp part for which I have less experience.\nGetting into the habit of taking notes has been a great experience for me. I no longer fear forgetting ideas or useful thoughts as long as I write them in my \u0026ldquo;second brain\u0026rdquo;.\nI hope this post was informative, and it will help you get a nice note website up and running! Enjoy!\n","title":"My Org Roam Notes Workflow","url":"https://hugocisneros.com/blog/my-org-roam-notes-workflow/"},{"body":"This post describes my submission to the Minecraft Open-endedness challenge 2021. The submission was evaluated with the video below and the code here. This post will detail the methods and tools used for the submission and give hints for future work.\nIf you want a quick description of the work, have a look at the video:\nIntroduction Motivation My current research deals with evolution in complex systems and specifically cellular automata (CA). I want to see if we can use such systems to construct open-ended evolving simulations that create an endless stream of ever more complex structures and patterns.\nMinecraft being a grid world, it has a similar structure to some common flavors of CA. I was excited to see if I could port some of these ideas from the world of CA to Minecraft, where a player can interact with the structures in real time and change their evolution.\nThe ideal goal would be to create a CA that behaves in an open-ended way. Such CA would create new and progressively more complex structures autonomously. By mapping the states of such CA to block types in Minecraft, we could watch these structures emerge in-game. A feedback loop could be naturally added where a user\u0026rsquo;s actions are mapped back to the CA, enabling a form of \u0026ldquo;controlled\u0026rdquo; evolution.\nNeural cellular automata Such CA doesn\u0026rsquo;t exist yet, and our main hope for the time being is to approach this behavior on a reduced domain. Neural cellular automata, introduced in [1], can be trained with gradient descent to grow a fixed pattern stable under small perturbations from a small fixed seed.\nExample patterns learned by NCA, created with Mordvintsev et al's colab notebook In each of the examples above, a separate CA was trained on a pattern starting from a single black pixel. We extend this by training a single NCA on multiple patterns from different seeds. Instead of a single black cell, a small number of cells are arranged into simple shapes.\nExample seed patterns NCA have a lot of parameters (low compared to a regular neural network but still high compared to the information that is encoded in the system). This allowed us to learn more than one pattern (or larger patterns) without adding parameters in the architecture. The resulting patterns are of slightly lower quality but still stable under perturbations and able to grow.\nA NCA trained on 4 animal patterns with different seeds We turned this idea into a \u0026ldquo;game\u0026rdquo; by allowing user interaction with the evolving CA through Minecraft. Our system is not open-ended by itself, but needs human interaction to play with the shapes and seed patterns. This is in the spirit of open-ended games like Picbreeder [2] where players choose which generated patterns to mutate or evolve from. The algorithms can generate endless novelty but our human eyes provide the missing step to make it open-ended.\nPlayers can manipulate seeds in the game to create progressively more complex creatures out of the ones learned by the NCA.\nHybrids created on a single NCA by recombining parts of the initial seeds Evaluation We try to address the evaluation criteria from the competition\u0026rsquo;s website:\nSubmission will be evaluated by a panel of independent researchers based on -but not restricted to- the following criteria:\nDivergence: Open-ended algorithms are not expected to slow down or converge but rather keep expanding and generating more complex outputs over time.\nDiversity: Does the algorithm produce entities with strong phenotypic diversity? [\u0026hellip;]\nComplexity: Can the algorithm produce complex entities or entities interactions that give rise to complex systems? Are hierarchical and modular structures present?\nEcological interactions: Do the created entities interact with each other? [\u0026hellip;]\nLife-Like properties: Inspiration may be taken from other attributes of living systems [\u0026hellip;]\nDivergence: seeds can be combined as long as one wants. By splitting the seeds and using arbitrarily large canvasses, the possibilities are infinite. Diversity: The diversity is based on the diversity of patterns the NCA was trained on originally. From this, the algorithm produces combinations of the original patterns. Complexity: The produced hybrids are arguably modular and hierarchical. Ecological interactions: Different pieces of the evolved seeds may interact with each other, giving rise to surprising and unexpected structures, but this is arguably not what the organizers meant with this item. There are no proper ecological interactions in our system Life-like properties: Mutations and crossovers that we observe can be related to some living systems. Methods Modular seeds The patterns learned by NCA may have different spatial symmetries and structures. By using seed patterns that take advantage of these structures we can help making the hybrids modular.\nSeed displayed on top of the target pattern. One part of the seed should code for the body and the other for the head In the example above, the NCA learns to grow the pattern from this \u0026ldquo;double\u0026rdquo; seed: a head-seed and a body-seed. Because the fully trained NCA know how to repair a damaged pattern, it can actually grow the full pattern from each of these two sub-seeds. When two or more patterns are learned, we can swap sub-seeds to create hybrids.\nHybrid giraffe-gorilla obtained by using the head-seed of a giraffe and the body-seed of a gorilla Genome seeds We also try another way of encoding information about the patterns in a compact and modular form: genome seeds.\nGenome seeds: unique patterns of cells Future work Seed engineering Compression in NCA NCA trained on multiple patterns implement some form of data compression. Information about each pixel of the target images as well as instructions to grow the pattern from any position is encoded in the NCA parameters in some form. A natural follow-up question would be to look at the storage capacity of these NCA and if their low number of parameters could make it an interesting robust and error-correcting compression device.\n","title":"Open-ended creation of hybrid creatures with Neural Cellular Automata","url":"https://hugocisneros.com/blog/open-ended-creation-of-hybrid-creatures-with-neural-cellular-automata/"},{"body":"This post was largely inspired by Gabriel Peyré and Marco Cuturi\u0026rsquo;s excellent book about Computational Optimal Transport, which is free, (arXiv link, ref: [1]).\nA simple problem? Let\u0026rsquo;s start at the beginning: what is Optimal transport (OT)?\nIt all begins with Gaspard Monge, reading his mémoire [2] in front of eminent scientists and engineers of the time \u0026mdash; including famous Enlightenment philosopher Condorcet \u0026mdash; at the French Académie Royale des Sciences in 1776.\nThe memoir is in french, and is probably not relevant to someone interested about modern optimal transport. However, Monge\u0026rsquo;s problem could be explained in simple terms as follows:\nGiven two excavations $D_1$ and $D_2$, a cost of moving a unit of mass of dirt from any point of $D_1$ to any point of $D_2$,\nWhat is the cheapest way to move mass from $D_1$ to $D_2$ ? The answer is far from simple it turns out. Monge didn\u0026rsquo;t solve it in his mémoire, although he did lay the foundations of modern optimal transport theory.\nIs this the best (most efficient) way to move the blue squares from a mound to the other? Probably not: there are 39,916,800 possible ways to do it. Let\u0026rsquo;s move to the problem.\nDiscrete Optimal Transport The easiest way to think of the problem is in terms of atomic items.\nAssignment problem In its simplest form it can be viewed as an assignment problem: among all configurations, which one is the best? This is already quite restrictive, because we can only work with two sets of the same total size. However, this simple version is incredibly hard to solve.\nEach set (the above \u0026ldquo;excavations\u0026rdquo;) can be represented as a histogram (or vector) $\\mathbf{a}$ that belongs to the probability simplex \u0026mdash; the vectors which components sum to 1:\n\\[ \\mathbf{a} \\in \\left\\{ x = (x_1, \u0026hellip;, x_N) \\in \\mathbb{R}^N : \\sum_{i=1}^N x_i = 1 \\right\\} \\]\nIf we write $\\mathbf{C}_{i,j} $ the cost of moving an element from $i$ to $j$, the quantity we want to minimize is $\\sum_{i} \\mathbf{C}_{i,\\sigma(i)}$ , where $\\sigma$ is a permutation of the set $\\{1, \u0026hellip;, N\\}$. This permutation represents an assignment of bin $i$ of the first histogram to output positions $j$ in the second histogram.\nIn this form, optimal transport is fundamentally combinatorial, and might be summarized like so:\nHow can we assign every element $i$ of $\\{1, \u0026hellip;, N\\}$ to elements $j$ of $\\{1, \u0026hellip;, N\\}$ so as to minimize the cumulative cost of this assignment?\nThe result of this search is called the optimal assignment. As you may already know, there are exactly $N!$ possible solutions to this problem, which makes it very difficult to solve when $N$ grows large: e.g, with a histogram of size 20, there are already $2.43\\times10^{18}$ possible solutions.\nIt is interesting to note that this assignment is not unique, as shown in the example below where 2 elements are mapped to 2 others that form together the four corners of a 2D square with sides of size 1.\nNon-unique assignment. The other solution is dashed. Working with asymmetric histograms: the Monge Problem Using two equally sized histograms is a very strong limitation. By extending the above definition to a slightly larger family of histograms, we obtain the Monge problem. In this problem, several points $x_i$ can map to the same $y_j$ and their weights are summed.\nIn that case, the mapping between inputs and outputs is no longer a permutation, but a surjective map $T$. If points $x_1, \u0026hellip;, x_n$ have weights $\\mathbf{a} = (a_1, \u0026hellip;, a_n)$ and points $y_1, \u0026hellip;, y_m$ have weights $\\mathbf{b} = (b_1, \u0026hellip;, b_m)$, $T$ must verify:\n\\[ \\forall j \\in \\{1, \u0026hellip; m\\},\\quad b_j = \\sum_{i:T(x_i) = y_j} a_i \\]\nMonge problem With this formulation, we cannot work if the mass conservation constraint is not satisfied. It is also not easier to solve because we are still assigning elements $x_i$ to elements $y_j$.\nKantorovitch relaxation Even with the above extension, this formulation of the optimal transport problem is still too constrained to be practically useful in many cases. In 1942, Leonid Kantorovitch [3] introduced another key idea, which is to relax the deterministic portion of transportation. Source points $x_i$ no longer have to map to a single target point and can be fragmented into smaller pieces, this is called mass splitting.\nThis new formulation is much more adapted to some real world situations, for instance logistic problems. Frank L. Hitchcock stated a version of this problem in [4] as follows\nWhen several factories supply a product to a number of cities we desire the least costly manner of distribution. Due to freight rates and other matters the cost of a ton of product to a particular city will vary according to which factory supplies it, and will also vary from city to city.\nFactories with different supply capacities have to deliver goods to cities with various demands. To reflect this change, we modify our previous formulation slightly, by replacing the permutation $\\sigma$ by a coupling matrix $\\mathbf{P} = \\mathbf{P}_ {ij} \\in \\mathbb{R}_ {+}^{n\\times m}$. In the example above, each $\\mathbf{P}_{ij}$ would be the weight of the arrow from factory $i$ to city $j$. Admissible coupling for our problem can therefore be written as\n\\[ \\mathbf{U}(\\mathbf{a}, \\mathbf{b}) = \\left\\{\\mathbf{P} \\in \\mathbb{R}_{+}^{n \\times m}: \\mathbf{P}\\mathbb{1}_m = \\mathbf{a} \\text{ and } \\mathbf{P}^\\text{T}\\mathbb{1}_n = \\mathbf{b} \\right\\} \\]\nThis new formulation, contrary to the Monge formulation, is symmetric. This means that we can reverse the irreversible mapping of the problem above, because if $\\mathbf{P} \\in \\mathbf{U}(\\mathbf{a}, \\mathbf{b})$, then $\\mathbf{P}^{\\text{T}} \\in \\mathbf{U}(\\mathbf{b}, \\mathbf{a})$.\nWe can now formulate the problem in a mathematically cleaner way, and it reads:\n\\[ L_\\mathbf{C}(\\mathbf{a}, \\mathbf{b}) = \\min_{\\mathbf{P}\\in\\mathbf{U}(\\mathbf{b}, \\mathbf{a})} \\sum_{i,j} \\mathbf{C}_{i,j} \\mathbf{P}_{i,j} = \\min_{\\mathbf{P} \\in \\mathbf{U}(\\mathbf{b}, \\mathbf{a})} \\langle \\mathbf{C}, \\mathbf{P} \\rangle \\]\nIf you are familiar with optimization, you might have recognized a linear program, where the constraints are a set of $m+n$ equality constraints \u0026mdash; or $2(m+n)$ inequalities, which are contained in the admissible set of solutions. These constraints define a convex polytope.\nThis is good news, because we have moved from the dreadful realm of combinatorics to the comfortable world of convex optimization. Optimization over a matrix space might sound hard but it is usually much simpler than searching among a set of possible assignments.\nI will not go into the details of the tools used to solve this problem because they are widely used and taught in optimization and operations research courses. You will find a detailed explanation of the simplex algorithm, and other algorithmic tools to solve the OT problem, such as dual ascent methods or the Auction algorithm in [1].\nRegularized optimal transport This is great, we have transformed our exponentially hard to solve problem in a much more pleasant looking one! Unfortunately, this is still not easy to solve, and the algorithms mentioned above have mostly polynomial complexities with exponents larger than 2, and sometimes exponential worst-case complexities.\nEntropic regularization Regularizing optimal transport was proposed in [4]. It is a method for approximating solutions to the optimal transport problem by adding a regularizing term to the objective function. This forces the solution to satisfy a number of constraints, making the problem easier to solve.\nThe entropy of a coupling matrix $\\mathbf{P}$ is defined as\n\\[ \\mathbf{H}(\\mathbf{P}) = - \\sum_{i,j} \\mathbf{P_{i,j}}(\\log(\\mathbf{P}_{i,j}) - 1) \\]\nand the objective function for two histograms $\\mathbf{a}$ and $\\mathbf{b}$ now reads\n\\[ L^{\\varepsilon}_\\mathbf{C}(\\mathbf{a}, \\mathbf{b}) = \\min _{\\mathbf{P} \\in \\mathbf{U}(\\mathbf{a), \\mathbf{b}})} \\langle \\mathbf{P}, \\mathbf{C} \\rangle - \\varepsilon \\mathbf{H} (\\mathbf{P}) \\]\nThis addition makes the objective function an $\\varepsilon$-strongly convex function. There is therefore a unique optimal solution $\\mathbf{P}$. It can be shown that the solution to this regularized problem has the following form:\n\\[ \\forall (i,j) \\in \\{1,\u0026hellip;,n\\}\\times \\{1, \u0026hellip;, m\\},\\quad \\mathbf{P}_ {i,j} = \\mathbf{u}_ i \\mathbf{K} _{i,j} \\mathbf{v}_j \\]\nwhere $\\mathbf{K}_{i,j} = e^{-\\frac{\\mathbf{C} _{i,j}}{\\varepsilon}}$ and $\\mathbf{u}$ and $\\mathbf{v}$ are unknown scaling variables. This is a really big improvement, because we now have an explicit formula for an optimal coupling.\nSinkhorn\u0026rsquo;s algorithm The problem is known as the matrix scaling problem: we are trying to find two scaling vectors that when multiplied with $\\mathbf{K}$ give $\\mathbf{P}$. This can be achieved by alternatively updating $\\mathbf{u}$ and $\\mathbf{v}$ with Sinkhorn\u0026rsquo;s algorithm updates:\n\\[ \\mathbf{u}^{(\\ell+1)} = \\dfrac{\\mathbf{a}}{\\mathbf{Kv}^{(\\ell)}}\\ \\text{and} \\ \\mathbf{v}^{(\\ell + 1)} = \\dfrac{\\mathbf{b}}{\\mathbf{K}^{T}\\mathbf{u}^{(\\ell + 1)}} \\]\nAlthough this algorithm likely appeared at the beginning of the 20th century, the proof of its convergence is attributed to Sinkhorn [5]. The algorithm not only converges, but it does so at a linear rate. Don\u0026rsquo;t get fooled by the name, it is fast! This means it exists a certain factor $\\mu \\in [0, 1]$ and constant $C$, such that the solution $\\mathbf{u}^*$ of iterates $\\mathbf{u}^{(\\ell)}$ is approached at the speed of a geometric progression\n\\[ \\left|\\mathbf{u}^{(\\ell)} - \\mathbf{u}^* \\right| = C\\mu^\\ell \\]\nRegularized OT and Sinkhorn\u0026rsquo;s algorithms received renewed attention in the machine learning and data science community following a paper from Marco Cuturi in 2013 [6] that showed that Sinkhorn\u0026rsquo;s updates were an efficient and scalable approximation to OT that can be easily parallelized, for instance on GPUs.\nGoing further: Wasserstein distances, WGANs, Variational problems The machine learning community is getting excited by the possibilities that optimal transport has to offer. Wasserstein distances (more on this here) can be used as a loss function, leveraging physical and geometric ideas \u0026mdash; such as mass displacement, ground cost, etc. \u0026mdash; that are more natural than information based divergence between probability measures (such as Kullback-Leibler divergence). This excitement is illustrated by the number of papers accepted at NeurIPS mentioning the concept.\nWith more than a thousands citation in 2018-2019 (according to Semantic scholar), Wasserstein Generative Adversarial Networks (WGANs) are a good illustration of optimal transport ideas going mainstream \u0026mdash; although the fact that it is a paper about GANs might account more for those citations than the maths.\nMentions of \\\u0026#34;wasserstein\\\u0026#34; and \\\u0026#34;optimal transport\\\u0026#34; in NeurIPS paper titles over time Wasserstein distances In the case where the cost function corresponds to a distance \u0026mdash; like most examples above, the solution to the optimal transport problem for two measures (or histograms here) is a distance called Wasserstein distance. Formally, it is written\n\\[ W_p(\\mathbf{a}, \\mathbf{b}) = L _{\\mathbf{D}^p}(\\mathbf{a}, \\mathbf{b})^{1/p} \\]\nThis is the form of OT that has had the most applications in machine learning and data science, because of its ability to use a ground metric between bins and transform it into a metric between histograms of such bins. Earth mover\u0026rsquo;s distance is a particular example of $W_1$ with the Euclidean distance in $\\mathbb{R}^d$.\nWasserstein GANs WGANs [8] uses the Wasserstein distance in GAN training, instead of the Jensen-Shannon divergence. This results in improved stability and converging loss, and the added benefit that for applications such as computer vision, this loss should correspond better with image qualities because of the properties of OT.\nVariational problems When using Wasserstein distances as a loss function, the main technical challenge lies in differentiating it efficiently. Many algorithmic and mathematical tricks enable it in some settings, unlocking very interesting applications.\nFor example in [9], the authors are able to compute Wasserstein barycenters between 2D or 3D shapes. This barycenter is a intermediate shape, equidistant from a weighted set of starting shapes \u0026mdash; where the distance is the Wasserstein distance \u0026mdash; with a very natural looking \u0026ldquo;mass displacement\u0026rdquo; resembling interpolation. The figure below is from this paper.\nInterpolation between a cow, a duck and a torus The below animation illustrates this interpolation process. You can see parts of the shapes that get split and merged to fit the other shape in a smooth and natural-looking way.\nSmooth transitions between shapes, computed with POT's Convolutional Wasserstein Distances implementation Conclusion There is a lot more to say about optimal transport, but this post only aims at introducing the concept and is already long enough. The theory and mathematics are beautiful and get very advanced the deeper you go with this subject (see e.g. [10]). There is a lot of ongoing research into the theory and applications of optimal transport, especially in the machine learning community. If you are interested in getting started learning about it, I suggest you check Gabriel Peyré and Marco Cuturi\u0026rsquo;s book Computational optimal transport. It gives a very interesting overview of optimal transport and its applications. It can be read at different levels of mathematical complexity depending on your level of comfort. It is particularly adapted to data science and machine learning people who wish to learn more about the subject and the theory behind it.\n","title":"The Elegance of Optimal Transport","url":"https://hugocisneros.com/blog/the-elegance-of-optimal-transport/"}]