<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
  <title>Notes on: Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021) - Hugo Cisneros</title>
  <meta property="og:title" content="Notes on: Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021) - Hugo Cisneros">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/">
  <meta property="og:description" content="Notes about Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021)">
  <meta name="Description" property="description" content="Notes about Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021)">
  <link rel="me" href="https://twitter.com/@cisne_hug">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="me" href="https://scholar.social/@hugcis">
  <link rel="me" href="https://github.com/hugcis">
  <meta property="keywords" content="transformers">
  <link rel="stylesheet" href="https://hugocisneros.com/css/style.min.css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
  <script>
  function updateMode(){localStorage.theme==="dark"||!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}function toggleMode(){localStorage.theme==="dark"?localStorage.theme="light":localStorage.theme="dark",updateMode()}window.onload=updateMode();function toggleMenu(){let e=document.getElementById("navbar-default");e.classList.contains("hidden")?e.classList.remove("hidden"):e.classList.add("hidden")}
  </script>
</head>
<body>
  <header class="md:px-0 px-2">
    <nav>
      <div class="container flex flex-wrap justify-between items-center mx-auto">
        <div class="nav-main my-2.5">
          <a href="https://hugocisneros.com/" class="nav-title py-2.5 text-2xl text-zinc-600 dark:text-zinc-300 hover:border-b-0">Hugo Cisneros</a>
        </div><button type="button" onclick="toggleMenu()" class="inline-flex items-center p-2 ml-3 text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="navbar-default" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" aria-hidden="true" fill="currentcolor" viewbox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
        <path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button>
        <div class="hidden w-full md:block md:w-auto" id="navbar-default">
          <ul class="grid md:grid-flow-col items-center justify-between text-lg my-2.5">
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/blog/">Blog</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/notes/">Notes</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/projects/">Projects</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/resume/">Resume</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/contact/">Contact</a>
            </li>
            <li class="h-7 pl-2.5 pr-0 list-none"><button type="button" onclick="toggleMode()" class="h-full" aria-label="Toggle between dark and light mode"><img class="h-7 w-7 max-h-full mb-1.5 p-1.5 hidden dark:inline" id="ligh-mode-button-img" alt="A sun icon for switching to light mode" src="https://hugocisneros.com/img/light_mode.svg"> <img class="h-7 w-7 max-h-full mb-1.5 p-1.5 inline dark:hidden" id="dark-mode-button-img" alt="A moon icon for switching to dark mode" src="https://hugocisneros.com/img/dark_mode.svg"></button></li>
          </ul>
        </div>
      </div>
    </nav>
  </header>
  <main class="content h-card container mt-2 m-auto leading-loose md:px-0 px-2 z-0 Page(/notes/luPretrainedTransformersUniversal2021.md)" role="main">
    <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
      <div class="bg-zinc-100 dark:bg-zinc-700 pb-2 pt-1 px-3 rounded-lg mb-4">
        <h1 class="article-title p-name" itemprop="name">Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021)</h1>
        <div class="article-content e-content p-name" itemprop="articleBody">
          <dl>
            <dt>source</dt>
            <dd>
              (<a href="#luPretrainedTransformersUniversal2021"><cite itemprop="citation" itemscope itemid="key:luPretrainedTransformersUniversal2021">Lu et al. 2021</cite></a>)
            </dd>
            <dt>tags</dt>
            <dd>
              <a href="/notes/transformers/">Transformers</a>
            </dd>
          </dl>
          <h2 id="summary">Summary</h2>
          <p>Different types of <a href="/notes/neural_networks/">neural network</a> architecture encode different kinds of biases. For example, <a href="/notes/convolutional_neural_networks/">convolutional neural networks</a> perform local, translation-invariant operations and <a href="/notes/recurrent_neural_networks/">recurrent neural networks</a> operate on sequential data.</p>
          <p>One can use these biases in randomly initialized networks as a basis for interesting computations. This is on of the motivation for <a href="/notes/reservoir_computing/">reservoir computing</a> with <a href="/notes/echo_state_networks/">echo-state networks</a>, which uses fixed random recurrent neural network and a simple trainable linear transformation to perform complex computations. The intuition behind this is model is that interesting computations may be happening within the random RNN (and they are probably increasingly likely with bigger RNN). The linear layer can select useful computations and filter out the rest to give a result.</p>
          <p>This paper invastigates something similar to that idea. However, instead of random models they investigate the capacity of pretrained models, more precisely pretrained <a href="/notes/transformers/">transformers</a>.</p>
          <p>In their setup, all the weights of a transformer are frozen except for an input <a href="/notes/word_vectors/">embedding</a> layer, the positional embeddings, the layer norm parameters and a simple output layer. The architecture is shown in the Figure below taken from the paper:</p>
          <figure>
            <img src="/ox-hugo/luPretrainedTransformersUniversal2021.png">
          </figure>
          <p>This model is tested on a set of tasks:</p>
          <ul>
            <li>Bit memory: a task about memorizing bits in a sequence</li>
            <li>Bit XOR: a task about being able to XOR bits from two sequences</li>
            <li>ListOps: this task is akin to an elementary interpreter which has to perform operations on lists of integers</li>
            <li>MNIST: digit classification</li>
            <li>CIFAR-10: image classification</li>
            <li>CIFAR-10 LRA: a modified version of CIFAR where images are converted to grayscale and fed to the transformer with a token length of 1.</li>
            <li>Remote homology detection</li>
          </ul>
          <p>The paper uses a range of setups to understand which combination of architecture/pretraining/etc. works the best.</p>
          <h2 id="comments">Comments</h2>
          <p>The part which is most interesting to me is not the main result of that paper but the results from table 3, shown below</p>
          <figure>
            <img src="/ox-hugo/luPretrainedTransformersUniversal2021bis.png">
          </figure>
          <p>It is very interesting to see a possible superiority of random transformers over random LSTM, which may also explain why the former is so much better at many tasks. However I couldn’t find information about the number of frozen parameters in that LSTM vs. the tested transformers. Also, because LSTMs don’t have layer normalization or positional embeddings, there have significantly less trainable parameters (80% less for a task like CIFAR according to numbers from the paper).</p>
          <p>Maybe the authors factored this in by changing the number of parameters in the LSTM model but I couldn’t find that information in the paper.</p>
          <p>A fairer comparison would have a transformer and LSTM with the same number of parameters. However this would make gradient computation in the LSTM very expensive compared to the transformer — which was partly invented to cope with that problem.</p>
          <p>I also think that it is nice to find some evidence for a set of primitives being learned by <a href="/notes/language_modeling/">language models</a> and this may pave the way for unsupervised models that keep learning new things and reusing these elementary functions.</p>
          <h2 id="bibliography">Bibliography</h2>
          <ol class="biblio-list pl-0">
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="luPretrainedTransformersUniversal2021"><span itemprop="author">Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch</span>. <time datetime="2021" itemprop="datePublished">March 9, 2021</time>. "<span itemprop="name">Pretrained Transformers as Universal Computation Engines</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Arxiv:2103.05247 [cs]</i></span>. <a itemprop="sameAs" href="http://arxiv.org/abs/2103.05247">http://arxiv.org/abs/2103.05247</a>.</span></li>
          </ol>
        </div>
        <div class="text-center" style="font-variant-caps:all-small-caps">
          Last changed <a class="u-url" href="https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/"><time itemprop="datePublished" class="dt-published" datetime="2021-08-25T15:19:00+0200">25/08/2021</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
        </div>
      </div>
    </article>
    <h3>Comments</h3>
    <script data-isso="https://comment.hugocisneros.com/" data-isso-require-author="true" data-isso-vote="true" src="https://comment.hugocisneros.com/js/embed.min.js"></script>
    <section id="isso-thread"></section><br>
    <a href="/notes#lupretrainedtransformersuniversal2021"><b>← Back to Notes</b></a>
  </main>
  <footer class="footer container h-10 text-center mt-1">
    <hr class="my-4">
    <ul class="pl-0 mt-1">
      <li class="first:before:content-none before:content-['•'] inline-block list-none">
        <a class="rss-link inline-block text-neutral-800 dark:text-neutral-400 border-none" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon w-4 inline-block" src="https://hugocisneros.com/img/RSS.svg" alt="RSS feed icon"></a>
      </li>
      <li class="ml-2 first:before:content-none before:content-['•'] inline-block list-none">
        <a class="ml-2 text-neutral-800 dark:text-neutral-400 border-none" href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
      </li>
      <li class="ml-2 first:before:content-none before:content-['•'] text-neutral-800 dark:text-neutral-400 inline-block list-none"><span class="ml-2">© Hugo Cisneros 2022</span></li>
    </ul>
  </footer>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>
  document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})
  </script>
  <script data-goatcounter="https://stats.hugocisneros.com/count" async src="//stats.hugocisneros.com/count.js"></script>
</body>
</html>
