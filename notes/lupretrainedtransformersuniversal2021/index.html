<!DOCTYPE html>
<html lang="en-us">
<head>


<meta charset="utf-8">
<meta name="viewport" content=
"width=device-width,initial-scale=1.0,minimum-scale=1">
<title>Notes on: Pretrained Transformers as Universal Computation
Engines by Lu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021) -
Hugo Cisneros - Personal page</title>
<meta property="og:title" content=
"Notes on: Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021) - Hugo Cisneros - Personal page">
<meta property="og:type" content="article">
<meta property="og:image" content="/img/main.jpeg">
<meta property="og:url" content=
"https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/">
<meta property="og:description" content=
"Notes about Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021)">
<meta name="Description" property="description" content=
"Notes about Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021)">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@cisne_hug">
<meta name="twitter:creator" content="@cisne_hug">
<link rel="stylesheet" href=
"https://hugocisneros.com/css/main.min.da0ecaf5695aac82ac1d036476c57133068282267487d121fc3ad2571b732644.css"
media="all" type="text/css">
<link rel="apple-touch-icon" sizes="180x180" href=
"/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href=
"/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href=
"/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color=
"#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<link rel="webmention" href=
"https://webmention.io/hugocisneros.com/webmention">
<link rel="pingback" href=
"https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
<div class="wrapper">
<header class="header">
<nav class="nav">
<div class="nav-main"><a href="https://hugocisneros.com/" class=
"nav-title">Hugo Cisneros - Personal page</a></div>
<ul class="nav-links">
<li><a href="/about/">About</a></li>
<li><a href="/blog/">Blog</a></li>
<li><a href="/notes/">Notes</a></li>
<li><a href="/resume/">Resume</a></li>
</ul>
</nav>
</header>
<main class="content" role="main">
<article class="article h-entry" itemprop="mainEntity" itemscope
itemtype="http://schema.org/BlogPosting">
<div class="single-note note-container">
<h1 class="article-title p-name" itemprop="name">Pretrained
Transformers as Universal Computation Engines by Lu, K., Grover,
A., Abbeel, P., & Mordatch, I. (2021)</h1>
<div class="article-content e-content p-name" itemprop=
"articleBody">
<dl>
<dt>source</dt>
<dd>(<a href="#org0ad8d8b">Lu et al. 2021</a>)</dd>
<dt>tags</dt>
<dd><a href="/notes/transformers/">Transformers</a></dd>
</dl>
<h2 id="summary">Summary</h2>
<p>Different types of <a href="/notes/neural_networks/">neural
network</a> architecture encode different kinds of biases. For
example, <a href=
"/notes/convolutional_neural_networks/">convolutional neural
networks</a> perform local, translation-invariant operations and
<a href="/notes/recurrent_neural_networks/">recurrent neural
networks</a> operate on sequential data.</p>
<p>One can use these biases in randomly initialized networks as a
basis for interesting computations. This is on of the motivation
for <a href="/notes/reservoir_computing/">reservoir computing</a>
with <a href="/notes/echo_state_networks/">echo-state networks</a>,
which uses fixed random recurrent neural network and a simple
trainable linear transformation to perform complex computations.
The intuition behind this is model is that interesting computations
may be happening within the random RNN (and they are probably
increasingly likely with bigger RNN). The linear layer can select
useful computations and filter out the rest to give a result.</p>
<p>This paper invastigates something similar to that idea. However,
instead of random models they investigate the capacity of
pretrained models, more precisely pretrained <a href=
"/notes/transformers/">transformers</a>.</p>
<p>In their setup, all the weights of a transformer are frozen
except for an input <a href="/notes/word_vectors/">embedding</a>
layer, the positional embeddings, the layer norm parameters and a
simple output layer. The architecture is shown in the Figure below
taken from the paper:</p>
<figure><img src=
"/ox-hugo/luPretrainedTransformersUniversal2021.png"></figure>
<p>This model is tested on a set of tasks:</p>
<ul>
<li>Bit memory: a task about memorizing bits in a sequence</li>
<li>Bit XOR: a task about being able to XOR bits from two
sequences</li>
<li>ListOps: this task is akin to an elementary interpreter which
has to perform operations on lists of integers</li>
<li>MNIST: digit classification</li>
<li>CIFAR-10: image classification</li>
<li>CIFAR-10 LRA: a modified version of CIFAR where images are
converted to grayscale and fed to the transformer with a token
length of 1.</li>
<li>Remote homology detection</li>
</ul>
<p>The paper uses a range of setups to understand which combination
of architecture/pretraining/etc. works the best.</p>
<h2 id="comments">Comments</h2>
<p>The part which is most interesting to me is not the main result
of that paper but the results from table 3, shown below</p>
<figure><img src=
"/ox-hugo/luPretrainedTransformersUniversal2021bis.png"></figure>
<p>It is very interesting to see a possible superiority of random
transformers over random LSTM, which may also explain why the
former is so much better at many tasks. However I couldn’t find
information about the number of frozen parameters in that LSTM vs.
the tested transformers. Also, because LSTMs don’t have layer
normalization or positional embeddings, there have significantly
less trainable parameters (80% less for a task like CIFAR according
to numbers from the paper).</p>
<p>Maybe the authors factored this in by changing the number of
parameters in the LSTM model but I couldn’t find that information
in the paper.</p>
<p>A fairer comparison would have a transformer and LSTM with the
same number of parameters. However this would make gradient
computation in the LSTM very expensive compared to the transformer
— which was partly invented to cope with that problem.</p>
<p>I also think that it is nice to find some evidence for a set of
primitives being learned by <a href=
"/notes/language_modeling/">language models</a> and this may pave
the way for unsupervised models that keep learning new things and
reusing these elementary functions.</p>
<h2 id="bibliography">Bibliography</h2>
<p><a id="org0ad8d8b"></a>Lu, Kevin, Aditya Grover, Pieter Abbeel,
and Igor Mordatch. 2021. “Pretrained Transformers as Universal
Computation Engines.” <em>arXiv:2103.05247 [Cs]</em>, March.</p>
</div>
<div class="bl-section">
<h2>Links to this note</h2>
<div class="backlinks">
<ul>
<li><a href="/notes/lupretrainedtransformersuniversal2021/">Notes
on: Pretrained Transformers as Universal Computation Engines by Lu,
K., Grover, A., Abbeel, P., & Mordatch, I. (2021)</a></li>
</ul>
</div>
</div>
<div class="note-footer">Last changed <a class="u-url" href=
"https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/">
<time itemprop="datePublished" class="dt-published" datetime=
"2021-03-29T12:22:00+0200">29/03/2021</time></a> | authored by
<a href="https://hugocisneros.com/" rel="author" class=
"p-author h-card" itemprop="author" itemscope itemtype=
"http://schema.org/Person"><span itemprop="name">Hugo
Cisneros</span></a></div>
</div>
</article>
<br>
<a href="/notes#lupretrainedtransformersuniversal2021"><b>← Back to
Notes</b></a>
<hr></main>
<footer class="footer">
<ul class="footer-links">
<li><a class="rss-link" href="/blog/index.xml" type=
"application/rss+xml" target="_blank">Blog <img class="rss-icon"
src="/img/RSS.svg" alt="RSS feed icon"></a></li>
<li><a href=
"https://github.com/hugcis/natrium-custom">Code</a></li>
<li>© Hugo Cisneros 2021</li>
</ul>
</footer>
</div>
<script>
 MathJax = {
     tex: {
         inlineMath: [['$','$'], ['\\(', '\\)']],
         tags: 'ams'
     }
 };
</script> 
<script type="text/javascript" rel="preconnect" id="MathJax-script"
async src=
"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
