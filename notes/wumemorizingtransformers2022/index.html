<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
  <title>Notes on: Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., & Szegedy, C. (2022) - Hugo Cisneros</title>
  <meta property="og:title" content="Notes on: Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., &amp; Szegedy, C. (2022) - Hugo Cisneros">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/wumemorizingtransformers2022/">
  <meta property="og:description" content="Notes about Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., &amp; Szegedy, C. (2022)">
  <meta name="Description" property="description" content="Notes about Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., &amp; Szegedy, C. (2022)">
  <link rel="me" href="https://twitter.com/@cisne_hug">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="me" href="https://scholar.social/@hugcis">
  <link rel="me" href="https://github.com/hugcis">
  <meta property="keywords" content="transformers, memory in neural networks">
  <link rel="stylesheet" href="https://hugocisneros.com/main.min.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/projects/">Projects</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
          <li>
            <a href="/contact/">Contact</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., & Szegedy, C. (2022)</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>source</dt>
              <dd>
                (<a href="#wuMemorizingTransformers2022"><cite itemprop="citation" itemscope itemid="doi:10.48550/arXiv.2203.08913">Wu et al. 2022</cite></a>)
              </dd>
              <dt>tags</dt>
              <dd>
                <a href="/notes/transformers/">Transformers</a>, <a href="/notes/memory_in_neural_networks/">Memory in neural networks</a>
              </dd>
            </dl>
            <h2 id="summary">Summary</h2>
            <p>This paper introduces a method to extend the classical <a href="/notes/transformers/">Transformer</a> <a href="/notes/neural_networks/">neural network</a> model with an addressable memory that can be queried and updated at inference time.</p>
            <p>This memory is addressed using an <a href="/notes/attention/">attention</a> mechanism. It is a set of cached attention (key, value) vector pairs. At some arbitrary depth of the attention “stack” the memory mechanism is inserted.</p>
            <p>A query \(\bm{Q}\) is used both for the self-attention over the local context (the other tokens in the input) as well as the (key, value) pairs stored in the memory. We write \(\bm{K}_l\), \(\bm{V}_l\) the key value matrices of the local context. The local self-attention output (denoted \(\bm{V}_c\) in the paper) is computed as follows:</p>
            <p>\[\bm{V}_l’ = \text{softmax}\left( \dfrac{\bm{Q}\bm{K}_l^{T}}{\sqrt{D}} \right) \bm{V}_l.\]</p>
            <p>For each query (i.e. each input token position), a set of \(k\) keys is retrieved from memory. They correspond to the \(k\) nearest neighbor keys to the query.</p>
            <p>Combining the memory with the query is done similarly to standard attention, except that each query has a different set of (key, value) pairs to attend to. With \(\bm{K}_m\) and \(\bm{V}_m\) the matrix of (key, values) retrieved from the memory for query \(\bm{q}_i\) we have: \[\bm{v}_{m, i} ’ = \text{softmax}\left(\dfrac{\bm{q}_i \bm{K}_m^{T}}{\sqrt{D}} \right) \bm{V}_m.\]</p>
            <p>Then the final value matrix is computed by combining \(\bm{V}_l’\) and \(\bm{V}_{m}’\):</p>
            <p>\[ \bm{V} = \bm{V}_m \odot g + \bm{V}_l \odot (1 - g) \]</p>
            <p>with \(g = \sigma(b_g)\) is the sigmoid of a per-head scalar parameter that determines how much importance is given to the external memory.</p>
            <h2 id="bibliography">Bibliography</h2>
            <ol class="biblio-list">
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="wuMemorizingTransformers2022" itemid="doi:10.48550/arXiv.2203.08913">
                <span itemprop="author">Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, Christian Szegedy</span>. <time datetime="2022" itemprop="datePublished">March 16, 2022</time>. "<span itemprop="name">Memorizing Transformers</span>". arXiv. <a itemprop="sameAs" href="https://doi.org/10.48550/arXiv.2203.08913">DOI</a>.
              </li>
            </ol>
          </div>
          <div class="bl-section">
            <h2>Links to this note</h2>
            <div class="backlinks">
              <ul>
                <li>
                  <a href="/notes/wumemorizingtransformers2022/">Notes on: Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., & Szegedy, C. (2022)</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/wumemorizingtransformers2022/"><time itemprop="datePublished" class="dt-published" datetime="2022-05-25T13:26:00+0200">25/05/2022</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article>
      <h3>Comments</h3>
      <script data-isso="https://comment.hugocisneros.com/" data-isso-require-author="true" data-isso-vote="true" src="https://comment.hugocisneros.com/js/embed.min.js"></script>
      <section id="isso-thread"></section><br>
      <a href="/notes#wumemorizingtransformers2022"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
        </li>
        <li>© Hugo Cisneros 2022</li>
      </ul>
    </footer>
  </div>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>
  document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})
  </script>
  <script data-goatcounter="https://stats.hugocisneros.com/count" async src="//stats.hugocisneros.com/count.js"></script>
</body>
</html>
