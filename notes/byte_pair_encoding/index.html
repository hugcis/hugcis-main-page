<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
  <title>Byte-pair encoding - Hugo Cisneros</title>
  <meta property="og:title" content="Byte-pair encoding - Hugo Cisneros">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/byte_pair_encoding/">
  <meta property="og:description" content="Notes about Byte-pair encoding">
  <meta name="Description" property="description" content="Notes about Byte-pair encoding">
  <link rel="me" href="https://twitter.com/@cisne_hug">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="me" href="https://scholar.social/@hugcis">
  <link rel="me" href="https://github.com/hugcis">
  <meta property="keywords" content="nlp">
  <link rel="stylesheet" href="https://hugocisneros.com/main.min.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/projects/">Projects</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
          <li>
            <a href="/contact/">Contact</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">Byte-pair encoding</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>tags</dt>
              <dd>
                <a href="/notes/nlp/">NLP</a>
              </dd>
            </dl>
            <p>The process of byte-pair encoding can be summarized as follow:</p>
            <ul>
              <li>Each character is a token</li>
              <li>Find pairs that occur most often</li>
              <li>Create a new token that encoded those common pairs</li>
              <li>Repeat the process until target vocabulary size is reached</li>
            </ul>
            <p>The output of this process is both a vocabulary and a set of merging rules for tokens to be used to process more data.</p>
            <p>This technique has several advantages:</p>
            <ul>
              <li>It is inexpensive</li>
              <li>It can deal with previously unseen words and make reasonable predictions about them if the token matches semantic information about the word.</li>
            </ul>
            <p>For these reasons, this encoding method is currently the dominant one for <a href="/notes/transformers/">transformers</a> architecture for example.</p>
            <h2 id="minimal-example-in-python--python-dot-md">Minimal example in <a href="/notes/python/">Python</a></h2>
            <p>Here is a very simple Python snippet to apply a BPE pass on a sequence of tokens.</p>
            <div class="highlight">
              <pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#719e07">from</span> collections <span style="color:#719e07">import</span> counter
<span style="color:#719e07">from</span> typing <span style="color:#719e07">import</span> Sequence, Tuple

<span style="color:#586e75"># An encoding step will reduce the most common pair to a single token and returns</span>
<span style="color:#586e75"># the modified "dataset" along with the vocabulary.</span>
<span style="color:#719e07">def</span> <span style="color:#268bd2">bp_encode_step</span>(words: Sequence[<span style="color:#b58900">str</span>]) <span style="color:#719e07">-&gt;</span> Tuple[<span style="color:#b58900">list</span>[<span style="color:#b58900">str</span>], <span style="color:#b58900">set</span>[<span style="color:#b58900">str</span>]]:
    <span style="color:#2aa198">"""This iteratively creates a new list of tokens where the most common pair
</span><span style="color:#2aa198">    has been merged into a single token."""</span>
    pair_count <span style="color:#719e07">=</span> counter(<span style="color:#b58900">zip</span>(words[:<span style="color:#719e07">-</span><span style="color:#2aa198">1</span>], words[<span style="color:#2aa198">1</span>:]))
    most_common_pair <span style="color:#719e07">=</span> pair_count<span style="color:#719e07">.</span>most_common(<span style="color:#2aa198">1</span>)[<span style="color:#2aa198">0</span>][<span style="color:#2aa198">0</span>]
    output <span style="color:#719e07">=</span> []
    n <span style="color:#719e07">=</span> <span style="color:#2aa198">0</span>
    <span style="color:#719e07">while</span> n <span style="color:#719e07">&lt;</span> <span style="color:#b58900">len</span>(words):
        w <span style="color:#719e07">=</span> words[n]

        <span style="color:#719e07">if</span> (
            n <span style="color:#719e07">&lt;</span> <span style="color:#b58900">len</span>(words) <span style="color:#719e07">-</span> <span style="color:#2aa198">1</span>
            <span style="color:#719e07">and</span> w <span style="color:#719e07">==</span> most_common_pair[<span style="color:#2aa198">0</span>]
            <span style="color:#719e07">and</span> words[n <span style="color:#719e07">+</span> <span style="color:#2aa198">1</span>] <span style="color:#719e07">==</span> most_common_pair[<span style="color:#2aa198">1</span>]
        ):
            output<span style="color:#719e07">.</span>append(w <span style="color:#719e07">+</span> words[n <span style="color:#719e07">+</span> <span style="color:#2aa198">1</span>])
            n <span style="color:#719e07">+=</span> <span style="color:#2aa198">1</span>
        <span style="color:#719e07">else</span>:
            output<span style="color:#719e07">.</span>append(w)
        n <span style="color:#719e07">+=</span> <span style="color:#2aa198">1</span>
    <span style="color:#719e07">return</span> output, <span style="color:#b58900">set</span>(output)
</code></pre>
            </div>
            <p>We apply the encoding function to a fake dataset for several steps:</p>
            <p><a id="code-snippet--bpe-encode"></a></p>
            <div class="highlight">
              <pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#586e75"># The "dataset" we will be using</span>
words <span style="color:#719e07">=</span> <span style="color:#2aa198">"This is the first time I experienced this kind of thing. </span><span style="color:#cb4b16">\
</span><span style="color:#2aa198">Let's try this thing again."</span>

<span style="color:#719e07">for</span> iteration <span style="color:#719e07">in</span> <span style="color:#b58900">range</span>(<span style="color:#2aa198">24</span>):
    words, vocab <span style="color:#719e07">=</span> bp_encode_step(words)
    <span style="color:#719e07">if</span> iteration <span style="color:#719e07">%</span> <span style="color:#2aa198">4</span> <span style="color:#719e07">==</span> <span style="color:#2aa198">0</span>:
        <span style="color:#b58900">print</span>(<span style="color:#2aa198">f</span><span style="color:#2aa198">"Step </span><span style="color:#2aa198">{</span>iteration<span style="color:#2aa198">}</span><span style="color:#2aa198"> vocab size is </span><span style="color:#2aa198">{</span><span style="color:#b58900">len</span>(vocab)<span style="color:#2aa198">}</span><span style="color:#2aa198">,"</span>,
              <span style="color:#2aa198">"tokenized sentence:"</span>, <span style="color:#2aa198">"_"</span><span style="color:#719e07">.</span>join(words), <span style="color:#2aa198">"</span><span style="color:#cb4b16">\n</span><span style="color:#2aa198">"</span>)
</code></pre>
            </div>
            <h3 id="results">Results</h3>
            <p>The various tokens are separated by the symbol <code>_</code> in the results:</p>
            <div class="highlight">
              <pre tabindex="0" style="color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Step 0 vocab size is 25, tokenized sentence: T_h_i_s_ _i_s_ t_h_e_ _f_i_r_s_t_ t_i_m_e_ _I_ _e_x_p_e_r_i_e_n_c_e_d_ t_h_i_s_ _k_i_n_d_ _o_f_ t_h_i_n_g_._ _L_e_t_'_s_ t_r_y_ t_h_i_s_ t_h_i_n_g_ _a_g_a_i_n_.
Step 4 vocab size is 29, tokenized sentence: T_hi_s _i_s t_h_e_ _f_i_r_s_t_ t_i_m_e_ _I_ _e_x_p_e_r_i_e_n_c_e_d_ thi_s _k_i_n_d_ _o_f_ thi_n_g_._ _L_e_t_'_s t_r_y_ thi_s_ thi_n_g_ _a_g_a_i_n_.
Step 8 vocab size is 32, tokenized sentence: T_hi_s _i_s t_h_e _f_i_r_s_t_ t_i_m_e _I_ _e_x_p_e_r_i_e_n_c_e_d_ thi_s _k_in_d_ _o_f_ thing_._ _L_e_t_'_s t_r_y_ thi_s_ thing_ _a_g_a_in_.
Step 12 vocab size is 31, tokenized sentence: This is t_h_e _f_i_r_s_t_ t_i_m_e _I_ _e_x_p_e_r_i_e_n_c_e_d_ thi_s _k_in_d_ _o_f_ thing_._ _L_e_t_'_s t_r_y_ thi_s_ thing_ _a_g_a_in_.
Step 16 vocab size is 30, tokenized sentence: This is the fi_r_s_t_ t_i_m_e _I_ _e_x_p_e_r_i_e_n_c_e_d_ thi_s _k_in_d_ _o_f_ thing_._ _L_e_t_'_s t_r_y_ thi_s_ thing_ _a_g_a_in_.
Step 20 vocab size is 29, tokenized sentence: This is the first t_i_m_e _I_ _e_x_p_e_r_i_e_n_c_e_d_ thi_s _k_in_d_ _o_f_ thing_._ _L_e_t_'_s t_r_y_ thi_s_ thing_ _a_g_a_in_.
</code></pre>
            </div>
            <p>On such a small “dataset”, the benefits of BPE are not apparent. The vocabulary quickly ends up with pairs of tokens appearing only once for which there isn’t anything more to do.</p>
            <p>Notice how from step 8, the very common word <code>this</code> is encoded as a single token.</p>
          </div>
          <div class="bl-section">
            <h2>Links to this note</h2>
            <div class="backlinks">
              <ul>
                <li>
                  <a href="/notes/nlp/">NLP</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/byte_pair_encoding/"><time itemprop="datePublished" class="dt-published" datetime="2022-04-14T17:28:00+0200">14/04/2022</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article>
      <h3>Comments</h3>
      <script data-isso="https://comment.hugocisneros.com/" data-isso-require-author="true" data-isso-vote="true" src="https://comment.hugocisneros.com/js/embed.min.js"></script>
      <section id="isso-thread"></section><br>
      <a href="/notes#byte_pair_encoding"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
        </li>
        <li>© Hugo Cisneros 2022</li>
      </ul>
    </footer>
  </div>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>
  document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})
  </script> 
  <script data-goatcounter="https://stats.hugocisneros.com/count" async src="//stats.hugocisneros.com/count.js"></script>
</body>
</html>
