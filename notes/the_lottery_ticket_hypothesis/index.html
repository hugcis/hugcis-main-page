<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1">
  <title>The Lottery ticket hypothesis - Hugo Cisneros - Personal page</title>
  <meta property="og:title" content="The Lottery ticket hypothesis - Hugo Cisneros - Personal page">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/the_lottery_ticket_hypothesis/">
  <meta property="og:description" content="Notes about The Lottery ticket hypothesis">
  <meta name="Description" property="description" content="Notes about The Lottery ticket hypothesis">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="stylesheet" href="https://hugocisneros.com/css/main.min.914a9407ca164cc303752993663b9b622d3922262de606128ff489937b2e2dd7.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros - Personal page</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/about/">About</a>
          </li>
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">The Lottery ticket hypothesis</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>tags</dt>
              <dd>
                <a href="/notes/neural_network_training/">Neural network training</a>
              </dd>
              <dt>resources</dt>
              <dd>
                <a href="https://soundcloud.com/theaipodcast/mit-the-lottery-hypothesis">The AI podcast</a>
              </dd>
              <dt>papers</dt>
              <dd>
                (<a href="#org46680ec">Frankle and Carbin 2018</a>)
              </dd>
            </dl>
            <p>When training very large <a href="/notes/neural_networks/">neural networks</a>, the obtained net might have a lot of unused neurons. It is possible, through <a href="/notes/neural_network_pruning/">neural network pruning</a>, to remove a lot of those unused connections to make the overall architecture lighter and faster to run on some hardware.</p>
            <p>However, once you have the pruned architecture, it will often not be able to learn anything interesting when it is trained from scratch. The lottery ticket hypothesis is about the reason some of these randomly initialized neurons became more important than others. It is possible that random initialization is actually very important for a neuron to be useful after training, and the lottery ticket hypothesis is about finding this “magic” initialization to use it on small networks.</p>
            <p>From (<a href="#org46680ec">Frankle and Carbin 2018</a>):</p>
            <blockquote>
              <p><strong>The Lottery Ticket Hypothesis.</strong> A randomly-initialized, dense neural network contains a subnetwork that is initialized such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.</p>
            </blockquote>
            <p>It turns out restarting pruned small network training from the exact initialization the initial network started from achieves excellent results. However, this hypothesis holds mostly for small neural networks. In larger networks, the pruned network is usually found more early in training.</p>
            <h2 id="bibliography">Bibliography</h2>
            <p><a id="org46680ec"></a><span itemscope itemtype="https://schema.org/ScholarlyArticle"><span itemprop="author"><span itemprop="author">Frankle, Jonathan</span>, and <span itemprop="author">Michael Carbin</span></span>. <span datetime="2018" itemprop="datePublished">March 2018</span>. “<span itemprop="name">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</span>”. <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">arXiv:1803.03635 [Cs]</i></span>, March.</span></p>
          </div>
          <div class="bl-section">
            <h2>Links to this note</h2>
            <div class="backlinks">
              <ul>
                <li>
                  <a href="/notes/greydanusscalingdeeplearning2020/">Notes on: Scaling down Deep Learning by Greydanus, S. (2020)</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/the_lottery_ticket_hypothesis/"><time itemprop="datePublished" class="dt-published" datetime="2020-07-09T12:42:00+0200">09/07/2020</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article><br>
      <a href="/notes#the_lottery_ticket_hypothesis"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/natrium-custom">Code</a>
        </li>
        <li>© Hugo Cisneros 2021</li>
      </ul>
    </footer>
  </div>
  <script>
  MathJax = {
     tex: {
         inlineMath: [['$','$'], ['\\(', '\\)']],
         tags: 'ams'
     }
  };
  </script> 
  <script type="text/javascript" rel="preconnect" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
