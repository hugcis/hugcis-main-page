<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
  <title>Notes on: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020) - Hugo Cisneros</title>
  <meta property="og:title" content="Notes on: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., &amp; Fleuret, F. (2020) - Hugo Cisneros">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/katharopoulostransformersarernns2020/">
  <meta property="og:description" content="Notes about Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., &amp; Fleuret, F. (2020)">
  <meta name="Description" property="description" content="Notes about Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., &amp; Fleuret, F. (2020)">
  <link rel="me" href="https://twitter.com/@cisne_hug">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="me" href="https://scholar.social/@hugcis">
  <link rel="me" href="https://github.com/hugcis">
  <meta property="keywords" content="transformers, rnn">
  <link rel="stylesheet" href="https://hugocisneros.com/css/style.min.css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
  <script>
  function updateMode(){localStorage.theme==="dark"||!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}function toggleMode(){localStorage.theme==="dark"?localStorage.theme="light":localStorage.theme="dark",updateMode()}window.onload=updateMode();function toggleMenu(){let e=document.getElementById("navbar-default");e.classList.contains("hidden")?e.classList.remove("hidden"):e.classList.add("hidden")}
  </script>
</head>
<body>
  <header class="md:px-0 px-2">
    <nav>
      <div class="container flex flex-wrap justify-between items-center mx-auto">
        <div class="nav-main my-2.5">
          <a href="https://hugocisneros.com/" class="nav-title py-2.5 text-2xl text-zinc-600 dark:text-zinc-300 hover:border-b-0">Hugo Cisneros</a>
        </div><button type="button" onclick="toggleMenu()" class="inline-flex items-center p-2 ml-3 text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="navbar-default" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" aria-hidden="true" fill="currentcolor" viewbox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
        <path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button>
        <div class="hidden w-full md:block md:w-auto" id="navbar-default">
          <ul class="grid md:grid-flow-col items-center justify-between text-lg my-2.5">
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/blog/">Blog</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/notes/">Notes</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/projects/">Projects</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/resume/">Resume</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/contact/">Contact</a>
            </li>
            <li class="h-7 pl-2.5 pr-0 list-none"><button type="button" onclick="toggleMode()" class="h-full"><img class="h-7 w-7 max-h-full mb-1.5 p-1.5 hidden dark:inline" id="dark-mode-button-img" src="https://hugocisneros.com/img/light_mode.svg"> <img class="h-7 w-7 max-h-full mb-1.5 p-1.5 inline dark:hidden" id="dark-mode-button-img" src="https://hugocisneros.com/img/dark_mode.svg"></button></li>
          </ul>
        </div>
      </div>
    </nav>
  </header>
  <main class="content h-card container mt-2 m-auto leading-loose md:px-0 px-2 z-0 Page(/notes/katharopoulosTransformersAreRNNs2020.md)" role="main">
    <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
      <div class="bg-zinc-100 dark:bg-zinc-700 pb-2 pt-1 px-3 rounded-lg mb-4">
        <h1 class="article-title p-name" itemprop="name">Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020)</h1>
        <div class="article-content e-content p-name" itemprop="articleBody">
          <dl>
            <dt>tags</dt>
            <dd>
              <a href="/notes/transformers/">Transformers</a>, <a href="/notes/recurrent_neural_networks/">RNN</a>
            </dd>
            <dt>source</dt>
            <dd>
              (<a href="#katharopoulosTransformersAreRNNs2020"><cite itemprop="citation" itemscope itemid="key:katharopoulosTransformersAreRNNs2020">Katharopoulos et al. 2020</cite></a>)
            </dd>
          </dl>
          <h2 id="summary">Summary</h2>
          <p><a href="/notes/transformers/">Transformers</a> have traditionally been described as different models from <a href="/notes/recurrent_neural_networks/">RNNs</a>. This is because instead of processing the sequence one token at a time, Transformers use <a href="/notes/attention/">attention</a> to process all elements simultaneously.</p>
          <p>The paper introduces an interesting new formulation, replacing the softmax attention with a feature map-based dot product.</p>
          <p>This new formulation yields better time and memory complexity as well as a model that is casual and autoregressive (similar to RNNs).</p>
          <p>A Transformer applied on sequence \(x\) is presented as a composition of multiple Transformer layers \(T_l\), with</p>
          <p>\[ T_l(x) = f_l(A_l(x) + x) \]</p>
          <p>Function \(f_l\) is applied to each component independently, while attention \(A_l\) is applied to the whole input sequence.</p>
          <p>Softmax <a href="/notes/attention/">self-attention</a> at layer \(l\) with <em>queries</em>, <em>keys</em> and <em>values</em> matrices is written</p>
          <p>\[A_l(x) = V’ = \text{softmax}\left( \dfrac{QK^{T}}{\sqrt{D}} \right) V.\]</p>
          <p>The equation above can be generalized to any similarity function \(\text{sim}\), and if \(V’_i\) designates the $i$-th row of \(V’\),</p>
          <p>\[ V’_i = \dfrac{\sum_{j = 1}^N \text{sim}(Q_i, K_j) V_j}{\sum_{j = 1}^N \text{sim}(Q_i, K_j)} \]</p>
          <h3 id="linearizing-attention">Linearizing attention</h3>
          <p>In particular, all <a href="/notes/kernel_methods/">kernels</a> \(k(x, y) = \langle\phi(x), \phi(y)\rangle_\mathcal{S} : \mathbb{R}^{2\times F} \rightarrow \mathbb{R}_+\) can be used as a similarity function, changing the equation above to</p>
          <p>\[ V’_i = \dfrac{\phi(Q_i)^T \sum_{j = 1}^N \phi(K_j) V_j^T}{ \phi(Q_i)^T \sum_{j = 1}^N \phi(K_j)}.\]</p>
          <p>Because the right term of the numerator and denominator above does not depend on \(i\), it can be computed once for all sequence, and time and memory complexity become \(\mathcal{O}(N)\).</p>
          <h3 id="masking-for-autoregressive-models">Masking for autoregressive models</h3>
          <p>By replacing \(N\) by \(i\) in the expression above, one readily obtains a formulation of the Transformer function which only depends on previous tokens. This is used to train <a href="/notes/language_modeling/">language models</a> in particular, because the prediction of a token can only depend on the previous tokens.</p>
          <h3 id="transformers-are-rnns">Transformers are RNNs</h3>
          <p>By rewriting the main kernel formulation of a Transformer above, one sees how it can actually be seen as a RNN. Timesteps of the recurrence are denoted as subscripts.</p>
          <p>\[ \begin{aligned} & s_0 = 0, z_0 = 0 \newline & s_i = s_{i-1} + \phi(x_i W_K) (x_i W_V)^T \newline & z_i = z_{i-1} + \phi(x_i W_K) \newline & y_i = f_l \left( \dfrac{\phi(x_i W_Q)^T s_i}{\phi(x_i W_Q)^T z_i} + x_i \right) \end{aligned} \]</p>
          <p>The resulting RNN has two hidden states, namely the attention memory \(s\) and the normalizer memory \(z\).</p>
          <h2 id="comments">Comments</h2>
          <p>The parallel between RNNs and Transformer models is clearly made in this paper. I believe this is significant because it give insights into why Transformers might be better at language modeling than RNN-based models.</p>
          <p>It would seem from this new formulation that they aren’t better than RNNs but the choice of update function (in the equation above) they are equivalent to is superior.</p>
          <p>Another possibility is that RNNs and Transformers have always had the same potential. The hype might have fuelled more effort into making Transformers models work better and have thus widened the performance gap between the two otherwise equivalent models. Recent research into RNN models also seems to have favored a few dominant models (standard RNN, LSTM and GRU) and might have slowed the discovery of other, more effective cells.</p>
          <p>Experiments in the paper only demonstrate the performance of their new model on small tasks and I would like to see how this holds up for language modeling.</p>
          <h2 id="bibliography">Bibliography</h2>
          <ol class="biblio-list">
            <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="katharopoulosTransformersAreRNNs2020">
              <span itemprop="author">Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, François Fleuret</span>. <time datetime="2020" itemprop="datePublished">June 29, 2020</time>. "<span itemprop="name">Transformers Are Rnns: Fast Autoregressive Transformers with Linear Attention</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Arxiv:2006.16236 [cs, Stat]</i></span>. <a itemprop="sameAs" href="http://arxiv.org/abs/2006.16236">http://arxiv.org/abs/2006.16236</a>.
            </li>
          </ol>
        </div>
        <div class="text-center" style="font-variant-caps:all-small-caps">
          Last changed <a class="u-url" href="https://hugocisneros.com/notes/katharopoulostransformersarernns2020/"><time itemprop="datePublished" class="dt-published" datetime="2021-09-02T12:07:00+0200">02/09/2021</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
        </div>
      </div>
    </article>
    <h3>Comments</h3>
    <script data-isso="https://comment.hugocisneros.com/" data-isso-require-author="true" data-isso-vote="true" src="https://comment.hugocisneros.com/js/embed.min.js"></script>
    <section id="isso-thread"></section><br>
    <a href="/notes#katharopoulostransformersarernns2020"><b>← Back to Notes</b></a>
    <hr>
  </main>
  <footer class="footer container h-10 text-center">
    <ul class="pl-0">
      <li class="first:before:content-none before:content-['•'] inline-block list-none">
        <a class="rss-link inline-block text-neutral-800 dark:text-neutral-400 border-none" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon w-4 inline-block" src="https://hugocisneros.com/img/RSS.svg" alt="RSS feed icon"></a>
      </li>
      <li class="ml-2 first:before:content-none before:content-['•'] inline-block list-none">
        <a class="ml-2 text-neutral-800 dark:text-neutral-400 border-none" href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
      </li>
      <li class="ml-2 first:before:content-none before:content-['•'] text-neutral-800 dark:text-neutral-400 inline-block list-none"><span class="ml-2">© Hugo Cisneros 2022</span></li>
    </ul>
  </footer>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>
  document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})
  </script>
  <script data-goatcounter="https://stats.hugocisneros.com/count" async src="//stats.hugocisneros.com/count.js"></script>
</body>
</html>
