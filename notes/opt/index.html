<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
  <title>OPT: Open Pre-trained Transformer - Hugo Cisneros</title>
  <meta property="og:title" content="OPT: Open Pre-trained Transformer - Hugo Cisneros">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/opt/">
  <meta property="og:description" content="Notes about OPT: Open Pre-trained Transformer">
  <meta name="Description" property="description" content="Notes about OPT: Open Pre-trained Transformer">
  <link rel="me" href="https://twitter.com/@cisne_hug">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="me" href="https://scholar.social/@hugcis">
  <link rel="me" href="https://github.com/hugcis">
  <meta property="keywords" content="transformers, gpt, nlp">
  <link rel="stylesheet" href="https://hugocisneros.com/main.min.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/projects/">Projects</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
          <li>
            <a href="/contact/">Contact</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">OPT: Open Pre-trained Transformer</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>tags</dt>
              <dd>
                <a href="/notes/transformers/">Transformers</a>, <a href="/notes/gpt/">GPT</a>, <a href="/notes/nlp/">NLP</a>
              </dd>
              <dt>paper</dt>
              <dd>
                (<a href="#zhangOPTOpenPretrained2022"><cite itemprop="citation" itemscope itemid="key:zhangOPTOpenPretrained2022">Zhang et al. 2022</cite></a>)
              </dd>
            </dl>
            <h2 id="architecture">Architecture</h2>
            <p>It is the same architecture as <a href="/notes/gpt_3/">GPT-3</a> but with some training improvements from <a href="/notes/megatron/">Megatron</a>.</p>
            <h2 id="parameter-count">Parameter count</h2>
            <p>175B</p>
            <h2 id="bibliography">Bibliography</h2>
            <ol class="biblio-list">
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="zhangOPTOpenPretrained2022">
                <span itemprop="author">Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, et al.</span>. <time datetime="2022" itemprop="datePublished">June 21, 2022</time>. "<span itemprop="name">OPT: Open Pre-trained Transformer Language Models</span>". arXiv. <a itemprop="sameAs" href="http://arxiv.org/abs/2205.01068">http://arxiv.org/abs/2205.01068</a>.
              </li>
            </ol>
          </div>
          <div class="bl-section">
            <h2>Links to this note</h2>
            <div class="backlinks">
              <ul>
                <li>
                  <a href="/notes/alife_2020/">ALife 2020</a>
                </li>
                <li>
                  <a href="/notes/article_uncertain_times/">Article: Uncertain times</a>
                </li>
                <li>
                  <a href="/notes/automated_discovery_in_complex_systems/">Automated discovery in complex systems</a>
                </li>
                <li>
                  <a href="/notes/automatic_differentiation/">Automatic differentiation</a>
                </li>
                <li>
                  <a href="/notes/cellular_automata_as_cnns/">Cellular automata as convolutional neural networks</a>
                </li>
                <li>
                  <a href="/notes/chinchilla/">Chinchilla</a>
                </li>
                <li>
                  <a href="/notes/church_turing_thesis/">Church-Turing thesis</a>
                </li>
                <li>
                  <a href="/notes/complex_systems/">Complex Systems</a>
                </li>
                <li>
                  <a href="/notes/compression/">Compression</a>
                </li>
                <li>
                  <a href="/notes/distillation/">Distillation</a>
                </li>
                <li>
                  <a href="/notes/economic_liberalism/">Economic liberalism</a>
                </li>
                <li>
                  <a href="/notes/entropy/">Entropy</a>
                </li>
                <li>
                  <a href="/notes/evaluating_nlp/">Evaluating NLP</a>
                </li>
                <li>
                  <a href="/notes/genetic_algorithm/">Genetic algorithms</a>
                </li>
                <li>
                  <a href="/notes/gradient_descent/">Gradient descent</a>
                </li>
                <li>
                  <a href="/notes/gradient_descent_for_wide_two_layer_neural_networks_i_global_convergence/">Gradient descent for wide two-layer neural networks – I : Global convergence</a>
                </li>
                <li>
                  <a href="/notes/gradient_flow/">Gradient flow</a>
                </li>
                <li>
                  <a href="/notes/image_processing/">Image processing</a>
                </li>
                <li>
                  <a href="/notes/kullback_leibler_divergence/">Kullback-leibler divergence</a>
                </li>
                <li>
                  <a href="/notes/linear_programming/">Linear programming</a>
                </li>
                <li>
                  <a href="/notes/meta_learning/">Meta-learning</a>
                </li>
                <li>
                  <a href="/notes/neural_network_training/">Neural network training</a>
                </li>
                <li>
                  <a href="/notes/raimbaultmodelurbanevolution2020/">Notes on: A model of urban evolution based on innovation diffusion by Raimbault, J. (2020)</a>
                </li>
                <li>
                  <a href="/notes/cluneaigasaigeneratingalgorithms2019/">Notes on: AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence by Clune, J. (2019)</a>
                </li>
                <li>
                  <a href="/notes/hudrinkingfirehosecontinual2020/">Notes on: Drinking from a Firehose: Continual Learning with Web-scale Natural Language by Hu, H., Sener, O., Sha, F., & Koltun, V. (2020)</a>
                </li>
                <li>
                  <a href="/notes/stanleyevolvingneuralnetworks2002/">Notes on: Evolving Neural Networks through Augmenting Topologies by Stanley, K. O., & Miikkulainen, R. (2002)</a>
                </li>
                <li>
                  <a href="/notes/chnggarfgaussianactivated2022/">Notes on: GARF: Gaussian Activated Radiance Fields for High Fidelity Reconstruction and Pose Estimation by Chng, S., Ramasinghe, S., Sherrah, J., & Lucey, S. (2022)</a>
                </li>
                <li>
                  <a href="/notes/brooksintelligencerepresentation1991/">Notes on: Intelligence without representation by Brooks, R. A. (1991)</a>
                </li>
                <li>
                  <a href="/notes/tanciklearnedinitializationsoptimizing2021/">Notes on: Learned Initializations for Optimizing Coordinate-Based Neural Representations by Tancik, M., Mildenhall, B., Wang, T., Schmidt, D., Srinivasan, P. P., Barron, J. T., & Ng, R. (2021)</a>
                </li>
                <li>
                  <a href="/notes/yenetworkdeconvolution2020/">Notes on: Network Deconvolution by Ye, C., Evanusa, M., He, H., Mitrokhin, A., Goldstein, T., Yorke, J. A., Fermuller, Cornelia, … (2020)</a>
                </li>
                <li>
                  <a href="/notes/yangonemodellearning2022/">Notes on: One model for the learning of language by Yang, Y., & Piantadosi, S. T. (2022)</a>
                </li>
                <li>
                  <a href="/notes/wangpoetopenendedcoevolution2019/">Notes on: POET: open-ended coevolution of environments and their optimized solutions by Wang, R., Lehman, J., Clune, J., & Stanley, K. O. (2019)</a>
                </li>
                <li>
                  <a href="/notes/aitkengeometryintegrationtext2020/">Notes on: The geometry of integration in text classification RNNs by Aitken, K., Ramasesh, V. V., Garg, A., Cao, Y., Sussillo, D., & Maheswaranathan, N. (2020)</a>
                </li>
                <li>
                  <a href="/notes/optimal_control/">Optimal control</a>
                </li>
                <li>
                  <a href="/notes/ordinary_least_squares/">Ordinary least squares</a>
                </li>
                <li>
                  <a href="/notes/pattern_defeating_quicksort/">Pattern-defeating quicksort</a>
                </li>
                <li>
                  <a href="/notes/program_synthesis/">Program synthesis</a>
                </li>
                <li>
                  <a href="/notes/projection_on_convex_sets/">Projection on convex sets</a>
                </li>
                <li>
                  <a href="/notes/pytorch/">Pytorch</a>
                </li>
                <li>
                  <a href="/notes/roberta/">RoBERTa</a>
                </li>
                <li>
                  <a href="/notes/sed_utility/">Sed utility</a>
                </li>
                <li>
                  <a href="/notes/statistical_complexity/">Statistical complexity</a>
                </li>
                <li>
                  <a href="/notes/supervised_learning/">Supervised learning</a>
                </li>
                <li>
                  <a href="/notes/talk_differentiation_of_black_box_combinatorial_solvers/">Talk: Differentiation of black-box combinatorial solvers</a>
                </li>
                <li>
                  <a href="/notes/talk_the_importance_of_open_endedness_in_ai_and_machine_learning/">Talk: The Importance of Open-Endedness in AI and Machine Learning</a>
                </li>
                <li>
                  <a href="/notes/the_meta_problem_of_consciousness_with_david_chalmers/">The Meta-Problem of Consciousness with David Chalmers</a>
                </li>
                <li>
                  <a href="/notes/turing_machine/">Turing Machine</a>
                </li>
                <li>
                  <a href="/notes/turing_nlg/">Turing-NLG</a>
                </li>
                <li>
                  <a href="/notes/why_programming_is_a_good_medium/">Why programming is a good medium for expressing poorly understood and sloppily-formulated ideas</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/opt/"><time itemprop="datePublished" class="dt-published" datetime="2022-07-27T10:40:00+0200">27/07/2022</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article>
      <h3>Comments</h3>
      <script data-isso="https://comment.hugocisneros.com/" data-isso-require-author="true" data-isso-vote="true" src="https://comment.hugocisneros.com/js/embed.min.js"></script>
      <section id="isso-thread"></section><br>
      <a href="/notes#opt"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
        </li>
        <li>© Hugo Cisneros 2022</li>
      </ul>
    </footer>
  </div>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>
  document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})
  </script>
  <script data-goatcounter="https://stats.hugocisneros.com/count" async src="//stats.hugocisneros.com/count.js"></script>
</body>
</html>
