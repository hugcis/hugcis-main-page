<!DOCTYPE html>
<html lang="en-us">
<head>


<meta charset="utf-8">
<meta name="viewport" content=
"width=device-width,initial-scale=1.0,minimum-scale=1">
<title>Neural networks - Hugo Cisneros - Personal page</title>
<meta property="og:title" content=
"Neural networks - Hugo Cisneros - Personal page">
<meta property="og:type" content="article">
<meta property="og:image" content="/img/main.jpeg">
<meta property="og:url" content=
"https://hugocisneros.com/notes/neural_networks/">
<meta property="og:description" content=
"Notes about Neural networks">
<meta name="Description" property="description" content=
"Notes about Neural networks">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@cisne_hug">
<meta name="twitter:creator" content="@cisne_hug">
<link rel="stylesheet" href=
"https://hugocisneros.com/css/main.min.2748d42cc00e1d64a84f8ba0e8e381dea8a40ce5a09292f5c308dd35fc373bd5.css"
media="all" type="text/css">
<link rel="apple-touch-icon" sizes="180x180" href=
"/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href=
"/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href=
"/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color=
"#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<link rel="webmention" href=
"https://webmention.io/hugocisneros.com/webmention">
<link rel="pingback" href=
"https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
<div class="wrapper">
<header class="header">
<nav class="nav">
<div class="nav-main"><a href="https://hugocisneros.com/" class=
"nav-title">Hugo Cisneros - Personal page</a></div>
<ul class="nav-links">
<li><a href="/about/">About</a></li>
<li><a href="/blog/">Blog</a></li>
<li><a href="/notes/">Notes</a></li>
<li><a href="/resume/">Resume</a></li>
</ul>
</nav>
</header>
<main class="content" role="main">
<article class="article h-entry" itemprop="mainEntity" itemscope
itemtype="http://schema.org/BlogPosting">
<div class="single-note note-container">
<h1 class="article-title p-name" itemprop="name">Neural
networks</h1>
<div class="article-content e-content p-name" itemprop=
"articleBody">
<dl>
<dt>tags</dt>
<dd><a href="/notes/machine_learning/">Machine learning</a></dd>
</dl>
<h2 id="two-layers-neural-network">Two-layers neural network</h2>
<p>Mathematically, a simple two-layers neural network with relu
non-linearities can be written like below. For an input vector \(x
\in \mathbb{R}^D\), \(\mathbf{a} = (a_1, \cdots, a_N)\in
\mathbb{R}^M\) are the <em>output weights</em>, \(\mathbf{b} =
(b_1, \cdots, b_N)\in \mathbb{R}^D\) are the <em>input
weights</em></p>
<p>\[ h(x) = \frac{1}{m} \sum_{i=1}^m a_i \max\{ b_i^\top x,0\},
\]</p>
<h2 id="universal-approximation-theorem">Universal approximation
theorem</h2>
<p>Cybenko showed in 1989 that a neural network of arbitrary width
with sigmoid activation function could approximate any continuous
function (<a href="#orgc906c9a">Cybenko 1989</a>).</p>
<p>Barron added rates of convergence by enforcing smoothness
condition on the target function (<a href="#orgf290f7d">Barron
1993</a>).</p>
<h2 id="bibliography">Bibliography</h2>
<p><a id="orgf290f7d"></a>Barron, A. R. 1993. “Universal
Approximation Bounds for Superpositions of a Sigmoidal Function.”
<em>IEEE Transactions on Information Theory</em> 39 (3):930–45.</p>
<p><a id="orgc906c9a"></a>Cybenko, G. 1989. “Approximation by
Superpositions of a Sigmoidal Function.” <em>Mathematics of
Control, Signals, and Systems</em> 2 (4):303–14.</p>
</div>
<div>
<div class="bl-section">
<h2>Links to this note</h2>
<div class="backlinks">
<ul>
<li><a href="/notes/adaptive_computation_time/">Adaptive
Computation Time</a></li>
<li><a href="/notes/adversarial_examples/">Adversarial
examples</a></li>
<li><a href="/notes/attention/">Attention</a></li>
<li><a href="/notes/autoencoders/">Autoencoders</a></li>
<li><a href="/notes/backward_rnn/">Backward RNN</a></li>
<li><a href="/notes/cellular_automata_as_cnns/">Cellular automata
as CNNs</a></li>
<li><a href="/notes/cellular_neural_networks/">Cellular neural
networks</a></li>
<li><a href="/notes/complex_systems/">Complex Systems</a></li>
<li><a href="/notes/compression/">Compression</a></li>
<li><a href="/notes/convolutional_neural_networks/">Convolutional
neural networks</a></li>
<li><a href="/notes/cppn/">CPPN</a></li>
<li><a href="/notes/data_representation/">Data
representation</a></li>
<li><a href="/notes/echo_state_networks/">Echo-state
networks</a></li>
<li><a href="/notes/generative_adversarial_networks/">Generative
adversarial networks</a></li>
<li><a href=
"/notes/gradient_descent_for_wide_two_layer_neural_networks_i_global_convergence/">
Gradient descent for wide two-layer neural networks – I : Global
convergence</a></li>
<li><a href="/notes/graph_convolutional_networks/">Graph
convolutional networks</a></li>
<li><a href="/notes/graph_neural_networks/">Graph neural
networks</a></li>
<li><a href="/notes/hopfield_networks/">Hopfield Networks</a></li>
<li><a href="/notes/implicit_neural_representations/">Implicit
neural representations</a></li>
<li><a href="/notes/language_modeling/">Language modeling</a></li>
<li><a href="/notes/mean_field_theory_of_neural_networks/">Mean
field theory of neural networks (talk)</a></li>
<li><a href="/notes/neural_architecture_search/">Neural
architecture search</a></li>
<li><a href="/notes/neural_network_pruning/">Neural network
pruning</a></li>
<li><a href="/notes/neural_network_training/">Neural network
training</a></li>
<li><a href="/notes/neural_tangent_kernel/">Neural tangent
kernel</a></li>
<li><a href="/notes/tutumadaptingunseenenvironments2020/">Notes on:
Adapting to Unseen Environments through Explicit Representation of
Context by Tutum, C., & Miikkulainen, R. (2020)</a></li>
<li><a href="/notes/benderclimbingnlumeaning2020/">Notes on:
Climbing towards NLU: On Meaning, Form, and Understanding in the
Age of Data by Bender, E. M., & Koller, A. (2020)</a></li>
<li><a href=
"/notes/pathakcuriositydrivenexplorationselfsupervised2017/">Notes
on: Curiosity-Driven Exploration by Self-Supervised Prediction by
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T.
(2017)</a></li>
<li><a href="/notes/hudrinkingfirehosecontinual2020/">Notes on:
Drinking from a Firehose: Continual Learning with Web-scale Natural
Language by Hu, H., Sener, O., Sha, F., & Koltun, V.
(2020)</a></li>
<li><a href="/notes/stanleyevolvingneuralnetworks2002/">Notes on:
Evolving Neural Networks through Augmenting Topologies by Stanley,
K. O., & Miikkulainen, R. (2002)</a></li>
<li><a href="/notes/voelkerlegendrememoryunits2019/">Notes on:
Legendre Memory Units: Continuous-Time Representation in Recurrent
Neural Networks by Voelker, A., Kajić, I., & Eliasmith, C.
(2019)</a></li>
<li><a href="/notes/wierstramodelingsystemsinternal2005/">Notes on:
Modeling systems with internal state using evolino by Wierstra, D.,
Gomez, F. J., & Schmidhuber, J. (2005)</a></li>
<li><a href=
"/notes/maziarkamoleculeattentiontransformer2020/">Notes on:
Molecule Attention Transformer by Maziarka, Ł., Danel, T., Mucha,
S., Rataj, K., Tabor, J., & Jastrzębski, S. (2020)</a></li>
<li><a href="/notes/yenetworkdeconvolution2020/">Notes on: Network
Deconvolution by Ye, C., Evanusa, M., He, H., Mitrokhin, A.,
Goldstein, T., Yorke, J. A., Fermuller, Cornelia, … (2020)</a></li>
<li><a href="/notes/zophneuralarchitecturesearch2017/">Notes on:
Neural Architecture Search with Reinforcement Learning by Zoph, B.,
& Le, Q. V. (2017)</a></li>
<li><a href="/notes/lechnerneuralcircuitpolicies2020/">Notes on:
Neural Circuit Policies Enabling Auditable Autonomy by Lechner, M.,
Hasani, R., Amini, A., Henzinger, T. A., Rus, D., & Grosu, R.
(2020)</a></li>
<li><a href="/notes/greydanusscalingdeeplearning2020/">Notes on:
Scaling down Deep Learning by Greydanus, S. (2020)</a></li>
<li><a href="/notes/aitkengeometryintegrationtext2020/">Notes on:
The geometry of integration in text classification RNNs by Aitken,
K., Ramasesh, V. V., Garg, A., Cao, Y., Sussillo, D., &
Maheswaranathan, N. (2020)</a></li>
<li><a href="/notes/katharopoulostransformersarernns2020/">Notes
on: Transformers are RNNs: Fast Autoregressive Transformers with
Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., &
Fleuret, F. (2020)</a></li>
<li><a href="/notes/recurrent_neural_networks/">Recurrent neural
networks</a></li>
<li><a href="/notes/self_replication/">Self-replication</a></li>
<li><a href=
"/notes/talk_artificial_intelligence_a_guide_for_thinking_humans/">Talk:
Artificial Intelligence: A Guide for Thinking Humans</a></li>
<li><a href=
"/notes/talk_the_importance_of_open_endedness_in_ai_and_machine_learning/">
Talk: The Importance of Open-Endedness in AI and Machine
Learning</a></li>
<li><a href="/notes/the_lottery_ticket_hypothesis/">The Lottery
ticket hypothesis</a></li>
<li><a href="/notes/transformers/">Transformers</a></li>
<li><a href="/notes/variational_autoencoders/">Variational
autoencoders</a></li>
</ul>
</div>
</div>
</div>
<div class="note-footer">Last changed <a class="u-url" href=
"https://hugocisneros.com/notes/neural_networks/"><time itemprop=
"datePublished" class="dt-published" datetime=
"2020-12-02T09:42:00+0100">02/12/2020</time></a> | authored by
<a href="https://hugocisneros.com/" rel="author" class=
"p-author h-card" itemprop="author" itemscope itemtype=
"http://schema.org/Person"><span itemprop="name">Hugo
Cisneros</span></a></div>
</div>
</article>
<br>
<a href="/notes#neural_networks"><b>← Back to Notes</b></a>
<hr></main>
<footer class="footer">
<ul class="footer-links">
<li><a class="rss-link" href="/blog/index.xml" type=
"application/rss+xml" target="_blank">Blog <img class="rss-icon"
src="/img/RSS.svg" alt="RSS feed icon"></a></li>
<li><a href=
"https://github.com/hugcis/natrium-custom">Code</a></li>
<li>© Hugo Cisneros 2020</li>
</ul>
</footer>
</div>
<script>
 MathJax = {
     tex: {
         inlineMath: [['$','$'], ['\\(', '\\)']],
         tags: 'ams'
     }
 };
</script> 
<script type="text/javascript" rel="preconnect" id="MathJax-script"
async src=
"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
