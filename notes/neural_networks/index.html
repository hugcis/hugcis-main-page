<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1">
  <title>Neural networks - Hugo Cisneros - Personal page</title>
  <meta property="og:title" content="Neural networks - Hugo Cisneros - Personal page">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/neural_networks/">
  <meta property="og:description" content="Notes about Neural networks">
  <meta name="Description" property="description" content="Notes about Neural networks">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="stylesheet" href="https://hugocisneros.com/css/main.min.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros - Personal page</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/projects/">Projects</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
          <li>
            <a href="/contact/">Contact</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">Neural networks</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>tags</dt>
              <dd>
                <a href="/notes/machine_learning/">Machine learning</a>
              </dd>
            </dl>
            <h2 id="two-layers-neural-network">Two-layers neural network</h2>
            <p>Mathematically, a simple two-layers neural network with relu non-linearities can be written like below. For an input vector \(x \in \mathbb{R}^D\), \(\mathbf{a} = (a_1, \cdots, a_N)\in \mathbb{R}^M\) are the <em>output weights</em>, \(\mathbf{b} = (b_1, \cdots, b_N)\in \mathbb{R}^D\) are the <em>input weights</em></p>
            <p>\[ h(x) = \frac{1}{m} \sum_{i=1}^m a_i \max\{ b_i^\top x,0\}, \]</p>
            <h2 id="universal-approximation-theorem">Universal approximation theorem</h2>
            <p>Cybenko showed in 1989 that a neural network of arbitrary width with sigmoid activation function could approximate any continuous function (<a href="#orge1315dd">Cybenko 1989</a>).</p>
            <p>Barron added rates of convergence by enforcing smoothness condition on the target function (<a href="#orgbcb551c">Barron 1993</a>).</p>
            <h2 id="bibliography">Bibliography</h2>
            <ol class="biblio-list">
              <a id="orgbcb551c"></a>
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle">
                <span itemprop="author">Barron, A. R.</span>. <span datetime="1993" itemprop="datePublished">May 1993</span>. "<span itemprop="name">Universal Approximation Bounds for Superpositions of a Sigmoidal Function</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">IEEE Transactions on Information Theory</i></span> 39 (3):930–45. <a itemprop="sameAs" href="https://doi.org/10.1109/18.256500">DOI</a>.
                <p><a id="orge1315dd"></a></p>
              </li>
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle">
                <span itemprop="author">Cybenko, G.</span>. <span datetime="1989" itemprop="datePublished">December 1989</span>. “<span itemprop="name">Approximation by Superpositions of a Sigmoidal Function</span>”. <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Mathematics of Control, Signals, and Systems</i></span> 2 (4):303–14. <a itemprop="sameAs" href="https://doi.org/10.1007/BF02551274">DOI</a>.
              </li>
            </ol>
          </div>
          <div class="bl-section">
            <h2>Links to this note</h2>
            <div class="backlinks">
              <ul>
                <li>
                  <a href="/notes/adaptive_computation_time/">Adaptive Computation Time</a>
                </li>
                <li>
                  <a href="/notes/adversarial_examples/">Adversarial examples</a>
                </li>
                <li>
                  <a href="/notes/attention/">Attention</a>
                </li>
                <li>
                  <a href="/notes/attention_graph_networks/">Attention graph networks</a>
                </li>
                <li>
                  <a href="/notes/attractor_networks/">Attractor networks</a>
                </li>
                <li>
                  <a href="/notes/autoencoders/">Autoencoders</a>
                </li>
                <li>
                  <a href="/notes/backward_rnn/">Backward RNN</a>
                </li>
                <li>
                  <a href="/notes/cellular_automata_as_cnns/">Cellular automata as convolutional neural networks</a>
                </li>
                <li>
                  <a href="/notes/cellular_automata_as_recurrent_neural_networks/">Cellular automata as recurrent neural networks</a>
                </li>
                <li>
                  <a href="/notes/cellular_neural_networks/">Cellular neural networks</a>
                </li>
                <li>
                  <a href="/notes/complex_systems/">Complex Systems</a>
                </li>
                <li>
                  <a href="/notes/compression/">Compression</a>
                </li>
                <li>
                  <a href="/notes/convolutional_neural_networks/">Convolutional neural networks</a>
                </li>
                <li>
                  <a href="/notes/conway_s_game_of_life/">Conway's Game of Life</a>
                </li>
                <li>
                  <a href="/notes/cppn/">CPPN</a>
                </li>
                <li>
                  <a href="/notes/data_representation/">Data representation</a>
                </li>
                <li>
                  <a href="/notes/distillation/">Distillation</a>
                </li>
                <li>
                  <a href="/notes/echo_state_networks/">Echo-state networks</a>
                </li>
                <li>
                  <a href="/notes/generative_adversarial_networks/">Generative adversarial networks</a>
                </li>
                <li>
                  <a href="/notes/gradient_descent_for_wide_two_layer_neural_networks_i_global_convergence/">Gradient descent for wide two-layer neural networks – I : Global convergence</a>
                </li>
                <li>
                  <a href="/notes/graph_cellular_automata/">Graph cellular automata</a>
                </li>
                <li>
                  <a href="/notes/graph_convolutional_networks/">Graph convolutional networks</a>
                </li>
                <li>
                  <a href="/notes/graph_neural_networks/">Graph neural networks</a>
                </li>
                <li>
                  <a href="/notes/hopfield_networks/">Hopfield Networks</a>
                </li>
                <li>
                  <a href="/notes/implicit_neural_representations/">Implicit neural representations</a>
                </li>
                <li>
                  <a href="/notes/language_modeling/">Language modeling</a>
                </li>
                <li>
                  <a href="/notes/mean_field_theory_of_neural_networks/">Mean field theory of neural networks (talk)</a>
                </li>
                <li>
                  <a href="/notes/message_passing_graph_networks/">Message-passing graph networks</a>
                </li>
                <li>
                  <a href="/notes/meta_learning/">Meta-learning</a>
                </li>
                <li>
                  <a href="/notes/neural_architecture_search/">Neural architecture search</a>
                </li>
                <li>
                  <a href="/notes/neural_network_pruning/">Neural network pruning</a>
                </li>
                <li>
                  <a href="/notes/neural_network_training/">Neural network training</a>
                </li>
                <li>
                  <a href="/notes/neural_networks_as_dynamical_systems/">Neural networks as dynamical systems</a>
                </li>
                <li>
                  <a href="/notes/neural_tangent_kernel/">Neural tangent kernel</a>
                </li>
                <li>
                  <a href="/notes/tutumadaptingunseenenvironments2020/">Notes on: Adapting to Unseen Environments through Explicit Representation of Context by Tutum, C., & Miikkulainen, R. (2020)</a>
                </li>
                <li>
                  <a href="/notes/benderclimbingnlumeaning2020/">Notes on: Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data by Bender, E. M., & Koller, A. (2020)</a>
                </li>
                <li>
                  <a href="/notes/pathakcuriositydrivenexplorationselfsupervised2017/">Notes on: Curiosity-Driven Exploration by Self-Supervised Prediction by Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017)</a>
                </li>
                <li>
                  <a href="/notes/hudrinkingfirehosecontinual2020/">Notes on: Drinking from a Firehose: Continual Learning with Web-scale Natural Language by Hu, H., Sener, O., Sha, F., & Koltun, V. (2020)</a>
                </li>
                <li>
                  <a href="/notes/stanleyevolvingneuralnetworks2002/">Notes on: Evolving Neural Networks through Augmenting Topologies by Stanley, K. O., & Miikkulainen, R. (2002)</a>
                </li>
                <li>
                  <a href="/notes/aachgeneralizationdifferentcellular2021/">Notes on: Generalization over different cellular automata rules learned by a deep feed-forward neural network by Aach, M., Goebbert, J. H., & Jitsev, J. (2021)</a>
                </li>
                <li>
                  <a href="/notes/voelkerlegendrememoryunits2019/">Notes on: Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks by Voelker, A., Kajić, I., & Eliasmith, C. (2019)</a>
                </li>
                <li>
                  <a href="/notes/wierstramodelingsystemsinternal2005/">Notes on: Modeling systems with internal state using evolino by Wierstra, D., Gomez, F. J., & Schmidhuber, J. (2005)</a>
                </li>
                <li>
                  <a href="/notes/maziarkamoleculeattentiontransformer2020/">Notes on: Molecule Attention Transformer by Maziarka, Ł., Danel, T., Mucha, S., Rataj, K., Tabor, J., & Jastrzębski, S. (2020)</a>
                </li>
                <li>
                  <a href="/notes/yenetworkdeconvolution2020/">Notes on: Network Deconvolution by Ye, C., Evanusa, M., He, H., Mitrokhin, A., Goldstein, T., Yorke, J. A., Fermuller, Cornelia, … (2020)</a>
                </li>
                <li>
                  <a href="/notes/zophneuralarchitecturesearch2017/">Notes on: Neural Architecture Search with Reinforcement Learning by Zoph, B., & Le, Q. V. (2017)</a>
                </li>
                <li>
                  <a href="/notes/lechnerneuralcircuitpolicies2020/">Notes on: Neural Circuit Policies Enabling Auditable Autonomy by Lechner, M., Hasani, R., Amini, A., Henzinger, T. A., Rus, D., & Grosu, R. (2020)</a>
                </li>
                <li>
                  <a href="/notes/lupretrainedtransformersuniversal2021/">Notes on: Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021)</a>
                </li>
                <li>
                  <a href="/notes/greydanusscalingdeeplearning2020/">Notes on: Scaling down Deep Learning by Greydanus, S. (2020)</a>
                </li>
                <li>
                  <a href="/notes/aitkengeometryintegrationtext2020/">Notes on: The geometry of integration in text classification RNNs by Aitken, K., Ramasesh, V. V., Garg, A., Cao, Y., Sussillo, D., & Maheswaranathan, N. (2020)</a>
                </li>
                <li>
                  <a href="/notes/weissthinkingtransformers2021/">Notes on: Thinking Like Transformers by Weiss, G., Goldberg, Y., & Yahav, E. (2021)</a>
                </li>
                <li>
                  <a href="/notes/katharopoulostransformersarernns2020/">Notes on: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020)</a>
                </li>
                <li>
                  <a href="/notes/recurrent_neural_networks/">Recurrent neural networks</a>
                </li>
                <li>
                  <a href="/notes/reinforcement_learning/">Reinforcement learning</a>
                </li>
                <li>
                  <a href="/notes/reservoir_computing/">Reservoir computing</a>
                </li>
                <li>
                  <a href="/notes/self_replication/">Self-replication</a>
                </li>
                <li>
                  <a href="/notes/style_transfer/">Style transfer</a>
                </li>
                <li>
                  <a href="/notes/talk_artificial_intelligence_a_guide_for_thinking_humans/">Talk: Artificial Intelligence: A Guide for Thinking Humans</a>
                </li>
                <li>
                  <a href="/notes/talk_the_importance_of_open_endedness_in_ai_and_machine_learning/">Talk: The Importance of Open-Endedness in AI and Machine Learning</a>
                </li>
                <li>
                  <a href="/notes/the_bitter_lesson/">The Bitter Lesson</a>
                </li>
                <li>
                  <a href="/notes/the_lottery_ticket_hypothesis/">The Lottery ticket hypothesis</a>
                </li>
                <li>
                  <a href="/notes/transformers/">Transformers</a>
                </li>
                <li>
                  <a href="/notes/variational_autoencoders/">Variational autoencoders</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/neural_networks/"><time itemprop="datePublished" class="dt-published" datetime="2020-12-02T09:42:00+0100">02/12/2020</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article><br>
      <a href="/notes#neural_networks"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/natrium-custom">Code</a>
        </li>
        <li>© Hugo Cisneros 2021</li>
      </ul>
    </footer>
  </div>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>

    document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "\\[", right: "\\]", display: true},
            {left: "$$", right: "$$", display: true},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false},
        ]
  })
    });
  </script>
</body>
</html>
