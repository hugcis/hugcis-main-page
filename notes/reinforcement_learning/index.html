<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1">
  <title>Reinforcement learning - Hugo Cisneros</title>
  <meta property="og:title" content="Reinforcement learning - Hugo Cisneros">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/reinforcement_learning/">
  <meta property="og:description" content="Notes about Reinforcement learning">
  <meta name="Description" property="description" content="Notes about Reinforcement learning">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="stylesheet" href="https://hugocisneros.com/main.min.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/projects/">Projects</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
          <li>
            <a href="/contact/">Contact</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">Reinforcement learning</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>tags</dt>
              <dd>
                <a href="/notes/machine_learning/">Machine learning</a>
              </dd>
            </dl>
            <p>In reinforcement learning, agents take actions within an environment. Usually, both the agent and environment states change in reaction to this action. A reward is given to the agent to tell it if the action was positive or negative.</p>
            <p>The goal of a learning agent is to act so as to maximize that reward.</p>
            <p>An agent can be anything from a fixed set of <code>if-else</code> statements to a deep <a href="/notes/neural_networks/">neural network</a>.</p>
            <h2 id="algorithms">Algorithms</h2>
            <h3 id="q-learning">Q-learning</h3>
            <h3 id="a3c">A3C</h3>
            <h3 id="trpo">TRPO</h3>
            <h3 id="ppo">PPO</h3>
            <h3 id="sac">SAC</h3>
            <h3 id="evolutionary-strategies--genetic-algorithm-dot-md--in-rl"><a href="/notes/genetic_algorithm/">Evolutionary strategies</a> in RL</h3>
            <p>A survey of evolutionary strategies for RL (<a href="#mullerChallengesHighdimensionalReinforcement2018"><cite itemprop="citation" itemscope itemid="key:mullerChallengesHighdimensionalReinforcement2018">Müller and Glasmachers 2018</cite></a>).</p>
            <h3 id="other-misc-algorithms-hacks-and-tricks">Other/Misc algorithms, hacks and tricks</h3>
            <p>Current RL is full of tricks to make the algorithms behave the way we want them to. It is not clear if the algorithms are getting better overall thanks to that collection of tricks or if this makes them over-specialized for a particular type of application.</p>
            <h4 id="exploration-bonuses">Exploration bonuses</h4>
            <p>Exploration bonuses are a class of methods that encourage an agent to explore even when the environment reward is sparse. This is done by adding an extra reward term. This may help an agent explore more states that are visually different from the ones before, or with different histories, etc.</p>
            <p>An example of exploration bonus using random network <a href="/notes/distillation/">distillation</a> (<a href="#burdaExplorationRandomNetwork2018"><cite itemprop="citation" itemscope itemid="key:burdaExplorationRandomNetwork2018">Burda et al. 2018</cite></a>).</p>
            <h2 id="bibliography">Bibliography</h2>
            <ol class="biblio-list">
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="mullerChallengesHighdimensionalReinforcement2018">
                <span itemprop="author">Müller, Nilsand Tobias Glasmachers</span>. <time datetime="2018" itemprop="datePublished">July 1, 2018</time>. "<span itemprop="name">Challenges in High-dimensional Reinforcement Learning with Evolution Strategies</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Arxiv:1806.01224 [cs]</i></span>. <a itemprop="sameAs" href="http://arxiv.org/abs/1806.01224">http://arxiv.org/abs/1806.01224</a>.
              </li>
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="burdaExplorationRandomNetwork2018"><span itemprop="author">Burda, Yuri, Harrison Edwards, Amos Storkeyand Oleg Klimov</span>. <time datetime="2018" itemprop="datePublished">2018</time>. "<span itemprop="name">Exploration by Random Network Distillation</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Arxiv Preprint Arxiv:1810.12894</i></span>.</li>
            </ol>
          </div>
          <div class="bl-section">
            <h2>Links to this note</h2>
            <div class="backlinks">
              <ul>
                <li>
                  <a href="/notes/adversarial_examples/">Adversarial examples</a>
                </li>
                <li>
                  <a href="/notes/intrinsic_motivation/">Intrinsic motivation</a>
                </li>
                <li>
                  <a href="/notes/language_modeling/">Language modeling</a>
                </li>
                <li>
                  <a href="/notes/map_elites/">MAP-Elites</a>
                </li>
                <li>
                  <a href="/notes/neural_architecture_search/">Neural architecture search</a>
                </li>
                <li>
                  <a href="/notes/tutumadaptingunseenenvironments2020/">Notes on: Adapting to Unseen Environments through Explicit Representation of Context by Tutum, C., & Miikkulainen, R. (2020)</a>
                </li>
                <li>
                  <a href="/notes/pathakcuriositydrivenexplorationselfsupervised2017/">Notes on: Curiosity-Driven Exploration by Self-Supervised Prediction by Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017)</a>
                </li>
                <li>
                  <a href="/notes/khalifapcgrlproceduralcontent2020/">Notes on: PCGRL: Procedural Content Generation via Reinforcement Learning by Khalifa, A., Bontrager, P., Earle, S., & Togelius, J. (2020)</a>
                </li>
                <li>
                  <a href="/notes/wangpoetopenendedcoevolution2019/">Notes on: POET: open-ended coevolution of environments and their optimized solutions by Wang, R., Lehman, J., Clune, J., & Stanley, K. O. (2019)</a>
                </li>
                <li>
                  <a href="/notes/horiberegeneratingsoftrobots2021/">Notes on: Regenerating Soft Robots through Neural Cellular Automata by Horibe, K., Walker, K., & Risi, S. (2021)</a>
                </li>
                <li>
                  <a href="/notes/grbicsafereinforcementlearning2020/">Notes on: Safe Reinforcement Learning through Meta-learned Instincts by Grbic, D., & Risi, S. (2020)</a>
                </li>
                <li>
                  <a href="/notes/quality_diversity/">Quality diversity</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/reinforcement_learning/"><time itemprop="datePublished" class="dt-published" datetime="2021-06-14T21:54:00+0200">14/06/2021</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article>
      <h3>Comments</h3>
      <script data-isso="//comment.hugocisneros.com/" data-isso-require-author="true" data-isso-vote="true" src="//comment.hugocisneros.com/js/embed.min.js"></script>
      <section id="isso-thread"></section><br>
      <a href="/notes#reinforcement_learning"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
        </li>
        <li>© Hugo Cisneros 2022</li>
      </ul>
    </footer>
  </div>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>

    document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "\\[", right: "\\]", display: true},
            {left: "$$", right: "$$", display: true},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false},
        ]
  })
    });
  </script>
</body>
</html>
