<!DOCTYPE html>
<html lang="en-us">
<head>


<meta charset="utf-8">
<meta name="viewport" content=
"width=device-width,initial-scale=1.0,minimum-scale=1">
<title>Notes on: Mordvintsev, A., Randazzo, E., Niklasson, E., &
Levin, M. (2020): Growing Neural Cellular Automata - Hugo Cisneros
- Personal page</title>
<meta property="og:title" content=
"Notes on: Mordvintsev, A., Randazzo, E., Niklasson, E., &amp; Levin, M. (2020): Growing Neural Cellular Automata - Hugo Cisneros - Personal page">
<meta property="og:type" content="article">
<meta property="og:image" content="/img/main.jpeg">
<meta property="og:url" content=
"https://hugocisneros.com/notes/mordvintsevgrowingneuralcellular2020/">
<meta property="og:description" content=
"Notes about Mordvintsev, A., Randazzo, E., Niklasson, E., &amp; Levin, M. (2020): Growing Neural Cellular Automata">
<meta name="Description" property="description" content=
"Notes about Mordvintsev, A., Randazzo, E., Niklasson, E., &amp; Levin, M. (2020): Growing Neural Cellular Automata">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@cisne_hug">
<meta name="twitter:creator" content="@cisne_hug">
<link rel="stylesheet" href=
"https://hugocisneros.com/css/main.min.441eebc8829fa32ca1cc41ed23bc40ffdc668c3678d374d81e3bfc8123466c7c.css"
media="all" type="text/css">
<link rel="apple-touch-icon" sizes="180x180" href=
"/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href=
"/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href=
"/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color=
"#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
<link rel="webmention" href=
"https://webmention.io/hugocisneros.com/webmention">
<link rel="pingback" href=
"https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
<div class="wrapper">
<header class="header">
<nav class="nav">
<div class="nav-main"><a href="https://hugocisneros.com/" class=
"nav-title">Hugo Cisneros - Personal page</a></div>
<ul class="nav-links">
<li><a href="/about/">About</a></li>
<li><a href="/blog/">Blog</a></li>
<li><a href="/other/">Other</a></li>
<li><a href="/resume/cv.pdf">Resume</a></li>
</ul>
</nav>
</header>
<main class="content" role="main">
<article class="article h-entry" itemprop="mainEntity" itemscope
itemtype="http://schema.org/BlogPosting">
<div class="single-note note-container">
<h1 class="article-title p-name" itemprop="name">Mordvintsev, A.,
Randazzo, E., Niklasson, E., & Levin, M. (2020): Growing Neural
Cellular Automata</h1>
<div class="article-content e-content p-name" itemprop=
"articleBody">
<dl>
<dt>tags</dt>
<dd><a href="/notes/cellular_automata/">Cellular automata</a></dd>
<dt>source</dt>
<dd><a id="b71c53b928ac63b5999b34d6c7621b33" href=
"#mordvintsevGrowingNeuralCellular2020">(Mordvintsev et al.,
2020)</a></dd>
</dl>
<h2 id="summary">Summary</h2>
<p>This paper introduces interesting ideas for training <a href=
"/notes/cellular_automata_as_cnns/">cellular automata as CNNs</a>
to have self-repairing stable structures. The automata have 16
dimensional continuous states. The main modeling ideas are:</p>
<ul>
<li>Use hard-coded filters for the initial perception step. The
filters are Sobel convolutions and those two are concatenated with
the current state.</li>
<li>Update rules are then 1D convolutions applied to the \(3 * 16 =
48\) dimensional state vector. They use a neural network with
dimensions 40 -&gt; 128 -&gt; 16.</li>
<li>States are updated with probability .5 only, making them highly
asynchronous.</li>
<li>One of the 16 channels is an \(\alpha\) channel that determines
whether the cell is <em>alive</em> or <em>dead</em>. The threshold
is set to 0.1 for setting a cell to alive. <em>dead</em> cells have
their state manually set to 0 at each step.</li>
</ul>
<p>The authors then apply several training tricks to make the
patterns more robust, self-repairing, etc.</p>
<h2 id="comments">Comments</h2>
<p>I find the paper quite interesting, especially with its take on
CA update as CNN. The fixed convolutions restricts the possible
rules while enabling a more stable search process probably.</p>
<p>Some of the modeling ideas such as the size of the downstream
neural in 1D convolutions and the asynchronous updates aren’t
really justified clearly. The final quantization step to make the
whole thing work in browsers is particularly interesting to me: the
end up with a CA that has \(16 * 8 = 128\) bits states. Or maybe a
120 bits states and 8 bits alive/dead semi-independent state. This
is something like \(10^36\) states which is many orders of
magnitude larger than my experiments.</p>
<p>The very last paragraph I particularly like:</p>
<blockquote>
<p>Engineering and machine learning</p>
<p>The models described in this article run on the powerful GPU of
a modern computer or a smartphone. Yet, let’s speculate about what
a “more physical” implementation of such a system could look like.
We can imagine it as a grid of tiny independent computers,
simulating individual cells. Each of those computers would require
approximately 10Kb of ROM to store the “cell genome”: neural
network weights and the control code, and about 256 bytes of RAM
for the cell state and intermediate activations. The cells must be
able to communicate their 16-value state vectors to neighbors. Each
cell would also require an RGB-diode to display the color of the
pixel it represents. A single cell update would require about 10k
multiply-add operations and does not have to be synchronised across
the grid. We propose that cells might wait for random time
intervals between updates. The system described above is uniform
and decentralised. Yet, our method provides a way to program it to
reach the predefined global state, and recover this state in case
of multi-element failures and restarts. We therefore conjecture
this kind of modeling may be used for designing reliable,
self-organising agents. On the more theoretical machine learning
front, we show an instance of a decentralized model able to
accomplish remarkably complex tasks. We believe this direction to
be opposite to the more traditional global modeling used in the
majority of contemporary work in the deep learning field, and we
hope this work to be an inspiration to explore more decentralized
learning modeling.</p>
</blockquote>
<h1 id="bibliography">Bibliography</h1>
<p><a id="mordvintsevGrowingNeuralCellular2020" target=
"_blank">Mordvintsev, A., Randazzo, E., Niklasson, E., & Levin, M.,
<em>Growing Neural Cellular Automata</em>, Distill, <em>5(2)</em>,
23 (2020).</a> <a href=
"http://dx.doi.org/10.23915/distill.00023">http://dx.doi.org/10.23915/distill.00023</a>
<a href="#b71c53b928ac63b5999b34d6c7621b33">↩</a></p>
</div>
<div class="note-footer">Last changed <a class="u-url" href=
"https://hugocisneros.com/notes/mordvintsevgrowingneuralcellular2020/">
<time itemprop="datePublished" class="dt-published" datetime=
"2020-06-26T11:28:00+0200">26/06/2020</time></a> | authored by
<a href="https://hugocisneros.com/" rel="author" class=
"p-author h-card" itemprop="author" itemscope itemtype=
"http://schema.org/Person"><span itemprop="name">Hugo
Cisneros</span></a></div>
</div>
</article>
<br>
<a href="/notes#mordvintsevgrowingneuralcellular2020"><b>← Back to
Notes</b></a>
<hr></main>
<footer class="footer">
<ul class="footer-links">
<li><a href="/blog/index.xml" type="application/rss+xml" target=
"_blank">Blog RSS feed</a></li>
<li><a href=
"https://github.com/hugcis/natrium-custom">Code</a></li>
</ul>
</footer>
</div>
<script>
 MathJax = {
     tex: {
         inlineMath: [['$','$'], ['\\(', '\\)']],
         tags: 'ams'
     }
 };
</script> 
<script type="text/javascript" rel="preconnect" id="MathJax-script"
async src=
"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
