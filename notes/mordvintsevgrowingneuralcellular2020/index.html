<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1">
  <title>Notes on: Growing Neural Cellular Automata by Mordvintsev, A., Randazzo, E., Niklasson, E., & Levin, M. (2020) - Hugo Cisneros - Personal page</title>
  <meta property="og:title" content="Notes on: Growing Neural Cellular Automata by Mordvintsev, A., Randazzo, E., Niklasson, E., &amp; Levin, M. (2020) - Hugo Cisneros - Personal page">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/mordvintsevgrowingneuralcellular2020/">
  <meta property="og:description" content="Notes about Growing Neural Cellular Automata by Mordvintsev, A., Randazzo, E., Niklasson, E., &amp; Levin, M. (2020)">
  <meta name="Description" property="description" content="Notes about Growing Neural Cellular Automata by Mordvintsev, A., Randazzo, E., Niklasson, E., &amp; Levin, M. (2020)">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="stylesheet" href="https://hugocisneros.com/css/main.min.eade29d17a79794f2a5f343f06dc6de3418db15c7eaf7b7d0250936a1e792b8d.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros - Personal page</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/about/">About</a>
          </li>
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">Growing Neural Cellular Automata by Mordvintsev, A., Randazzo, E., Niklasson, E., & Levin, M. (2020)</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>tags</dt>
              <dd>
                <a href="/notes/cellular_automata/">Cellular automata</a>
              </dd>
              <dt>source</dt>
              <dd>
                (<a href="#org635ea65">Mordvintsev et al. 2020</a>)
              </dd>
            </dl>
            <h2 id="summary">Summary</h2>
            <p>This paper introduces interesting ideas for training <a href="/notes/cellular_automata_as_cnns/">cellular automata as CNNs</a> to have self-repairing stable structures. The automata have 16 dimensional continuous states. The main modeling ideas are:</p>
            <ul>
              <li>Use hard-coded filters for the initial perception step. The filters are Sobel convolutions and those two are concatenated with the current state.</li>
              <li>Update rules are then 1D convolutions applied to the \(3 * 16 = 48\) dimensional state vector. They use a neural network with dimensions 40 -&gt; 128 -&gt; 16.</li>
              <li>States are updated with probability .5 only, making them highly asynchronous.</li>
              <li>One of the 16 channels is an \(\alpha\) channel that determines whether the cell is <em>alive</em> or <em>dead</em>. The threshold is set to 0.1 for setting a cell to alive. <em>dead</em> cells have their state manually set to 0 at each step.</li>
            </ul>
            <p>The authors then apply several training tricks to make the patterns more robust, self-repairing, etc.</p>
            <h2 id="comments">Comments</h2>
            <p>I find the paper quite interesting, especially with its take on CA update as CNN. The fixed convolutions restricts the possible rules while enabling a more stable search process probably.</p>
            <p>Some of the modeling ideas such as the size of the downstream neural in 1D convolutions and the asynchronous updates aren’t really justified clearly. The final quantization step to make the whole thing work in browsers is particularly interesting to me: the end up with a CA that has \(16 * 8 = 128\) bits states. Or maybe a 120 bits states and 8 bits alive/dead semi-independent state. This is something like \(10^36\) states which is many orders of magnitude larger than my experiments.</p>
            <p>The very last paragraph I particularly like:</p>
            <blockquote>
              <p>Engineering and machine learning</p>
              <p>The models described in this article run on the powerful GPU of a modern computer or a smartphone. Yet, let’s speculate about what a “more physical” implementation of such a system could look like. We can imagine it as a grid of tiny independent computers, simulating individual cells. Each of those computers would require approximately 10Kb of ROM to store the “cell genome”: neural network weights and the control code, and about 256 bytes of RAM for the cell state and intermediate activations. The cells must be able to communicate their 16-value state vectors to neighbors. Each cell would also require an RGB-diode to display the color of the pixel it represents. A single cell update would require about 10k multiply-add operations and does not have to be synchronised across the grid. We propose that cells might wait for random time intervals between updates. The system described above is uniform and decentralised. Yet, our method provides a way to program it to reach the predefined global state, and recover this state in case of multi-element failures and restarts. We therefore conjecture this kind of modeling may be used for designing reliable, self-organising agents. On the more theoretical machine learning front, we show an instance of a decentralized model able to accomplish remarkably complex tasks. We believe this direction to be opposite to the more traditional global modeling used in the majority of contemporary work in the deep learning field, and we hope this work to be an inspiration to explore more decentralized learning modeling.</p>
            </blockquote>
            <h2 id="bibliography">Bibliography</h2>
            <p><a id="org635ea65"></a><span itemscope itemtype="https://schema.org/ScholarlyArticle"><span itemprop="author"><span itemprop="author">Mordvintsev, Alexander</span>, <span itemprop="author">Ettore Randazzo</span>, <span itemprop="author">Eyvind Niklasson</span>, and <span itemprop="author">Michael Levin</span></span>. <span datetime="2020" itemprop="datePublished">February 2020</span>. “<span itemprop="name">Growing Neural Cellular Automata</span>”. <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Distill</i></span> 5 (2):e23.</span></p>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/mordvintsevgrowingneuralcellular2020/"><time itemprop="datePublished" class="dt-published" datetime="2021-03-25T09:57:00+0100">25/03/2021</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article><br>
      <a href="/notes#mordvintsevgrowingneuralcellular2020"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/natrium-custom">Code</a>
        </li>
        <li>© Hugo Cisneros 2021</li>
      </ul>
    </footer>
  </div>
  <script>
  MathJax = {
     tex: {
         inlineMath: [['$','$'], ['\\(', '\\)']],
         tags: 'ams'
     }
  };
  </script> 
  <script type="text/javascript" rel="preconnect" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
