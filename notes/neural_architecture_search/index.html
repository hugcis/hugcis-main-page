<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1">
  <title>Neural architecture search - Hugo Cisneros - Personal page</title>
  <meta property="og:title" content="Neural architecture search - Hugo Cisneros - Personal page">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/neural_architecture_search/">
  <meta property="og:description" content="Notes about Neural architecture search">
  <meta name="Description" property="description" content="Notes about Neural architecture search">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="stylesheet" href="https://hugocisneros.com/css/main.min.css" media="all" type="text/css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
</head>
<body>
  <div class="wrapper">
    <header class="header">
      <nav class="nav">
        <div class="nav-main">
          <a href="https://hugocisneros.com/" class="nav-title">Hugo Cisneros - Personal page</a>
        </div>
        <ul class="nav-links">
          <li>
            <a href="/blog/">Blog</a>
          </li>
          <li>
            <a href="/notes/">Notes</a>
          </li>
          <li>
            <a href="/projects/">Projects</a>
          </li>
          <li>
            <a href="/resume/">Resume</a>
          </li>
          <li>
            <a href="/contact/">Contact</a>
          </li>
        </ul>
      </nav>
    </header>
    <main class="content" role="main">
      <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
        <div class="single-note note-container">
          <h1 class="article-title p-name" itemprop="name">Neural architecture search</h1>
          <div class="article-content e-content p-name" itemprop="articleBody">
            <dl>
              <dt>tags</dt>
              <dd>
                <a href="/notes/search_algorithms/">Search</a>, <a href="/notes/neural_networks/">Neural networks</a>
              </dd>
            </dl>
            <p>Neural architecture search (NAS) is a method for finding <a href="/notes/neural_networks/">neural networks</a> architectures. It is usually based on three main components:</p>
            <dl>
              <dt>Search space</dt>
              <dd>Type of network that can be built.</dd>
              <dt>Search strategy</dt>
              <dd>The approach for exploring the space.</dd>
              <dt>Performance estimation strategy</dt>
              <dd>The way the performance of a constructed neural network is evaluated (without actually building it or training/running it).</dd>
            </dl>
            <h2 id="reinforcement-learning--reinforcement-learning-dot-md--based-nas"><a href="/notes/reinforcement_learning/">Reinforcement learning</a>-based NAS</h2>
            <p>The original idea was called Neural architecture search and is based on the use of a <a href="/notes/recurrent_neural_networks/">RNN</a> as a controller and generator of architectures. The search-space is pre-defined and explored in a rigid way. (<a href="#org986c1dc">Zoph and Le 2017</a>).</p>
            <p>The process of generating architectures from the first article was extremely lengthy and replaced later by a more constrained search. (<a href="#orgb2c8dd8">Zoph et al. 2018</a>).</p>
            <p>Recent ideas include the use of parameter sharing across architectures because the main bottleneck of previous techniques was essentially in the training of each child model. This results in significant speedup of RL-based NAS. (<a href="#org6176865">Pham et al. 2018</a>)</p>
            <h2 id="neuroevolution">Neuroevolution</h2>
            <p>This field is more focused on <a href="/notes/evolution/">evolution</a> neural network through evolutionary methods such as e.g <a href="/notes/genetic_algorithm/">genetic algorithms</a>. One of the main work that made that field popular is <a href="/notes/neat/">NEAT</a> (<a href="#org87da3a4">Stanley and Miikkulainen 2002</a>).</p>
            <h2 id="bibliography">Bibliography</h2>
            <ol class="biblio-list">
              <a id="org6176865"></a>
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle">
                <span itemprop="author"><span itemprop="author">Pham, Hieu</span>, <span itemprop="author">Melody Y. Guan</span>, <span itemprop="author">Barret Zoph</span>, <span itemprop="author">Quoc V. Le</span>, and <span itemprop="author">Jeff Dean</span></span>. <span datetime="2018" itemprop="datePublished">February 2018</span>. "<span itemprop="name">Efficient Neural Architecture Search via Parameter Sharing</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">arXiv:1802.03268 [Cs, Stat]</i></span>, February.
                <p><a id="org87da3a4"></a></p>
              </li>
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle">
                <span itemprop="author"><span itemprop="author">Stanley, Kenneth O.</span>, and <span itemprop="author">Risto Miikkulainen</span></span>. <span datetime="2002" itemprop="datePublished">June 2002</span>. “<span itemprop="name">Evolving Neural Networks Through Augmenting Topologies</span>”. <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Evolutionary Computation</i></span> 10 (2):99–127.
                <p><a id="orgb2c8dd8"></a></p>
              </li>
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle">
                <span itemprop="author"><span itemprop="author">Zoph, Barret</span>, <span itemprop="author">Vijay Vasudevan</span>, <span itemprop="author">Jonathon Shlens</span>, and <span itemprop="author">Quoc V. Le</span></span>. <span datetime="2018" itemprop="datePublished">April 2018</span>. “<span itemprop="name">Learning Transferable Architectures for Scalable Image Recognition</span>”. <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">arXiv:1707.07012 [Cs, Stat]</i></span>, April.
                <p><a id="org986c1dc"></a></p>
              </li>
              <li itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle"><span itemprop="author"><span itemprop="author">Zoph, Barret</span>, and <span itemprop="author">Quoc V. Le</span></span>. <span datetime="2017" itemprop="datePublished">February 2017</span>. “<span itemprop="name">Neural Architecture Search with Reinforcement Learning</span>”. <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">arXiv:1611.01578 [Cs]</i></span>, February.</li>
            </ol>
          </div>
          <div class="bl-section">
            <h2>Links to this note</h2>
            <div class="backlinks">
              <ul>
                <li>
                  <a href="/notes/cluneaigasaigeneratingalgorithms2019/">Notes on: AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence by Clune, J. (2019)</a>
                </li>
                <li>
                  <a href="/notes/stanleyevolvingneuralnetworks2002/">Notes on: Evolving Neural Networks through Augmenting Topologies by Stanley, K. O., & Miikkulainen, R. (2002)</a>
                </li>
                <li>
                  <a href="/notes/zophlearningtransferablearchitectures2018/">Notes on: Learning Transferable Architectures for Scalable Image Recognition by Zoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2018)</a>
                </li>
                <li>
                  <a href="/notes/zophneuralarchitecturesearch2017/">Notes on: Neural Architecture Search with Reinforcement Learning by Zoph, B., & Le, Q. V. (2017)</a>
                </li>
                <li>
                  <a href="/notes/floreanoneuroevolutionarchitectureslearning2008/">Notes on: Neuroevolution: from architectures to learning by Floreano, D., Dürr, P., & Mattiussi, C. (2008)</a>
                </li>
              </ul>
            </div>
          </div>
          <div class="note-footer">
            Last changed <a class="u-url" href="https://hugocisneros.com/notes/neural_architecture_search/"><time itemprop="datePublished" class="dt-published" datetime="2021-03-25T09:56:00+0100">25/03/2021</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
          </div>
        </div>
      </article><br>
      <a href="/notes#neural_architecture_search"><b>← Back to Notes</b></a>
      <hr>
    </main>
    <footer class="footer">
      <ul class="footer-links">
        <li>
          <a class="rss-link" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon" src="/img/RSS.svg" alt="RSS feed icon"></a>
        </li>
        <li>
          <a href="https://github.com/hugcis/natrium-custom">Code</a>
        </li>
        <li>© Hugo Cisneros 2021</li>
      </ul>
    </footer>
  </div>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>

    document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "\\[", right: "\\]", display: true},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "$", right: "$", display: false},
            {left: "\\(", right: "\\)", display: false},
        ]
  })
    });
  </script>
</body>
</html>
