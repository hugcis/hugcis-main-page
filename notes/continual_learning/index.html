<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
  <title>Continual learning - Hugo Cisneros</title>
  <meta property="og:title" content="Continual learning - Hugo Cisneros">
  <meta property="og:type" content="article">
  <meta property="og:image" content="/img/main.jpeg">
  <meta property="og:url" content="https://hugocisneros.com/notes/continual_learning/">
  <meta property="og:description" content="Notes about Continual learning">
  <meta name="Description" property="description" content="Notes about Continual learning">
  <link rel="me" href="https://twitter.com/@cisne_hug">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@cisne_hug">
  <meta name="twitter:creator" content="@cisne_hug">
  <link rel="me" href="https://scholar.social/@hugcis">
  <link rel="me" href="https://github.com/hugcis">
  <meta property="keywords" content="machine learning">
  <link rel="stylesheet" href="https://hugocisneros.com/css/style.min.css">
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/svg+xml" href="/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <meta name="theme-color" content="#ffffff">
  <link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
  <link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">
  <script>
  function updateMode(){localStorage.theme==="dark"||!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}function toggleMode(){localStorage.theme==="dark"?localStorage.theme="light":localStorage.theme="dark",updateMode()}window.onload=updateMode();function toggleMenu(){let e=document.getElementById("navbar-default");e.classList.contains("hidden")?e.classList.remove("hidden"):e.classList.add("hidden")}
  </script>
</head>
<body>
  <header class="md:px-0 px-2">
    <nav>
      <div class="container flex flex-wrap justify-between items-center mx-auto">
        <div class="nav-main my-2.5">
          <a href="https://hugocisneros.com/" class="nav-title py-2.5 text-2xl text-zinc-600 dark:text-zinc-300 hover:border-b-0">Hugo Cisneros</a>
        </div><button type="button" onclick="toggleMenu()" class="inline-flex items-center p-2 ml-3 text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600" aria-controls="navbar-default" aria-expanded="false"><span class="sr-only">Open main menu</span><svg class="w-6 h-6" aria-hidden="true" fill="currentcolor" viewbox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
        <path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button>
        <div class="hidden w-full md:block md:w-auto" id="navbar-default">
          <ul class="grid md:grid-flow-col items-center justify-between text-lg my-2.5">
            <li class="p-2.5 md:first:pl-0 md:border-none border-b dark:border-zinc-500 list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/blog/">Blog</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b dark:border-zinc-500 list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/notes/">Notes</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b dark:border-zinc-500 list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/projects/">Projects</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b dark:border-zinc-500 list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/resume/">Resume</a>
            </li>
            <li class="p-2.5 md:first:pl-0 md:border-none border-b dark:border-zinc-500 list-none">
              <a class="text-zinc-600 dark:text-zinc-300 hover:border-b-0" href="/contact/">Contact</a>
            </li>
            <li class="h-7 pl-2.5 pr-0 list-none"><button type="button" onclick="toggleMode()" class="h-full" aria-label="Toggle between dark and light mode"><img class="h-7 w-7 max-h-full mb-1.5 p-1.5 hidden dark:inline" id="ligh-mode-button-img" alt="A sun icon for switching to light mode" src="https://hugocisneros.com/img/light_mode.svg"> <img class="h-7 w-7 max-h-full mb-1.5 p-1.5 inline dark:hidden" id="dark-mode-button-img" alt="A moon icon for switching to dark mode" src="https://hugocisneros.com/img/dark_mode.svg"></button></li>
          </ul>
        </div>
      </div>
    </nav>
  </header>
  <main class="content h-card container mt-2 m-auto leading-loose md:px-0 px-2 z-0 Page(/notes/continual_learning.md)" role="main">
    <article class="article h-entry" itemprop="mainEntity" itemscope itemtype="http://schema.org/BlogPosting">
      <div class="bg-zinc-100 dark:bg-zinc-700 pb-2 pt-1 px-3 rounded-lg mb-4">
        <h1 class="article-title p-name" itemprop="name">Continual learning</h1>
        <div class="article-content e-content p-name" itemprop="articleBody">
          <dl>
            <dt>tags</dt>
            <dd>
              <a href="/notes/machine_learning/">Machine learning</a>
            </dd>
          </dl>
          <p>Continual learning is a type of <a href="/notes/supervised_learning/">supervised learning</a> where there is no “testing phase” associated to a decision process. Instead, training samples keep being processed by the <a href="/notes/algorithm/">algorithm</a> which has to simultaneously make predictions and keep learning.</p>
          <p>This is challenging for a fixed neural network architecture since it has a fixed capacity and is bound to either forget things or be unable to learn anything new.</p>
          <p>A definition from the survey (<a href="#delangeContinualLearningSurvey2020"><cite itemprop="citation" itemscope itemid="key:delangeContinualLearningSurvey2020">De Lange et al. 2020</cite></a>):</p>
          <blockquote>
            <p>The General Continual Learning setting considers an infinite stream of training data where at each time step, the system receives a (number of) new sample(s) drawn non i.i.d from a current distribution that could itself experience sudden or gradual changes.</p>
          </blockquote>
          <h2 id="theoretical-foundations">Theoretical foundations</h2>
          <h3 id="concept-shift">Concept shift</h3>
          <p>(<a href="#bartlettLearningChangingConcepts1996"><cite itemprop="citation" itemscope itemid="doi:10.1145/238061.238080">Bartlett et al. 1996</cite></a>) explores how to learn under the assumption of concept shift:</p>
          <blockquote>
            <p>The learner sees a sequence of random examples, labelled according to a sequence of functions, and must provide an accurate estimate of the target function sequence.</p>
          </blockquote>
          <p>Formally, a learner sees at time \(t\) a random example \(x_t\) from some domain \(X\). He also sees the value of \(f_t(x_t) \in \{0, 1\}\) where \(f_t\) is an unknown function from some known class \(F\).</p>
          <p>The paper addresses two problems of learning with changing concepts:</p>
          <ul>
            <li><strong>Estimation:</strong> When can we estimate a sequence \((f_1, \cdots, f_n)\) from observations \(((x_1, f_1(x_1)), \cdots, (x_n, f_n(x_n)))\)?</li>
            <li><strong>Prediction:</strong> When can one predict the next concept \(f_{n+1}\) from a sequence of concepts \((f_1, \cdots, f_n)\)?</li>
          </ul>
          <h3 id="formal-definitions-of-different-aspects-of-continual-learning">Formal definitions of different aspects of continual learning</h3>
          <h4 id="learning-to-learn">Learning to learn</h4>
          <p>The paper (<a href="#baxterTheoreticalModelsLearning1998"><cite itemprop="citation" itemscope itemid="doi:10.1007/978-1-4615-5529-2_4">Baxter 1998</cite></a>) defines the problem of learning to learn as follows (notations are chosen to contrast with regular <a href="/notes/supervised_learning/">supervised learning</a>):</p>
          <ul>
            <li>an input space \(X\) and an output space \(Y\),</li>
            <li>a loss function \(l: Y \times Y \rightarrow \mathbb{R}\),</li>
            <li>an environment \((P, Q)\) where \(P\) is the set of all probability distributions on \(X \times Y\) and \(Q\) is a distribution on \(P\),</li>
            <li>a hypothesis space family \(H = {\mathcal{H}}\) where each \(\mathcal{H} \in H\) is a set of functions \(h: X \rightarrow Y\).</li>
          </ul>
          <p>(<a href="#pentinaMultitaskLifelongLearning2015a"><cite itemprop="citation" itemscope itemid="doi:10.1007/978-3-319-24486-0_13">Pentina, Ben-David 2015</cite></a>)</p>
          <p>(<a href="#pentinaLifelongLearningWeighted2016"><cite itemprop="citation" itemscope itemid="key:pentinaLifelongLearningWeighted2016">Pentina, Urner 2016</cite></a>)</p>
          <p>(<a href="#pentinaLifelongLearningNoni2015"><cite itemprop="citation" itemscope itemid="key:pentinaLifelongLearningNoni2015">Pentina, Lampert 2015</cite></a>)</p>
          <p>(<a href="#balcanLifelongLearningCostly2020"><cite itemprop="citation" itemscope itemid="doi:10.1016/j.tcs.2019.11.010">Balcan et al. 2020</cite></a>)</p>
          <p>(<a href="#kiferDetectingChangeData2004"><cite itemprop="citation" itemscope itemid="key:kiferDetectingChangeData2004">Kifer et al. 2004</cite></a>)</p>
          <p>(<a href="#ben-davidNotionTaskRelatedness2008"><cite itemprop="citation" itemscope itemid="doi:10.1007/s10994-007-5043-5">Ben-David, Borbely 2008</cite></a>)</p>
          <p>(<a href="#ben-davidExploitingTaskRelatedness2003"><cite itemprop="citation" itemscope itemid="doi:10.1007/978-3-540-45167-9_41">Ben-David, Schuller 2003</cite></a>)</p>
          <p>(<a href="#geisaTheoryOutofdistributionLearning2022"><cite itemprop="citation" itemscope itemid="doi:10.48550/arXiv.2109.14501">Geisa et al. 2022</cite></a>)</p>
          <h2 id="examples-of-continual-learning-systems">Examples of continual learning systems</h2>
          <ul>
            <li>Never Ending Language Learner (NELL) (<a href="#carlsonArchitectureNeverEndingLanguage2010"><cite itemprop="citation" itemscope itemid="doi:10.1002/ajp.20927">Carlson et al. 2010</cite></a>)
            </li>
          </ul>
          <h2 id="benchmarks">Benchmarks</h2>
          <h3 id="computer-vision-based-benchmarks">Computer vision based benchmarks</h3>
          <ul>
            <li>
              <p>Split MNIST: the MNIST dataset is split into 5 2-classes tasks (<a href="#nguyenVariationalContinualLearning2017"><cite itemprop="citation" itemscope itemid="key:nguyenVariationalContinualLearning2017">Nguyen et al. 2017</cite></a>; <a href="#zenkeContinualLearningSynaptic2017"><cite itemprop="citation" itemscope itemid="key:zenkeContinualLearningSynaptic2017">Zenke et al. 2017</cite></a>; <a href="#shinContinualLearningDeep2017"><cite itemprop="citation" itemscope itemid="key:shinContinualLearningDeep2017">Shin et al. 2017</cite></a>).</p>
            </li>
            <li>
              <p>Split CIFAR10: the CIFAR10 dataset is split into 5 2-classes tasks (<a href="#krizhevskyLearningMultipleLayers2009"><cite itemprop="citation" itemscope itemid="key:krizhevskyLearningMultipleLayers2009">Krizhevsky, Hinton 2009</cite></a>).</p>
            </li>
            <li>
              <p>Split mini-ImageNet: a mini ImageNet (100 classes) task split into 20 5-classes tasks.</p>
            </li>
            <li>
              <p>Continual Transfer Learning Benchmark: <a href="https://github.com/facebookresearch/CTrLBenchmark">A benchmark from Facebook AI</a>, built from 7 computer vision datasets: MNIST, CIFAR10, CIFAR100, DTD, SVHN, Rainbow-MNIST, Fashion MNIST. The tasks are all 5-classes or 10-classes classification tasks. Some example task sequence constructions from (<a href="#veniatEfficientContinualLearning2021"><cite itemprop="citation" itemscope itemid="key:veniatEfficientContinualLearning2021">Veniat et al. 2021</cite></a>):</p>
              <blockquote>
                <p>The last task of \(S_{out}\) consists of a shuffling of the output labels of the first task. The last task of \(S_{in}\) is the same as its first task except that MNIST images have a different background color. \(S_{long}\) has 100 tasks, and it is constructed by first sampling a dataset, then 5 classes at random, and finally the amount of training data from a distribution that favors small tasks by the end of the learning experience.</p>
              </blockquote>
            </li>
            <li>
              <p>Permuted MNIST: here for each different task the pixels of the MNIST digits are permuted, generating a new task of equal difficulty as the original one but different solution. This task is not suitable if the model has some spatial prior (like a <a href="/notes/convolutional_neural_networks/">CNN</a>). Used first in (<a href="#goodfellowEmpiricalInvestigationCatastrophic2014"><cite itemprop="citation" itemscope itemid="key:goodfellowEmpiricalInvestigationCatastrophic2014">Goodfellow et al. 2014</cite></a>; <a href="#srivastavaCompeteCompute2013a"><cite itemprop="citation" itemscope itemid="key:srivastavaCompeteCompute2013a">Srivastava et al. 2013</cite></a>). Also in (<a href="#kirkpatrickOvercomingCatastrophicForgetting2017"><cite itemprop="citation" itemscope itemid="key:kirkpatrickOvercomingCatastrophicForgetting2017">Kirkpatrick et al. 2017</cite></a>)</p>
            </li>
            <li>
              <p>Rotated MNIST: each task contains digits rotated by a fixed angle between 0 and 180 degrees.</p>
            </li>
          </ul>
          <h2 id="bibliography">Bibliography</h2>
          <ol class="biblio-list pl-0">
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="delangeContinualLearningSurvey2020"><span itemprop="author">Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, Tinne Tuytelaars</span>. <time datetime="2020" itemprop="datePublished">May 26, 2020</time>. "<span itemprop="name">A Continual Learning Survey: Defying Forgetting in Classification Tasks</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Arxiv:1909.08383 [cs, Stat]</i></span>. <a itemprop="sameAs" href="http://arxiv.org/abs/1909.08383">http://arxiv.org/abs/1909.08383</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="bartlettLearningChangingConcepts1996" itemid="doi:10.1145/238061.238080"><span itemprop="author">Peter L. Bartlett, Shai Ben-David, Sanjeev R. Kulkarni</span>. <time datetime="1996" itemprop="datePublished">1996</time>. "<span itemprop="name">Learning Changing Concepts by Exploiting the Structure of Change</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Proceedings of the Ninth Annual Conference on Computational Learning Theory - COLT '96</i></span>, 131–39. Desenzano del Garda, Italy: ACM Press. <a itemprop="sameAs" href="https://doi.org/10.1145/238061.238080">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="baxterTheoreticalModelsLearning1998" itemid="doi:10.1007/978-1-4615-5529-2_4"><span itemprop="author">Jonathan Baxter</span>. <time datetime="1998" itemprop="datePublished">1998</time>. "<span itemprop="name">Theoretical Models of Learning to Learn</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Learning to Learn</i></span>, edited by Sebastian Thrun and Lorien Pratt, 71–94. Boston, MA: Springer US. <a itemprop="sameAs" href="https://doi.org/10.1007/978-1-4615-5529-2_4">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="pentinaMultitaskLifelongLearning2015a" itemid="doi:10.1007/978-3-319-24486-0_13"><span itemprop="author">Anastasia Pentina, Shai Ben-David</span>. <time datetime="2015" itemprop="datePublished">2015</time>. "<span itemprop="name">Multi-task and Lifelong Learning of Kernels</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Algorithmic Learning Theory</i></span>, edited by Kamalika Chaudhuri, CLAUDIO GENTILE, and Sandra Zilles, 194–208. Lecture Notes in Computer Science. Cham: Springer International Publishing. <a itemprop="sameAs" href="https://doi.org/10.1007/978-3-319-24486-0_13">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="pentinaLifelongLearningWeighted2016"><span itemprop="author">Anastasia Pentina, Ruth Urner</span>. <time datetime="2016" itemprop="datePublished">2016</time>. "<span itemprop="name">Lifelong Learning with Weighted Majority Votes</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Advances in Neural Information Processing Systems</i></span>. Vol. 29. Curran Associates, Inc.. <a itemprop="sameAs" href="https://proceedings.neurips.cc/paper/2016/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html">https://proceedings.neurips.cc/paper/2016/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="pentinaLifelongLearningNoni2015"><span itemprop="author">Anastasia Pentina, Christoph H Lampert</span>. <time datetime="2015" itemprop="datePublished">2015</time>. "<span itemprop="name">Lifelong Learning with Non-i.i.d. Tasks</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Advances in Neural Information Processing Systems</i></span>. Vol. 28. Curran Associates, Inc.. <a itemprop="sameAs" href="https://proceedings.neurips.cc/paper/2015/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html">https://proceedings.neurips.cc/paper/2015/hash/9232fe81225bcaef853ae32870a2b0fe-Abstract.html</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="balcanLifelongLearningCostly2020" itemid="doi:10.1016/j.tcs.2019.11.010"><span itemprop="author">Maria-Florina Balcan, Avrim Blum, Vaishnavh Nagarajan</span>. <time datetime="2020" itemprop="datePublished">February 12, 2020</time>. "<span itemprop="name">Lifelong Learning in Costly Feature Spaces</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Theoretical Computer Science</i></span>, Special Issue on Algorithmic Learning Theory, 808 (February):14–37. <a itemprop="sameAs" href="https://doi.org/10.1016/j.tcs.2019.11.010">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="kiferDetectingChangeData2004"><span itemprop="author">Daniel Kifer, Shai Ben-David, Johannes Gehrke</span>. <time datetime="2004" itemprop="datePublished">2004</time>. "<span itemprop="name">Detecting Change in Data Streams</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">VLDB</i></span>, 4:180–91. Toronto, Canada.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="ben-davidNotionTaskRelatedness2008" itemid="doi:10.1007/s10994-007-5043-5"><span itemprop="author">Shai Ben-David, Reba Schuller Borbely</span>. <time datetime="2008" itemprop="datePublished">December 1, 2008</time>. "<span itemprop="name">A Notion of Task Relatedness Yielding Provable Multiple-task Learning Guarantees</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Machine Learning</i></span> 73 (3):273–87. <a itemprop="sameAs" href="https://doi.org/10.1007/s10994-007-5043-5">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="ben-davidExploitingTaskRelatedness2003" itemid="doi:10.1007/978-3-540-45167-9_41"><span itemprop="author">Shai Ben-David, Reba Schuller</span>. <time datetime="2003" itemprop="datePublished">2003</time>. "<span itemprop="name">Exploiting Task Relatedness for Multiple Task Learning</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Learning Theory and Kernel Machines</i></span>, edited by Bernhard Schölkopf and Manfred K. Warmuth, 567–80. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. <a itemprop="sameAs" href="https://doi.org/10.1007/978-3-540-45167-9_41">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="geisaTheoryOutofdistributionLearning2022" itemid="doi:10.48550/arXiv.2109.14501"><span itemprop="author">Ali Geisa, Ronak Mehta, Hayden S. Helm, Jayanta Dey, Eric Eaton, Jeffery Dick, Carey E. Priebe, Joshua T. Vogelstein</span>. <time datetime="2022" itemprop="datePublished">January 6, 2022</time>. "<span itemprop="name">Towards a Theory of Out-of-distribution Learning</span>". arXiv. <a itemprop="sameAs" href="https://doi.org/10.48550/arXiv.2109.14501">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="carlsonArchitectureNeverEndingLanguage2010" itemid="doi:10.1002/ajp.20927"><span itemprop="author">Andrew Carlson, Justin Betteridge, Bryan Kisiel</span>. <time datetime="2010" itemprop="datePublished">2010</time>. "<span itemprop="name">Toward an Architecture for Never-ending Language Learning.</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">In Proceedings of the Conference on Artificial Intelligence (AAAI) (2010)</i></span>, 1306–13. <a itemprop="sameAs" href="https://doi.org/10.1002/ajp.20927">DOI</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="nguyenVariationalContinualLearning2017"><span itemprop="author">Cuong V. Nguyen, Yingzhen Li, Thang D. Bui, Richard E. Turner</span>. <time datetime="2017" itemprop="datePublished">2017</time>. "<span itemprop="name">Variational Continual Learning</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Corr</i></span> abs/1710.10628. <a itemprop="sameAs" href="http://arxiv.org/abs/1710.10628">http://arxiv.org/abs/1710.10628</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="zenkeContinualLearningSynaptic2017"><span itemprop="author">Friedemann Zenke, Ben Poole, Surya Ganguli</span>. <time datetime="2017" itemprop="datePublished">2017</time>. "<span itemprop="name">Continual Learning Through Synaptic Intelligence</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017</i></span>, edited by Doina Precup and Yee Whye Teh, 70:3987–95. Proceedings of Machine Learning Research. PMLR. <a itemprop="sameAs" href="http://proceedings.mlr.press/v70/zenke17a.html">http://proceedings.mlr.press/v70/zenke17a.html</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="shinContinualLearningDeep2017"><span itemprop="author">Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim</span>. <time datetime="2017" itemprop="datePublished">2017</time>. "<span itemprop="name">Continual Learning with Deep Generative Replay</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</i></span>, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 2990–99. <a itemprop="sameAs" href="https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="krizhevskyLearningMultipleLayers2009"><span itemprop="author">Alex Krizhevsky, Geoffrey Hinton</span>. <time datetime="2009" itemprop="datePublished">2009</time>. "<span itemprop="name">Learning Multiple Layers of Features from Tiny Images</span>". University of Toronto.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="veniatEfficientContinualLearning2021"><span itemprop="author">Tom Veniat, Ludovic Denoyer, Marc'Aurelio Ranzato</span>. <time datetime="2021" itemprop="datePublished">February 12, 2021</time>. "<span itemprop="name">Efficient Continual Learning with Modular Networks and Task-driven Priors</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Arxiv:2012.12631 [cs]</i></span>. <a itemprop="sameAs" href="http://arxiv.org/abs/2012.12631">http://arxiv.org/abs/2012.12631</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="goodfellowEmpiricalInvestigationCatastrophic2014"><span itemprop="author">Ian J. Goodfellow, Mehdi Mirza, Xia Da, Aaron C. Courville, Yoshua Bengio</span>. <time datetime="2014" itemprop="datePublished">2014</time>. "<span itemprop="name">An Empirical Investigation of Catastrophic Forgeting in Gradient-based Neural Networks</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings</i></span>, edited by Yoshua Bengio and Yann LeCun. <a itemprop="sameAs" href="http://arxiv.org/abs/1312.6211">http://arxiv.org/abs/1312.6211</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="srivastavaCompeteCompute2013a"><span itemprop="author">Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian, Faustino J. Gomez, Jürgen Schmidhuber</span>. <time datetime="2013" itemprop="datePublished">2013</time>. "<span itemprop="name">Compete to Compute</span>". In <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a Meeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States</i></span>, edited by Christopher J. C. Burges, Léon Bottou, Zoubin Ghahramani, and Kilian Q. Weinberger, 2310–18. <a itemprop="sameAs" href="https://proceedings.neurips.cc/paper/2013/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html">https://proceedings.neurips.cc/paper/2013/hash/8f1d43620bc6bb580df6e80b0dc05c48-Abstract.html</a>.</span></li>
            <li class="list-none my-4 mx-2"><span itemprop="citation" itemscope itemtype="https://schema.org/ScholarlyArticle" id="kirkpatrickOvercomingCatastrophicForgetting2017"><span itemprop="author">James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, et al.</span>. <time datetime="2017" itemprop="datePublished">January 25, 2017</time>. "<span itemprop="name">Overcoming Catastrophic Forgetting in Neural Networks</span>". <span itemprop="isPartOf" itemscope itemtype="https://schema.org/Periodical"><i itemprop="name">Arxiv:1612.00796 [cs, Stat]</i></span>. <a itemprop="sameAs" href="http://arxiv.org/abs/1612.00796">http://arxiv.org/abs/1612.00796</a>.</span></li>
          </ol>
        </div>
        <div class="bl-section">
          <h2>Links to this note</h2>
          <div class="backlinks">
            <ul>
              <li>
                <a href="/notes/catastrophic_forgetting/">Catastrophic forgetting</a>
              </li>
            </ul>
          </div>
        </div>
        <div class="text-center" style="font-variant-caps:all-small-caps">
          Last changed <a class="u-url" href="https://hugocisneros.com/notes/continual_learning/"><time itemprop="datePublished" class="dt-published" datetime="2022-08-30T16:30:00+0200">30/08/2022</time></a> | authored by <a href="https://hugocisneros.com/" rel="author" class="p-author h-card" itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">Hugo Cisneros</span></a>
        </div>
      </div>
    </article>
    <h3>Comments</h3>
    <script data-isso="https://comment.hugocisneros.com/" data-isso-require-author="true" data-isso-vote="true" src="https://comment.hugocisneros.com/js/embed.min.js"></script>
    <section id="isso-thread"></section><br>
    <a href="/notes#continual_learning"><b>← Back to Notes</b></a>
  </main>
  <footer class="footer container h-10 text-center mt-1">
    <hr class="my-4">
    <ul class="pl-0 mt-1">
      <li class="first:before:content-none before:content-['•'] inline-block list-none">
        <a class="rss-link inline-block text-neutral-800 dark:text-neutral-400 border-none" href="/blog/index.xml" type="application/rss+xml" target="_blank">Blog <img class="rss-icon w-4 inline-block" src="https://hugocisneros.com/img/RSS.svg" alt="RSS feed icon"></a>
      </li>
      <li class="ml-2 first:before:content-none before:content-['•'] inline-block list-none">
        <a class="ml-2 text-neutral-800 dark:text-neutral-400 border-none" href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
      </li>
      <li class="ml-2 first:before:content-none before:content-['•'] text-neutral-800 dark:text-neutral-400 inline-block list-none"><span class="ml-2">© Hugo Cisneros 2022</span></li>
    </ul>
  </footer>
  <link rel="stylesheet" href="/js/katex/katex.min.css">
  <script src="/js/katex/katex.min.js"></script> 
  <script src="/js/katex/contrib/auto-render.min.js"></script> 
  <script>
  document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"\\[",right:"\\]",display:!0},{left:"\\begin{align}",right:"\\end{align}",display:!0},{left:"$$",right:"$$",display:!0},{left:"\\begin{equation}",right:"\\end{equation}",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})})
  </script>
  <script data-goatcounter="https://stats.hugocisneros.com/count" async src="//stats.hugocisneros.com/count.js"></script>
</body>
</html>
