<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Algorithm on Hugo Cisneros</title><link>https://hugocisneros.com/tags/algorithm/</link><description>Recent content in Algorithm on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 09 Sep 2022 09:34:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/algorithm/index.xml" rel="self" type="application/rss+xml"/><item><title>Frank-Wolfe algorithm</title><link>https://hugocisneros.com/notes/frank_wolfe_algorithm/</link><pubDate>Fri, 09 Sep 2022 09:34:00 +0200</pubDate><guid>https://hugocisneros.com/notes/frank_wolfe_algorithm/</guid><description>tags Optimization, Algorithm resources Fabian Pedregosa&amp;rsquo;s series on FW Definition It was originally published in (Frank, Wolfe 1956) and (Jaggi 2013) gives a more recent overview.
For a function \(f\) differentiable with $L$-Lipschitz gradients, and its domain \(\mathcal{C}\) is a convex and compact set, we want to solve the optimization problem:
\[ \min_{\boldsymbol{x} \in \mathcal{C}} f(\boldsymbol{x}) \]
The algorithm starts with an initial guess \(\boldsymbol{x}_0\) and constructs a sequence of values \(\boldsymbol{x}_1, \boldsymbol{x}_2, \cdots\) which converges to the solution.</description></item><item><title>Graham scan</title><link>https://hugocisneros.com/notes/graham_scan/</link><pubDate>Thu, 04 Aug 2022 14:10:00 +0200</pubDate><guid>https://hugocisneros.com/notes/graham_scan/</guid><description>tags Algorithm Graham scan is an algorithm to find the convex hull of a set of points in 2D. It runs with a time complexity of \(\mathcal{O}(n\log n)\).
The algorithm is relatively simple. It starts by selecting the point with lowest $y$-coordinate. At each step of the algorithm, remaining points are sorted by increasing order of the angle they and the last added point make. Then, if this new point is</description></item><item><title>Pattern-defeating quicksort</title><link>https://hugocisneros.com/notes/pattern_defeating_quicksort/</link><pubDate>Sun, 24 Apr 2022 13:19:00 +0200</pubDate><guid>https://hugocisneros.com/notes/pattern_defeating_quicksort/</guid><description> tags Algorithm resources Youtube, (Peters 2021) This is a sorting algorithm based on the well known quicksort algorithm. It uses an number of optimizations on top of the base algorithm:
Pivot selection Branchless partitioning Insertion sort base case Bounds check elimination Optimistic pre-sortedness Many equal values Breaking self-similarity \(O(n^2)\) worst-case prevention Bibliography Orson R. L. Peters. June 9, 2021. "Pattern-defeating Quicksort". Arxiv:2106.05123 [cs]. http://arxiv.org/abs/2106.05123.</description></item><item><title>Gradient descent</title><link>https://hugocisneros.com/notes/gradient_descent/</link><pubDate>Mon, 07 Mar 2022 16:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/gradient_descent/</guid><description>tags Optimization, Algorithm resources Slides by Christian S. Perone Fixed learning rate The simplest way to apply the gradient descent algorithm on a function \(g\) convex and $L-$smooth on \(\mathbb{R}^d\) is to use the parameter update:
\[ \theta_t = \theta_{t-1} - \gamma g&amp;rsquo;(\theta_{t-1}) \]
This is based on the standard first-order approximation of the function \(g\). It can be very sensitive to the learning rate and suffer from pathological curvature.</description></item><item><title>Levenshtein automata</title><link>https://hugocisneros.com/notes/levenshtein_automata/</link><pubDate>Mon, 07 Mar 2022 09:17:00 +0100</pubDate><guid>https://hugocisneros.com/notes/levenshtein_automata/</guid><description>tags Algorithm, Finite state machines resources Nick&amp;rsquo;s blog This is an algorithm used to find strings within a given Levenshtein distance of a target word.</description></item><item><title>Levenshtein distance</title><link>https://hugocisneros.com/notes/levenshtein_distance/</link><pubDate>Mon, 07 Mar 2022 09:15:00 +0100</pubDate><guid>https://hugocisneros.com/notes/levenshtein_distance/</guid><description> tags Algorithm, Natural language processing</description></item><item><title>Knuth-Morris-Pratt string-searching algorithm</title><link>https://hugocisneros.com/notes/knuth_morris_pratt_string_searching_algorithm/</link><pubDate>Tue, 24 Aug 2021 11:10:00 +0200</pubDate><guid>https://hugocisneros.com/notes/knuth_morris_pratt_string_searching_algorithm/</guid><description> tags Algorithm, Computer science resources Yurichev.com</description></item><item><title>Search algorithms</title><link>https://hugocisneros.com/notes/search_algorithms/</link><pubDate>Wed, 10 Feb 2021 15:16:00 +0100</pubDate><guid>https://hugocisneros.com/notes/search_algorithms/</guid><description> tags Algorithm</description></item><item><title>Stable marriage problem</title><link>https://hugocisneros.com/notes/stable_marriage_problem/</link><pubDate>Thu, 22 Oct 2020 09:42:00 +0200</pubDate><guid>https://hugocisneros.com/notes/stable_marriage_problem/</guid><description> tags Algorithm</description></item><item><title>Fast Marching method</title><link>https://hugocisneros.com/notes/fast_marching_method/</link><pubDate>Mon, 07 Sep 2020 10:30:00 +0200</pubDate><guid>https://hugocisneros.com/notes/fast_marching_method/</guid><description>tags Applied maths, Algorithm The fast marching method can be seen as a way to improve the metric issue with Dijkstra&amp;rsquo;s algorithm (which actually computes the \(\ell_1\) distance on a grid). The graph update is replaced with the Eikonal equation resolution in the FM method. This reduces the bias of using a grid and converges towards the underlying geodesic distance when the grid step size tends towards 0.
The FM algorithm replaces the graph update (\(D_j \leftarrow \min_{k \sim j} D_k + W_j\)) with a local resolution of the Eikonal equation</description></item><item><title>Dijkstra's algorithm</title><link>https://hugocisneros.com/notes/dijkstra_s_algorithm/</link><pubDate>Mon, 07 Sep 2020 10:28:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dijkstra_s_algorithm/</guid><description> tags Applied maths, Algorithm</description></item><item><title>Adaptive Computation Time</title><link>https://hugocisneros.com/notes/adaptive_computation_time/</link><pubDate>Tue, 21 Jul 2020 08:54:00 +0200</pubDate><guid>https://hugocisneros.com/notes/adaptive_computation_time/</guid><description>tags Neural networks, Algorithm Adaptive computation time (ACT) was introduced in (Graves 2017) as a way to make computations in RNN adaptive. The network learns how many computational steps to use before emitting an output.
This is done by outputting an extra halting probability at each update step, and considering two timelines:
the input timeline which plays the role of an outer loop, at each of those step, a new input symbol is fed to the RNN.</description></item></channel></rss>