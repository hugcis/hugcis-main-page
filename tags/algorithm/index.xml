<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Algorithm on Hugo Cisneros</title>
    <link>https://hugocisneros.com/tags/algorithm/</link>
    <description>Recent content in Algorithm on Hugo Cisneros</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Sep 2021 12:48:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/algorithm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Gradient descent</title>
      <link>https://hugocisneros.com/notes/gradient_descent/</link>
      <pubDate>Thu, 02 Sep 2021 12:48:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/gradient_descent/</guid>
      <description>tags Optimization, Algorithm resources Slides by Christian S. Perone  Fixed learning rate The simplest way to apply the gradient descent algorithm on a function \(g\) convex and $L-$smooth on \(\mathbb{R}^d\) is to use the parameter update:
\[ \theta_t = \theta_{t-1} - \gamma g&#39;(\theta_{t-1}) \]
This is based on the standard first-order approximation of the function \(g\). It can be very sensitive to the learning rate and suffer from pathological curvature.</description>
    </item>
    
    <item>
      <title>Knuth-Morris-Pratt string-searching algorithm</title>
      <link>https://hugocisneros.com/notes/knuth_morris_pratt_string_searching_algorithm/</link>
      <pubDate>Tue, 24 Aug 2021 11:10:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/knuth_morris_pratt_string_searching_algorithm/</guid>
      <description> tags Algorithm, Computer science resources Yurichev.com  </description>
    </item>
    
    <item>
      <title>Search algorithms</title>
      <link>https://hugocisneros.com/notes/search_algorithms/</link>
      <pubDate>Wed, 10 Feb 2021 15:16:00 +0100</pubDate>
      
      <guid>https://hugocisneros.com/notes/search_algorithms/</guid>
      <description> tags Algorithm  </description>
    </item>
    
    <item>
      <title>Stable marriage problem</title>
      <link>https://hugocisneros.com/notes/stable_marriage_problem/</link>
      <pubDate>Thu, 22 Oct 2020 09:42:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/stable_marriage_problem/</guid>
      <description> tags Algorithm  </description>
    </item>
    
    <item>
      <title>Fast Marching method</title>
      <link>https://hugocisneros.com/notes/fast_marching_method/</link>
      <pubDate>Mon, 07 Sep 2020 10:30:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/fast_marching_method/</guid>
      <description>tags Applied maths, Algorithm  The fast marching method can be seen as a way to improve the metric issue with Dijkstra&amp;rsquo;s algorithm (which actually computes the \(\ell_1\) distance on a grid). The graph update is replaced with the Eikonal equation resolution in the FM method. This reduces the bias of using a grid and converges towards the underlying geodesic distance when the grid step size tends towards 0.</description>
    </item>
    
    <item>
      <title>Dijkstra&#39;s algorithm</title>
      <link>https://hugocisneros.com/notes/dijkstra_s_algorithm/</link>
      <pubDate>Mon, 07 Sep 2020 10:28:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/dijkstra_s_algorithm/</guid>
      <description> tags Applied maths, Algorithm  </description>
    </item>
    
    <item>
      <title>Graham scan</title>
      <link>https://hugocisneros.com/notes/graham_scan/</link>
      <pubDate>Mon, 27 Jul 2020 22:28:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/graham_scan/</guid>
      <description>tags Algorithm  Graham scan is an algorithm to find the convex hull of a set of points in 2D. It runs with a time complexity of \(\mathcal{O}(n\log n)\).
The algorithm is relatively simple. It starts by selecting the point with lowest $y$-coordinate. At each step of the algorithm, remaining points are sorted by increasing order of the angle they and the last added point make. Then, if this new point is</description>
    </item>
    
    <item>
      <title>Adaptive Computation Time</title>
      <link>https://hugocisneros.com/notes/adaptive_computation_time/</link>
      <pubDate>Tue, 21 Jul 2020 08:54:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/adaptive_computation_time/</guid>
      <description>tags Neural networks, Algorithm  Adaptive computation time (ACT) was introduced in (Graves 2017) as a way to make computations in RNN adaptive. The network learns how many computational steps to use before emitting an output.
This is done by outputting an extra halting probability at each update step, and considering two timelines:
 the input timeline which plays the role of an outer loop, at each of those step, a new input symbol is fed to the RNN.</description>
    </item>
    
  </channel>
</rss>
