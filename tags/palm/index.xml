<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PaLM on Hugo Cisneros</title><link>https://hugocisneros.com/tags/palm/</link><description>Recent content in PaLM on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 26 Jul 2022 15:21:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/palm/index.xml" rel="self" type="application/rss+xml"/><item><title>Minerva</title><link>https://hugocisneros.com/notes/minerva/</link><pubDate>Tue, 26 Jul 2022 15:21:00 +0200</pubDate><guid>https://hugocisneros.com/notes/minerva/</guid><description> tags Transformers, Mathematics, PaLM paper (Lewkowycz et al. 2022) Architecture This model is PaLM fine-tuned on mathematical datasets.
Parameter count 540B
Bibliography Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, et al.. June 30, 2022. "Solving Quantitative Reasoning Problems with Language Models". arXiv. http://arxiv.org/abs/2206.14858.</description></item></channel></rss>