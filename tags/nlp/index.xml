<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Hugo Cisneros</title><link>https://hugocisneros.com/tags/nlp/</link><description>Recent content in NLP on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 12:19:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>GPT</title><link>https://hugocisneros.com/notes/gpt/</link><pubDate>Wed, 27 Jul 2022 12:19:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt/</guid><description> tags Transformers, NLP paper (Radford et al. 2018) Succesors The GPT architecture was improved upon and extended into GPT-2 and GPT-3. The original &amp;ldquo;GPT-1&amp;rdquo; was quickly abandoned in favor of its successor, but GPT is still used to refer to this family of models.
Parameter count 117M
Bibliography Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. 2018. "Improving Language Understanding by Generative Pre-training". OpenAI.</description></item><item><title>XLM-RoBERTa</title><link>https://hugocisneros.com/notes/xlm_roberta/</link><pubDate>Wed, 27 Jul 2022 12:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/xlm_roberta/</guid><description> tags Transformers, RoBERTa, NLP paper (Conneau et al. 2020) Architecture The model is an extension of RoBERTa that introduces small parameter tuning insights in the context of multilingual applications.
Parameter count Base = 270M Large = 550M Bibliography Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. April 7, 2020. "Unsupervised Cross-lingual Representation Learning at Scale". arXiv. DOI.</description></item><item><title>Wu Dao 2.0</title><link>https://hugocisneros.com/notes/wu_dao_2_0/</link><pubDate>Wed, 27 Jul 2022 11:52:00 +0200</pubDate><guid>https://hugocisneros.com/notes/wu_dao_2_0/</guid><description>tags Transformers, NLP website Wikipedia page for Wu Dao Architecture It is similar to GPT, being a decoder architecture but it applies a different pre-training task.
Parameter count 1.75T</description></item><item><title>Turing-NLG</title><link>https://hugocisneros.com/notes/turing_nlg/</link><pubDate>Wed, 27 Jul 2022 11:48:00 +0200</pubDate><guid>https://hugocisneros.com/notes/turing_nlg/</guid><description>tags Transformers, GPT, NLP website Microsoft Project Turing Architecture The architecture is similar to GPT-2 and GPT-3 with some parameter optimization and software/hardware platform to improve training.
Parameter count 17B originally, now up to 530B.</description></item><item><title>Transformer-XL</title><link>https://hugocisneros.com/notes/transformer_xl/</link><pubDate>Wed, 27 Jul 2022 11:42:00 +0200</pubDate><guid>https://hugocisneros.com/notes/transformer_xl/</guid><description> tags Transformers, NLP paper (Dai et al. 2019) Architecture This model uses relative positional embedding to enable using attention over longer contexts than the vanilla Transformer.
Parameter count 151M
Bibliography Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. June 2, 2019. "Transformer-xl: Attentive Language Models Beyond a Fixed-length Context". arXiv. DOI.</description></item><item><title>T5</title><link>https://hugocisneros.com/notes/t5/</link><pubDate>Wed, 27 Jul 2022 11:28:00 +0200</pubDate><guid>https://hugocisneros.com/notes/t5/</guid><description> tags Transformers, NLP paper (Raffel et al. 2020) Architecture It is the same as the original transformer with some relative positional embedding added (similar to Transformer-XL).
Parameter count 11B
Bibliography Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. July 28, 2020. "Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer". arXiv. DOI.</description></item><item><title>Switch transformer</title><link>https://hugocisneros.com/notes/switch_transformer/</link><pubDate>Wed, 27 Jul 2022 11:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/switch_transformer/</guid><description> tags Transformers, T5, NLP paper (Fedus et al. 2022) Architecture This model increases the parameter count of T5-like architecture while allowing efficient routing through different experts in a mixture of experts.
Parameter count 1T
Bibliography William Fedus, Barret Zoph, Noam Shazeer. June 16, 2022. "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity". arXiv. DOI.</description></item><item><title>RoBERTa</title><link>https://hugocisneros.com/notes/roberta/</link><pubDate>Wed, 27 Jul 2022 10:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/roberta/</guid><description> tags Transformers, BERT, NLP paper (Liu et al. 2019) Architecture This is an extension of BERT with more data and a better optimized training procedure.
Parameter count 356M
Bibliography Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. July 26, 2019. "Roberta: A Robustly Optimized BERT Pretraining Approach". arXiv. http://arxiv.org/abs/1907.11692.</description></item><item><title>Pegasus</title><link>https://hugocisneros.com/notes/pegasus/</link><pubDate>Wed, 27 Jul 2022 10:45:00 +0200</pubDate><guid>https://hugocisneros.com/notes/pegasus/</guid><description> tags Transformers, NLP paper (Zhang et al. 2020) Architecture This is a standard encoder/decoder architecture with a special pre-training task suited for summarization of text.
Parameter count Base = 223M Large = 568M Bibliography Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu. July 10, 2020. "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization". arXiv. http://arxiv.org/abs/1912.08777.</description></item><item><title>PaLM</title><link>https://hugocisneros.com/notes/palm/</link><pubDate>Wed, 27 Jul 2022 10:43:00 +0200</pubDate><guid>https://hugocisneros.com/notes/palm/</guid><description>tags Transformers, NLP paper (Chowdhery et al. 2022) Architecture This is a standard decoder-only architecture with some specific extensions:
SwiGLU activation functions Parallel layers Multi-query attention RoPE embeddings Shared input-output embeddings No biaises A 256k SentencePiece vocabulary generated from the training data Parameter count 540B
Bibliography Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al.. April 19, 2022. "Palm: Scaling Language Modeling with Pathways"</description></item><item><title>OPT: Open Pre-trained Transformer</title><link>https://hugocisneros.com/notes/opt/</link><pubDate>Wed, 27 Jul 2022 10:40:00 +0200</pubDate><guid>https://hugocisneros.com/notes/opt/</guid><description> tags Transformers, GPT, NLP paper (Zhang et al. 2022) Architecture It is the same architecture as GPT-3 but with some training improvements from Megatron.
Parameter count 175B
Bibliography Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, et al.. June 21, 2022. "OPT: Open Pre-trained Transformer Language Models". arXiv. http://arxiv.org/abs/2205.01068.</description></item><item><title>mBART</title><link>https://hugocisneros.com/notes/mbart/</link><pubDate>Tue, 26 Jul 2022 15:15:00 +0200</pubDate><guid>https://hugocisneros.com/notes/mbart/</guid><description> tags Transformers, NLP, BART paper (Liu et al. 2020) Architecture It&amp;rsquo;s an encoder-decoder architecture based on BART
Bibliography Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer. January 23, 2020. "Multilingual Denoising Pre-training for Neural Machine Translation". arXiv. DOI.</description></item><item><title>LAMDA</title><link>https://hugocisneros.com/notes/lamda/</link><pubDate>Tue, 26 Jul 2022 11:53:00 +0200</pubDate><guid>https://hugocisneros.com/notes/lamda/</guid><description> tags Transformers, NLP paper (Thoppilan et al. 2022) Parameter count 137B
Bibliography Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, et al.. February 10, 2022. "Lamda: Language Models for Dialog Applications". arXiv. http://arxiv.org/abs/2201.08239.</description></item><item><title>Jurassic-1</title><link>https://hugocisneros.com/notes/jurassic_1/</link><pubDate>Tue, 26 Jul 2022 11:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/jurassic_1/</guid><description> tags Transformers, GPT, NLP blog post AI21Labs blog Architecture This model is similar to GPT-3 with an improved tokenizer that increases the learning efficiency. It also has more parameters.
Parameter count 178B
Bibliography</description></item><item><title>Imagen</title><link>https://hugocisneros.com/notes/imagen/</link><pubDate>Tue, 26 Jul 2022 11:41:00 +0200</pubDate><guid>https://hugocisneros.com/notes/imagen/</guid><description>tags Transformers, Diffusion models, Computer vision, NLP, T5, CLIP paper (Saharia et al. 2022) Architecture This is based on the U-net diffusion architecture with a few extensions. T5 or CLIP or BERT is used as a frozen text encoder.
Parameter count 2B
Bibliography Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, et al.. May 23, 2022. "Photorealistic Text-to-image Diffusion Models with Deep Language Understanding"</description></item><item><title>GPTInstruct</title><link>https://hugocisneros.com/notes/gptinstruct/</link><pubDate>Tue, 26 Jul 2022 11:10:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gptinstruct/</guid><description> tags Transformers, GPT, NLP paper (Ouyang et al. 2022) Architecture This model starts off from a pretrained GPT-3. Reward modeling is added with Reinforcement learning.
Parameter count 175B
Bibliography Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al.. March 4, 2022. "Training Language Models to Follow Instructions with Human Feedback". arXiv. DOI.</description></item><item><title>GPT-Neo</title><link>https://hugocisneros.com/notes/gpt_neo/</link><pubDate>Tue, 26 Jul 2022 11:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_neo/</guid><description> tags Transformers, GPT, NLP software &amp;lt;&amp;amp;gpt-neo&amp;gt; Architecture This model is very similar to GPT-2, with the addition of local attention every other layer and a window size of 256 tokens.
Parameter count 1.5B, 2.7B (XL)
Bibliography</description></item><item><title>GLIDE</title><link>https://hugocisneros.com/notes/glide/</link><pubDate>Tue, 26 Jul 2022 11:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/glide/</guid><description> tags Diffusion models, NLP, Computer vision paper (Nichol et al. 2022) Architecture This model uses joint textual and visual embedding diffusion model followed by some upsampling.
Parameter count 3.5B
Bibliography Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen. March 8, 2022. "GLIDE: Towards Photorealistic Image Generation and Editing with Text-guided Diffusion Models". arXiv. DOI.</description></item><item><title>GPT-3</title><link>https://hugocisneros.com/notes/gpt_3/</link><pubDate>Tue, 26 Jul 2022 10:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_3/</guid><description> tags Transformers, NLP, GPT paper (Brown et al. 2020) Architecture Like GPT-2, with the addition of locally banded sparse attention.
Parameter count 175B
Bibliography Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al.. June 4, 2020. "Language Models Are Few-shot Learners". Arxiv:2005.14165 [cs]. http://arxiv.org/abs/2005.14165.</description></item><item><title>GLaM</title><link>https://hugocisneros.com/notes/glam/</link><pubDate>Tue, 26 Jul 2022 10:01:00 +0200</pubDate><guid>https://hugocisneros.com/notes/glam/</guid><description>tags Transformers, NLP paper (Du et al. 2021) Architecture The model is a mixture of 64 expert decoder-only transformer architectures. Two experts are activated per token, making the model relatively efficient for its number of parameters
Parameter count 1.2T total, 96B active per token.
Bibliography Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, et al.. December 13, 2021. "Glam: Efficient Scaling of Language Models with Mixture-of-experts"</description></item><item><title>Flamingo</title><link>https://hugocisneros.com/notes/flamingo/</link><pubDate>Tue, 26 Jul 2022 09:56:00 +0200</pubDate><guid>https://hugocisneros.com/notes/flamingo/</guid><description> tags Transformers, Computer vision, NLP, Chinchilla paper (Alayrac et al. 2022) Architecture Uses a frozen language model (e.g. Chinchilla) that is conditioned on a visual representation given from a normalizer-free ResNet.
Parameter count 80B
Bibliography Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al.. April 29, 2022. "Flamingo: A Visual Language Model for Few-shot Learning". arXiv. http://arxiv.org/abs/2204.14198.</description></item><item><title>ERNIE</title><link>https://hugocisneros.com/notes/ernie/</link><pubDate>Tue, 26 Jul 2022 09:51:00 +0200</pubDate><guid>https://hugocisneros.com/notes/ernie/</guid><description> tags Transformers, BERT, NLP paper (Zhang et al. 2019) Architecture This transformer uses two stacked BERT for encoding: one for the text, one for the entities in a knowledge graph.
Parameter count 114M
Bibliography Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu. June 4, 2019. "ERNIE: Enhanced Language Representation with Informative Entities". arXiv. DOI.</description></item><item><title>ELECTRA</title><link>https://hugocisneros.com/notes/electra/</link><pubDate>Tue, 26 Jul 2022 09:08:00 +0200</pubDate><guid>https://hugocisneros.com/notes/electra/</guid><description> tags Transformers, NLP paper (Clark et al. 2020) Paramter count Base = 110M Large = 330M Bibliography Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. 2020. "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators". In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=r1xMH1BtvB.</description></item><item><title>DQ-BART</title><link>https://hugocisneros.com/notes/dq_bart/</link><pubDate>Tue, 26 Jul 2022 09:05:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dq_bart/</guid><description> tags Transformers, BART, NLP paper (Li et al. 2022) Architecture It is a distilled and quantized version of BART. It improves performance as well as the model size.
Bibliography Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing Xiang, Dan Roth. March 21, 2022. "DQ-BART: Efficient Sequence-to-sequence Model via Joint Distillation and Quantization". arXiv. DOI.</description></item><item><title>DistillBERT</title><link>https://hugocisneros.com/notes/distillbert/</link><pubDate>Tue, 26 Jul 2022 08:43:00 +0200</pubDate><guid>https://hugocisneros.com/notes/distillbert/</guid><description> tags Transformers, BERT, NLP paper (Sanh et al. 2020) Architecture It is a distilled version of BERT that is much more efficient.
Parameter count 66M
Bibliography Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. February 29, 2020. "Distilbert, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter". arXiv. DOI.</description></item><item><title>DialoGPT</title><link>https://hugocisneros.com/notes/dialogpt/</link><pubDate>Fri, 22 Jul 2022 13:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dialogpt/</guid><description> tags GPT, Transformers, NLP paper (Zhang et al. 2020) Architecture It is exactly like a GPT-2 architecture but trained on dialog data.
Parameter count 1.5B
Bibliography Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan. May 2, 2020. "Dialogpt: Large-scale Generative Pre-training for Conversational Response Generation". arXiv. DOI.</description></item><item><title>ALBERT</title><link>https://hugocisneros.com/notes/albert/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/albert/</guid><description>tags Transformers, BERT, NLP paper (Lan et al. 2020) Architecture It is an encoder-only architecture. It extends BERT by using parameter-sharing and is more efficient than BERT with the same number of parameters.
Parameter count Base = 12M Large = 18M XLarge = 60M Bibliography Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. February 8, 2020. "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"</description></item><item><title>BERT</title><link>https://hugocisneros.com/notes/bert/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/bert/</guid><description> tags Transformers, NLP paper (Devlin et al. 2019) Parameter count Base = 110M Large = 340M Bibliography Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. May 24, 2019. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". arXiv. DOI.</description></item><item><title>BLOOM</title><link>https://hugocisneros.com/notes/bloom/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/bloom/</guid><description> tags Transformers, GPT, NLP blog post BLOOM announcement blog post Architecture It is similar to the architecture of GPT-3, using full attention instead of sparse attention.
Parameter count 176B
Bibliography</description></item><item><title>CTRL</title><link>https://hugocisneros.com/notes/ctrl/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/ctrl/</guid><description>tags Transformers, NLP paper (Keskar et al. 2019) Architecture This is a model that can generate text conditioned on control codes that specify the domain, style, topics, dates, entities, relationships between entities, plot points, and task-related behavior of the text.
Parameter count 1.63B
Bibliography Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher. September 20, 2019. "CTRL: A Conditional Transformer Language Model for Controllable Generation". arXiv. DOI.</description></item><item><title>Big bird</title><link>https://hugocisneros.com/notes/big_bird/</link><pubDate>Fri, 22 Jul 2022 13:01:00 +0200</pubDate><guid>https://hugocisneros.com/notes/big_bird/</guid><description>tags Transformers, NLP paper (Zaheer et al. 2021) Architecture Big bird can be used as both an encoder-only and an encoder/decoder architecture.
It extends the likes of BERT by implementing a sparse attention mechanism, making the attention computational complexity less than quadratic.
Bibliography Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al.. January 8, 2021. "Big Bird: Transformers for Longer Sequences". arXiv. DOI.</description></item><item><title>CLIP</title><link>https://hugocisneros.com/notes/clip/</link><pubDate>Fri, 22 Jul 2022 12:29:00 +0200</pubDate><guid>https://hugocisneros.com/notes/clip/</guid><description> tags Transformers, NLP, Computer vision paper (Radford et al. 2021) Architecture It is an encoder-only model which combines ViT and ResNet to encode images and a transformer for the text encoding.
Bibliography Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al.. February 26, 2021. "Learning Transferable Visual Models from Natural Language Supervision". arXiv. DOI.</description></item><item><title>Chinchilla</title><link>https://hugocisneros.com/notes/chinchilla/</link><pubDate>Fri, 22 Jul 2022 12:27:00 +0200</pubDate><guid>https://hugocisneros.com/notes/chinchilla/</guid><description> tags Transformers, GPT, NLP paper (Hoffmann et al. 2022) Architecture This model is very similar to Gopher, with some improvements to make the model smaller and more efficient.
Parameter count 70B
Bibliography Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al.. March 29, 2022. "Training Compute-optimal Large Language Models". arXiv. DOI.</description></item><item><title>Text classification</title><link>https://hugocisneros.com/notes/text_classification/</link><pubDate>Mon, 02 May 2022 10:43:00 +0200</pubDate><guid>https://hugocisneros.com/notes/text_classification/</guid><description> tags NLP resources (Minaee et al. 2020) A few examples are often cited as major applications of text classification:
Spam detection Sentiment analysis Auto-tagging Categorization into topics Bibliography Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, Jianfeng Gao. April 5, 2020. "Deep Learning Based Text Classification: A Comprehensive Review". Arxiv:2004.03705 [cs, Stat]. http://arxiv.org/abs/2004.03705.</description></item><item><title>Byte-pair encoding</title><link>https://hugocisneros.com/notes/byte_pair_encoding/</link><pubDate>Thu, 14 Apr 2022 17:28:00 +0200</pubDate><guid>https://hugocisneros.com/notes/byte_pair_encoding/</guid><description>tags NLP The process of byte-pair encoding can be summarized as follow:
Each character is a token Find pairs that occur most often Create a new token that encoded those common pairs Repeat the process until target vocabulary size is reached The output of this process is both a vocabulary and a set of merging rules for tokens to be used to process more data.
This technique has several advantages:</description></item><item><title>Language modeling</title><link>https://hugocisneros.com/notes/language_modeling/</link><pubDate>Mon, 11 Apr 2022 15:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/language_modeling/</guid><description>tags NLP LM with RNNs Different models have been studied, starting from the initial recurrent neural network based language model (Mikolov et al. 2011). Recurrent neural networks
LSTM were then used with more success than previous models (Zaremba et al. 2015).
Recently, transformers seem to have dominated language modeling. However it is not clear if this is due to their real superiority over RNNs or their practical scalability (Merity 2019).</description></item><item><title>Word vectors</title><link>https://hugocisneros.com/notes/word_vectors/</link><pubDate>Thu, 07 Apr 2022 19:33:00 +0200</pubDate><guid>https://hugocisneros.com/notes/word_vectors/</guid><description>tags NLP Definition Word vectors are abstract representation of words embedded in a dense space.
They are closely related to Language modeling, since the implicit representation a language model builds for prediction can often be used as a word (or sentence) vector.
Word vectors can be extracted from the intermediate representations of RNNs or transformers. They can also be created with dedicated algorithms such as Word2Vec.
Usage Word vectors can encode interesting information, such as semantic similarity between words.</description></item><item><title>Notes on: One model for the learning of language by Yang, Y., &amp; Piantadosi, S. T. (2022)</title><link>https://hugocisneros.com/notes/yangonemodellearning2022/</link><pubDate>Mon, 31 Jan 2022 13:55:00 +0100</pubDate><guid>https://hugocisneros.com/notes/yangonemodellearning2022/</guid><description>source (Yang, Piantadosi 2022) tags NLP, Artificial Intelligence, Machine learning Summary This paper introduces a model for learning language from few examples while generalizing effectively.
This model builds sentences with function that are combinations of elementary functions, including:
pair(L, C) : Concatenates character C onto list L first(L) : Return the first character of L flip(P) : Return true with probability P if(B, X, Y) : Return X if B else return Y (X and Y may be lists, sets, or probabilities) etc.</description></item><item><title>Notes on: Thinking Like Transformers by Weiss, G., Goldberg, Y., &amp; Yahav, E. (2021)</title><link>https://hugocisneros.com/notes/weissthinkingtransformers2021/</link><pubDate>Wed, 25 Aug 2021 15:19:00 +0200</pubDate><guid>https://hugocisneros.com/notes/weissthinkingtransformers2021/</guid><description>tags NLP, Computer science source (Weiss et al. 2021) Summary This paper introduces a programming language that is inspired by the way Transformers process input data. The language is called Restricted Access Sequence Processing Language (RASP).
Data is represented as sequences, which is the structure transformers manipulate (since they have been designed for NLP applications). The language has two types of internal data representation:
Sequence operators (s-ops) are functions that translate sequences into sequences.</description></item><item><title>Notes on: The geometry of integration in text classification RNNs by Aitken, K., Ramasesh, V. V., Garg, A., Cao, Y., Sussillo, D., &amp; Maheswaranathan, N. (2020)</title><link>https://hugocisneros.com/notes/aitkengeometryintegrationtext2020/</link><pubDate>Thu, 25 Mar 2021 10:20:00 +0100</pubDate><guid>https://hugocisneros.com/notes/aitkengeometryintegrationtext2020/</guid><description>tags RNN, NLP source (Aitken et al. 2020) Summary This paper takes a dynamical system based approach to study learning in RNNs. Gradient descent optimization in RNNs allows them to learn a simplified form of memory and information processing.
The authors use simple text classification tasks to try and understand if these learned properties can be understood by looking at the state dynamics of RNNs.
The RNNs usually behave like attractor networks, with the hidden state lying on a low-dimensional manifold.</description></item><item><title>Notes on: Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data by Bender, E. M., &amp; Koller, A. (2020)</title><link>https://hugocisneros.com/notes/benderclimbingnlumeaning2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/benderclimbingnlumeaning2020/</guid><description>source (Bender, Koller 2020) tags NLP, Artificial Intelligence, Evaluating NLP Summary The main point of the article could be summarized like so:
We argue that the language modeling task, because it only uses form as training data, cannot in principle lead to learning of meaning. We take the term language model to refer to any system trained only on the task of string prediction, whether it operates over characters, words or sentences, and sequentially or not.</description></item><item><title>Regular expressions</title><link>https://hugocisneros.com/notes/regular_expressions/</link><pubDate>Tue, 16 Feb 2021 21:10:00 +0100</pubDate><guid>https://hugocisneros.com/notes/regular_expressions/</guid><description> tags Coding, NLP</description></item><item><title>Posos Challenge</title><link>https://hugocisneros.com/projects/posos_dl/</link><pubDate>Mon, 25 Mar 2019 12:00:00 +0000</pubDate><guid>https://hugocisneros.com/projects/posos_dl/</guid><description>&lt;h3 id="links">Links&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://hugocisneros.com/pdf/Cisneros_slides_posos.pdf">Project slides&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hugocisneros.com/pdf/Cisneros_report_posos.pdf">PDF project report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Data journalism extractor</title><link>https://hugocisneros.com/projects/data_journalism_extractor/</link><pubDate>Mon, 29 Oct 2018 12:00:00 +0000</pubDate><guid>https://hugocisneros.com/projects/data_journalism_extractor/</guid><description>&lt;h3 id="links">Links&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/hugcis/data_journalism_extractor">Github repo&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://data-journalism-extractor.readthedocs.io/en/latest/">Online documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hugocisneros.com/pdf/report_data_journalism.pdf">Report (in French)&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Arxiv explorer</title><link>https://hugocisneros.com/projects/arxiv_explorer/</link><pubDate>Fri, 26 Oct 2018 12:00:00 +0000</pubDate><guid>https://hugocisneros.com/projects/arxiv_explorer/</guid><description>Github repo Description This project was about creating a tool similar to Arxiv Sanity with additional NLP functionalities for finding similar papers from their abstract.
I used a concept from [1], which uses earth mover&amp;rsquo;s distance metric between documents represented as normalized bag-of-words. The underlying transport cost between two words is given by their distance in a pre-trained word vector space. The app trains word vectors on all Arxiv abstracts and uses the EMD based metric to compute similarities between papers.</description></item></channel></rss>