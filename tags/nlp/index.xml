<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP on Hugo Cisneros</title><link>https://hugocisneros.com/tags/nlp/</link><description>Recent content in NLP on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 11 Apr 2022 15:07:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Language modeling</title><link>https://hugocisneros.com/notes/language_modeling/</link><pubDate>Mon, 11 Apr 2022 15:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/language_modeling/</guid><description>tags NLP LM with RNNs Different models have been studied, starting from the initial recurrent neural network based language model (Mikolov et al. 2011). Recurrent neural networks
LSTM were then used with more success than previous models (Zaremba et al. 2015).
Recently, transformers seem to have dominated language modeling. However it is not clear if this is due to their real superiority over RNNs or their practical scalability (Merity 2019).</description></item><item><title>Byte-pair encoding</title><link>https://hugocisneros.com/notes/byte_pair_encoding/</link><pubDate>Mon, 11 Apr 2022 14:26:00 +0200</pubDate><guid>https://hugocisneros.com/notes/byte_pair_encoding/</guid><description>tags NLP The process of byte-pair encoding can be summarized as follow:
Each character is a token Find pairs that occur most often Create a new token that encoded those common pairs Repeat the process until target vocabulary size is reached The output of this process is both a vocabulary and a set of merging rules for tokens to be used to process more data.
This technique has several advantages:</description></item><item><title>Word vectors</title><link>https://hugocisneros.com/notes/word_vectors/</link><pubDate>Thu, 07 Apr 2022 19:33:00 +0200</pubDate><guid>https://hugocisneros.com/notes/word_vectors/</guid><description>tags NLP Definition Word vectors are abstract representation of words embedded in a dense space.
They are closely related to Language modeling, since the implicit representation a language model builds for prediction can often be used as a word (or sentence) vector.
Word vectors can be extracted from the intermediate representations of RNNs or transformers. They can also be created with dedicated algorithms such as Word2Vec.
Usage Word vectors can encode interesting information, such as semantic similarity between words.</description></item><item><title>Notes on: One model for the learning of language by Yang, Y., &amp; Piantadosi, S. T. (2022)</title><link>https://hugocisneros.com/notes/yangonemodellearning2022/</link><pubDate>Mon, 31 Jan 2022 13:55:00 +0100</pubDate><guid>https://hugocisneros.com/notes/yangonemodellearning2022/</guid><description>source (Yang, Piantadosi 2022) tags NLP, Artificial Intelligence, Machine learning Summary This paper introduces a model for learning language from few examples while generalizing effectively.
This model builds sentences with function that are combinations of elementary functions, including:
pair(L, C) : Concatenates character C onto list L first(L) : Return the first character of L flip(P) : Return true with probability P if(B, X, Y) : Return X if B else return Y (X and Y may be lists, sets, or probabilities) etc.</description></item><item><title>Notes on: Thinking Like Transformers by Weiss, G., Goldberg, Y., &amp; Yahav, E. (2021)</title><link>https://hugocisneros.com/notes/weissthinkingtransformers2021/</link><pubDate>Wed, 25 Aug 2021 15:19:00 +0200</pubDate><guid>https://hugocisneros.com/notes/weissthinkingtransformers2021/</guid><description>tags NLP, Computer science source (Weiss et al. 2021) Summary This paper introduces a programming language that is inspired by the way Transformers process input data. The language is called Restricted Access Sequence Processing Language (RASP).
Data is represented as sequences, which is the structure transformers manipulate (since they have been designed for NLP applications). The language has two types of internal data representation:
Sequence operators (s-ops) are functions that translate sequences into sequences.</description></item><item><title>Notes on: The geometry of integration in text classification RNNs by Aitken, K., Ramasesh, V. V., Garg, A., Cao, Y., Sussillo, D., &amp; Maheswaranathan, N. (2020)</title><link>https://hugocisneros.com/notes/aitkengeometryintegrationtext2020/</link><pubDate>Thu, 25 Mar 2021 10:20:00 +0100</pubDate><guid>https://hugocisneros.com/notes/aitkengeometryintegrationtext2020/</guid><description>tags RNN, NLP source (Aitken et al. 2020) Summary This paper takes a dynamical system based approach to study learning in RNNs. Gradient descent optimization in RNNs allows them to learn a simplified form of memory and information processing.
The authors use simple text classification tasks to try and understand if these learned properties can be understood by looking at the state dynamics of RNNs.
The RNNs usually behave like attractor networks, with the hidden state lying on a low-dimensional manifold.</description></item><item><title>Notes on: Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data by Bender, E. M., &amp; Koller, A. (2020)</title><link>https://hugocisneros.com/notes/benderclimbingnlumeaning2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/benderclimbingnlumeaning2020/</guid><description>source (Bender, Koller 2020) tags NLP, Artificial Intelligence, Evaluating NLP Summary The main point of the article could be summarized like so:
We argue that the language modeling task, because it only uses form as training data, cannot in principle lead to learning of meaning. We take the term language model to refer to any system trained only on the task of string prediction, whether it operates over characters, words or sentences, and sequentially or not.</description></item><item><title>Regular expressions</title><link>https://hugocisneros.com/notes/regular_expressions/</link><pubDate>Tue, 16 Feb 2021 21:10:00 +0100</pubDate><guid>https://hugocisneros.com/notes/regular_expressions/</guid><description> tags Coding, NLP</description></item><item><title>Text classification</title><link>https://hugocisneros.com/notes/text_classification/</link><pubDate>Fri, 26 Jun 2020 11:13:00 +0200</pubDate><guid>https://hugocisneros.com/notes/text_classification/</guid><description> tags NLP (Minaee et al. 2020)
Bibliography Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, Jianfeng Gao. April 5, 2020. "Deep Learning Based Text Classification: A Comprehensive Review". Arxiv:2004.03705 [cs, Stat]. http://arxiv.org/abs/2004.03705.</description></item><item><title>Posos Challenge</title><link>https://hugocisneros.com/projects/posos_dl/</link><pubDate>Mon, 25 Mar 2019 12:00:00 +0000</pubDate><guid>https://hugocisneros.com/projects/posos_dl/</guid><description>&lt;h3 id="links">Links&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://hugocisneros.com/pdf/Cisneros_slides_posos.pdf">Project slides&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hugocisneros.com/pdf/Cisneros_report_posos.pdf">PDF project report&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Data journalism extractor</title><link>https://hugocisneros.com/projects/data_journalism_extractor/</link><pubDate>Mon, 29 Oct 2018 12:00:00 +0000</pubDate><guid>https://hugocisneros.com/projects/data_journalism_extractor/</guid><description>&lt;h3 id="links">Links&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/hugcis/data_journalism_extractor">Github repo&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://data-journalism-extractor.readthedocs.io/en/latest/">Online documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://hugocisneros.com/pdf/report_data_journalism.pdf">Report (in French)&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Arxiv explorer</title><link>https://hugocisneros.com/projects/arxiv_explorer/</link><pubDate>Fri, 26 Oct 2018 12:00:00 +0000</pubDate><guid>https://hugocisneros.com/projects/arxiv_explorer/</guid><description>Github repo Description This project was about creating a tool similar to Arxiv Sanity with additional NLP functionalities for finding similar papers from their abstract.
I used a concept from [1], which uses earth mover&amp;rsquo;s distance metric between documents represented as normalized bag-of-words. The underlying transport cost between two words is given by their distance in a pre-trained word vector space. The app trains word vectors on all Arxiv abstracts and uses the EMD based metric to compute similarities between papers.</description></item></channel></rss>