<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Memory in neural networks on Hugo Cisneros</title><link>https://hugocisneros.com/tags/memory-in-neural-networks/</link><description>Recent content in Memory in neural networks on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 25 May 2022 13:26:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/memory-in-neural-networks/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on: Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., &amp; Szegedy, C. (2022)</title><link>https://hugocisneros.com/notes/wumemorizingtransformers2022/</link><pubDate>Wed, 25 May 2022 13:26:00 +0200</pubDate><guid>https://hugocisneros.com/notes/wumemorizingtransformers2022/</guid><description>source (Wu et al. 2022) tags Transformers, Memory in neural networks Summary This paper introduces a method to extend the classical Transformer neural network model with an addressable memory that can be queried and updated at inference time.
This memory is addressed using an attention mechanism. It is a set of cached attention (key, value) vector pairs. At some arbitrary depth of the attention &amp;ldquo;stack&amp;rdquo; the memory mechanism is inserted.</description></item></channel></rss>