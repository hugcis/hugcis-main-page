<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Complexity metrics on Hugo Cisneros</title><link>https://hugocisneros.com/tags/complexity-metrics/</link><description>Recent content in Complexity metrics on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 25 Mar 2021 09:58:00 +0100</lastBuildDate><atom:link href="https://hugocisneros.com/tags/complexity-metrics/index.xml" rel="self" type="application/rss+xml"/><item><title>Entropy</title><link>https://hugocisneros.com/notes/entropy/</link><pubDate>Thu, 25 Mar 2021 09:58:00 +0100</pubDate><guid>https://hugocisneros.com/notes/entropy/</guid><description>tags Complexity metrics references (Shannon, Weaver 1975) For a discrete random variable \(X\) with outcomes \(x_i\), \(P(X=x_i) = P_i\), the entropy or uncertainty function of \(X\) is defined as \[ H(X) = -\sum_{i=1}^{N} P_i \log P_i \]
Entropy is always positive, and is maximized when the uncertainty is maximal, that is when \(P_1 = P_2 = &amp;hellip; = P_N = \frac{1}{N}\) entropy in that case is \(\log N\).</description></item><item><title>Assembly theory</title><link>https://hugocisneros.com/notes/assembly_theory/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/assembly_theory/</guid><description>tags Complexity metrics papers (Marshall et al. 2019) This complexity metric is based on ideas similar to Logical depth, where instead of just looking at the general process that led to the creation of an object, we also look at the number of elementary steps in that process.
Bibliography Stuart M Marshall, Douglas G Moore, Alastair R G Murray, Sara I Walker. July 2019. "Quantifying the Pathways to Life Using Assembly Spaces"</description></item><item><title>Statistical complexity</title><link>https://hugocisneros.com/notes/statistical_complexity/</link><pubDate>Wed, 29 Jul 2020 14:24:00 +0200</pubDate><guid>https://hugocisneros.com/notes/statistical_complexity/</guid><description>tags Complexity metrics papers (Crutchfield, Young 1989) One interpretation of the statistical complexity is that it is the minimum amount of historical information required to make optimal forecasts of bits in \(x\) at the error rate \(h_\mu\).
For periodic sequences, \(C_\mu(x) = 0\) and for ideal random sequences \(C_\mu(x) = 0\) too.
Several researchers have tried to capture the properties of statistical complexity with practical alternatives. The resulting complexity metrics include:</description></item><item><title>Effective measure complexity</title><link>https://hugocisneros.com/notes/effective_measure_complexity/</link><pubDate>Tue, 14 Jul 2020 08:30:00 +0200</pubDate><guid>https://hugocisneros.com/notes/effective_measure_complexity/</guid><description> tags Complexity metrics</description></item><item><title>Logical depth</title><link>https://hugocisneros.com/notes/logical_depth/</link><pubDate>Tue, 14 Jul 2020 08:21:00 +0200</pubDate><guid>https://hugocisneros.com/notes/logical_depth/</guid><description>tags Complexity metrics references (Bennett 1995) Logical depth can be defined as the run time of the Turing Machine that uses the minimal representation for an input \(x\), \(M_{\min}(x)\) &amp;mdash; which is also its Kolmogorov complexity . It is therefore uncomputable (because the minimal representation is uncomputable).
Bibliography Charles H. Bennett. 1995. "Logical Depth and Physical Complexity". In The Universal Turing Machine A Half-century Survey, edited by Rolf Herken, 2:207â€“35.</description></item></channel></rss>