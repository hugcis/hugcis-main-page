<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural language processing on Hugo Cisneros</title><link>https://hugocisneros.com/tags/natural-language-processing/</link><description>Recent content in Natural language processing on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 17 Jul 2020 13:46:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Evaluating NLP</title><link>https://hugocisneros.com/notes/evaluating_nlp/</link><pubDate>Fri, 17 Jul 2020 13:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/evaluating_nlp/</guid><description>tags Natural language processing Language model evaluation Perplexity For a given word sequence \(\mathbf{w} = (w_1, &amp;hellip;, w_n)\), perplexity (PPL) is defined \[ PPL = 2^{-\frac{1}{n} \sum_{i=1}^n \log_2 P(w_i |Â w_{i-1} &amp;hellip; w_1 )} \] It can be seen as the cross-entropy between an empirical distribution of test words and the predicted conditional word distribution. A language model that would encode each word with an average 8 bits has a perplexity of 256 (\(2^8\)).</description></item></channel></rss>