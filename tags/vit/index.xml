<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ViT on Hugo Cisneros</title><link>https://hugocisneros.com/tags/vit/</link><description>Recent content in ViT on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 11:04:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/vit/index.xml" rel="self" type="application/rss+xml"/><item><title>Swin Transformer</title><link>https://hugocisneros.com/notes/swin_transformer/</link><pubDate>Wed, 27 Jul 2022 11:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/swin_transformer/</guid><description> tags Transformers, ViT, Computer vision paper (Liu et al. 2021) Architecture This model extends ViT by replace the multi-head self-attention with a &amp;ldquo;shifted windows&amp;rdquo; module allowing ViT to work with higher resolution images.
Parameter count 29M - 197M
Bibliography Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. August 17, 2021. "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows". arXiv. DOI.</description></item><item><title>Global context ViT</title><link>https://hugocisneros.com/notes/global_context_vit/</link><pubDate>Tue, 26 Jul 2022 11:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/global_context_vit/</guid><description> tags Transformers, Computer vision, ViT paper (Hatamizadeh et al. 2022) Architecture This is a hierarchical version of ViT with both local and global attention.
Parameter count 90M
Bibliography Ali Hatamizadeh, Hongxu Yin, Jan Kautz, Pavlo Molchanov. June 20, 2022. "Global Context Vision Transformers". arXiv. DOI.</description></item></channel></rss>