<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Evaluating NLP on Hugo Cisneros</title><link>https://hugocisneros.com/tags/evaluating-nlp/</link><description>Recent content in Evaluating NLP on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 04 Aug 2022 13:42:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/evaluating-nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Semantic similarity</title><link>https://hugocisneros.com/notes/semantic_similarity/</link><pubDate>Thu, 04 Aug 2022 13:42:00 +0200</pubDate><guid>https://hugocisneros.com/notes/semantic_similarity/</guid><description>tags NLP, Evaluating NLP N-gram matching For two sequences \(x\) and \(\hat{x}\), we denote the sequence of $n$-grams with \(S_x^n\) and \(S^n_{\hat{x}}\). The number of matched $n$-grams between the two sentences is: \[ \sum_{w \in S_{\hat{x}}^n} \mathbb{I}[w \in S_{x}^n ] \] with \(\mathbb{I}\) the indicator function.
From this we can construct the exact match precision (Exact-\(P_n\)) and recall (Exact-\(R_n\)): \[ \text{Exact-}$P_n$ = \frac{\sum_{w \in S_{\hat{x}}^n} \mathbb{I}[w \in S_{x}^n ]}{| S_{\hat{x}}^n|} \] and \[ \text{Exact-}$R_n$ = \frac{\sum_{w \in S_{x}^n} \mathbb{I}[w \in S_{\hat{x}}^n ]}{| S_{x}^n|} \]</description></item><item><title>Notes on: Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data by Bender, E. M., &amp; Koller, A. (2020)</title><link>https://hugocisneros.com/notes/benderclimbingnlumeaning2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/benderclimbingnlumeaning2020/</guid><description>source (Bender, Koller 2020) tags NLP, Artificial Intelligence, Evaluating NLP Summary The main point of the article could be summarized like so:
We argue that the language modeling task, because it only uses form as training data, cannot in principle lead to learning of meaning. We take the term language model to refer to any system trained only on the task of string prediction, whether it operates over characters, words or sentences, and sequentially or not.</description></item><item><title>Notes on: Information-Theoretic Probing with Minimum Description Length by Voita, E., &amp; Titov, I. (2020)</title><link>https://hugocisneros.com/notes/voitainformationtheoreticprobingminimum2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/voitainformationtheoreticprobingminimum2020/</guid><description> tags Evaluating NLP, Transformers, Minimum description length source (Voita, Titov 2020) Summary Comments Bibliography Elena Voita, Ivan Titov. March 27, 2020. "Information-theoretic Probing with Minimum Description Length". Arxiv:2003.12298 [cs]. http://arxiv.org/abs/2003.12298.</description></item></channel></rss>