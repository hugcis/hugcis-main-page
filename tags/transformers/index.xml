<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformers on Hugo Cisneros</title>
    <link>https://hugocisneros.com/tags/transformers/</link>
    <description>Recent content in Transformers on Hugo Cisneros</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Sep 2021 12:52:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on: Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention by Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., &amp; Singh, V. (2021)</title>
      <link>https://hugocisneros.com/notes/xiongnystromformernystr2021/</link>
      <pubDate>Thu, 02 Sep 2021 12:52:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/xiongnystromformernystr2021/</guid>
      <description>tags Transformers source (Xiong et al. 2021)  Summary This paper describes a way of applying the Nyström method for approximating matrix multiplication to transformers. More precisely, the approximation is used in the self-attention mechanism&amp;rsquo;s softmax calculation.
This approximation adresses one of the biggest downside of attention: its computational complexity. The authors claim that their method reduces it from \(O(n^2)\) to \(O(n)\).
The goal of the method is to efficiently approximate the matrix</description>
    </item>
    
    <item>
      <title>Notes on: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., &amp; Fleuret, F. (2020)</title>
      <link>https://hugocisneros.com/notes/katharopoulostransformersarernns2020/</link>
      <pubDate>Thu, 02 Sep 2021 12:07:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/katharopoulostransformersarernns2020/</guid>
      <description>tags Transformers, RNN source (Katharopoulos et al. 2020)  Summary Transformers have traditionally been described as different models from RNNs. This is because instead of processing the sequence one token at a time, Transformers use attention to process all elements simultaneously.
The paper introduces an interesting new formulation, replacing the softmax attention with a feature map-based dot product.
This new formulation yields better time and memory complexity as well as a model that is casual and autoregressive (similar to RNNs).</description>
    </item>
    
    <item>
      <title>Notes on: Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021)</title>
      <link>https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/</link>
      <pubDate>Wed, 25 Aug 2021 15:19:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/</guid>
      <description>source (Lu et al. 2021) tags Transformers  Summary Different types of neural network architecture encode different kinds of biases. For example, convolutional neural networks perform local, translation-invariant operations and recurrent neural networks operate on sequential data.
One can use these biases in randomly initialized networks as a basis for interesting computations. This is on of the motivation for reservoir computing with echo-state networks, which uses fixed random recurrent neural network and a simple trainable linear transformation to perform complex computations.</description>
    </item>
    
    <item>
      <title>Notes on: Information-Theoretic Probing with Minimum Description Length by Voita, E., &amp; Titov, I. (2020)</title>
      <link>https://hugocisneros.com/notes/voitainformationtheoreticprobingminimum2020/</link>
      <pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate>
      
      <guid>https://hugocisneros.com/notes/voitainformationtheoreticprobingminimum2020/</guid>
      <description> tags Evaluating NLP, Transformers, Minimum description length source (Voita and Titov 2020)  Summary Comments Bibliography Voita, Elenaand Ivan Titov. March 27, 2020. &#34;Information-theoretic Probing with Minimum Description Length&#34;. Arxiv:2003.12298 [cs]. http://arxiv.org/abs/2003.12298.  </description>
    </item>
    
  </channel>
</rss>
