<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformers on Hugo Cisneros</title><link>https://hugocisneros.com/tags/transformers/</link><description>Recent content in Transformers on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 22 Feb 2023 13:28:00 +0100</lastBuildDate><atom:link href="https://hugocisneros.com/tags/transformers/index.xml" rel="self" type="application/rss+xml"/><item><title>Gopher</title><link>https://hugocisneros.com/notes/gopher/</link><pubDate>Wed, 22 Feb 2023 13:28:00 +0100</pubDate><guid>https://hugocisneros.com/notes/gopher/</guid><description> tags Transformers, GPT paper (Rae et al. 2022) Architecture This model is very similar to GPT-2 but uses RSNorm instead of LayerNorm and relative positional encoding rather than absolute positional encoding.
Parameter count 280B
Bibliography Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, et al.. January 21, 2022. "Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher". arXiv. DOI.</description></item><item><title>Sparrow</title><link>https://hugocisneros.com/notes/sparrow/</link><pubDate>Wed, 22 Feb 2023 13:28:00 +0100</pubDate><guid>https://hugocisneros.com/notes/sparrow/</guid><description>tags Transformers, GPT, Chinchilla paper (Glaese et al. 2022) blog post Deepmind announcement blog post Architecture Starts from the Chinchilla 70B model but adds RLHF (Reinforcement Learning with Human Feedback). It also adds inline evidence like GopherCite.
Parameter count 70B
Bibliography Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, et al.. September 28, 2022. "Improving Alignment of Dialogue Agents via Targeted Human Judgements". arXiv.</description></item><item><title>ChatGPT</title><link>https://hugocisneros.com/notes/chatgpt/</link><pubDate>Mon, 13 Feb 2023 13:26:00 +0100</pubDate><guid>https://hugocisneros.com/notes/chatgpt/</guid><description>tags GPT, Transformers, NLP blog post OpenAI blog post Architecture ChatGPT takes a GPT3.5 (aka GPT3 Davinci-003) pretrained model and uses RLHF to fine-tune the model similarly to InstructGPT but with some differences in the data collection. It is also more than &amp;ldquo;just&amp;rdquo; a model since it includes extensions for Memory Store and retrieval similar to BlenderBot 3.
Parameter count 175B</description></item><item><title>BlenderBot 3</title><link>https://hugocisneros.com/notes/blenderbot_3/</link><pubDate>Mon, 13 Feb 2023 13:18:00 +0100</pubDate><guid>https://hugocisneros.com/notes/blenderbot_3/</guid><description>tags Transformers, GPT, OPT: Open Pre-trained Transformer, NLP blog post Meta AI announcement blog post paper (Shuster et al. 2022) Architecture It is based on a pre-trained OPT model, with some optimizations to make it better as a dialog agent, such as long term memory and the ability to search the web.
It uses human feedback to fine-tune its results on some tasks.
Parameter count 175B
Bibliography Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, et al.</description></item><item><title>GPT</title><link>https://hugocisneros.com/notes/gpt/</link><pubDate>Wed, 27 Jul 2022 12:19:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt/</guid><description> tags Transformers, NLP paper (Radford et al. 2018) Succesors The GPT architecture was improved upon and extended into GPT-2 and GPT-3. The original &amp;ldquo;GPT-1&amp;rdquo; was quickly abandoned in favor of its successor, but GPT is still used to refer to this family of models.
Parameter count 117M
Bibliography Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever. 2018. "Improving Language Understanding by Generative Pre-training". OpenAI.</description></item><item><title>Gato</title><link>https://hugocisneros.com/notes/gato/</link><pubDate>Wed, 27 Jul 2022 12:12:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gato/</guid><description> tags Transformers, Reinforcement learning paper (Reed et al. 2022) Architecture A standard decoder-only transformer is preceded by an embedding layer that embeds text and images with positional encoding and spatial information if available.
Parameter count 1.2B
Bibliography Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al.. May 12, 2022. "A Generalist Agent". https://arxiv.org/abs/2205.06175v2.</description></item><item><title>XLNet</title><link>https://hugocisneros.com/notes/xlnet/</link><pubDate>Wed, 27 Jul 2022 12:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/xlnet/</guid><description> tags Transformers, Transformer-XL, NLP paper (Yang et al. 2020) Architecture The model adapts Transformer-XL to be a permutation based language model.
Parameter count Base = 117M Large = 360M Bibliography Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le. January 2, 2020. "Xlnet: Generalized Autoregressive Pretraining for Language Understanding". arXiv. DOI.</description></item><item><title>XLM-RoBERTa</title><link>https://hugocisneros.com/notes/xlm_roberta/</link><pubDate>Wed, 27 Jul 2022 12:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/xlm_roberta/</guid><description> tags Transformers, RoBERTa, NLP paper (Conneau et al. 2020) Architecture The model is an extension of RoBERTa that introduces small parameter tuning insights in the context of multilingual applications.
Parameter count Base = 270M Large = 550M Bibliography Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. April 7, 2020. "Unsupervised Cross-lingual Representation Learning at Scale". arXiv. DOI.</description></item><item><title>Wu Dao 2.0</title><link>https://hugocisneros.com/notes/wu_dao_2_0/</link><pubDate>Wed, 27 Jul 2022 11:52:00 +0200</pubDate><guid>https://hugocisneros.com/notes/wu_dao_2_0/</guid><description>tags Transformers, NLP website Wikipedia page for Wu Dao Architecture It is similar to GPT, being a decoder architecture but it applies a different pre-training task.
Parameter count 1.75T</description></item><item><title>Turing-NLG</title><link>https://hugocisneros.com/notes/turing_nlg/</link><pubDate>Wed, 27 Jul 2022 11:48:00 +0200</pubDate><guid>https://hugocisneros.com/notes/turing_nlg/</guid><description>tags Transformers, GPT, NLP website Microsoft Project Turing Architecture The architecture is similar to GPT-2 and GPT-3 with some parameter optimization and software/hardware platform to improve training.
Parameter count 17B originally, now up to 530B.</description></item><item><title>Vision transformer</title><link>https://hugocisneros.com/notes/vision_transformer/</link><pubDate>Wed, 27 Jul 2022 11:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/vision_transformer/</guid><description> tags Transformers, Computer vision, BERT paper (Dosovitskiy et al. 2021) Architecture It is an extension of the BERT architecture that can be trained on patches of images.
Parameter count 86M to 632M
Bibliography Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.. June 3, 2021. "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale". arXiv. DOI.</description></item><item><title>Transformer-XL</title><link>https://hugocisneros.com/notes/transformer_xl/</link><pubDate>Wed, 27 Jul 2022 11:42:00 +0200</pubDate><guid>https://hugocisneros.com/notes/transformer_xl/</guid><description> tags Transformers, NLP paper (Dai et al. 2019) Architecture This model uses relative positional embedding to enable using attention over longer contexts than the vanilla Transformer.
Parameter count 151M
Bibliography Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov. June 2, 2019. "Transformer-xl: Attentive Language Models Beyond a Fixed-length Context". arXiv. DOI.</description></item><item><title>Trajectory transformer</title><link>https://hugocisneros.com/notes/trajectory_transformer/</link><pubDate>Wed, 27 Jul 2022 11:41:00 +0200</pubDate><guid>https://hugocisneros.com/notes/trajectory_transformer/</guid><description> tags Transformers, Reinforcement learning, GPT paper (Janner et al. 2021) Architecture It is a similar model to Decision transformer, with some added techniques to encode a trajectory.
Bibliography Michael Janner, Qiyang Li, Sergey Levine. November 28, 2021. "Offline Reinforcement Learning as One Big Sequence Modeling Problem". arXiv. DOI.</description></item><item><title>T5</title><link>https://hugocisneros.com/notes/t5/</link><pubDate>Wed, 27 Jul 2022 11:28:00 +0200</pubDate><guid>https://hugocisneros.com/notes/t5/</guid><description> tags Transformers, NLP paper (Raffel et al. 2020) Architecture It is the same as the original transformer with some relative positional embedding added (similar to Transformer-XL).
Parameter count 11B
Bibliography Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. July 28, 2020. "Exploring the Limits of Transfer Learning with a Unified Text-to-text Transformer". arXiv. DOI.</description></item><item><title>Switch transformer</title><link>https://hugocisneros.com/notes/switch_transformer/</link><pubDate>Wed, 27 Jul 2022 11:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/switch_transformer/</guid><description> tags Transformers, T5, NLP paper (Fedus et al. 2022) Architecture This model increases the parameter count of T5-like architecture while allowing efficient routing through different experts in a mixture of experts.
Parameter count 1T
Bibliography William Fedus, Barret Zoph, Noam Shazeer. June 16, 2022. "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity". arXiv. DOI.</description></item><item><title>Swin Transformer</title><link>https://hugocisneros.com/notes/swin_transformer/</link><pubDate>Wed, 27 Jul 2022 11:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/swin_transformer/</guid><description> tags Transformers, ViT, Computer vision paper (Liu et al. 2021) Architecture This model extends ViT by replace the multi-head self-attention with a &amp;ldquo;shifted windows&amp;rdquo; module allowing ViT to work with higher resolution images.
Parameter count 29M - 197M
Bibliography Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. August 17, 2021. "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows". arXiv. DOI.</description></item><item><title>SeeKer</title><link>https://hugocisneros.com/notes/seeker/</link><pubDate>Wed, 27 Jul 2022 11:01:00 +0200</pubDate><guid>https://hugocisneros.com/notes/seeker/</guid><description>tags Transformers, GPT paper (Shuster et al. 2022) Architecture This is an extension that can be applied to any Transformer model by introducing “search”, “knowledge”, and “response” modules during pre-training of the model. It has the same applications as the base model it extends.
Parameter count Depends on the base model being extended.
Bibliography Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston. March 29, 2022. "</description></item><item><title>RoBERTa</title><link>https://hugocisneros.com/notes/roberta/</link><pubDate>Wed, 27 Jul 2022 10:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/roberta/</guid><description> tags Transformers, BERT, NLP paper (Liu et al. 2019) Architecture This is an extension of BERT with more data and a better optimized training procedure.
Parameter count 356M
Bibliography Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. July 26, 2019. "Roberta: A Robustly Optimized BERT Pretraining Approach". arXiv. http://arxiv.org/abs/1907.11692.</description></item><item><title>Pegasus</title><link>https://hugocisneros.com/notes/pegasus/</link><pubDate>Wed, 27 Jul 2022 10:45:00 +0200</pubDate><guid>https://hugocisneros.com/notes/pegasus/</guid><description> tags Transformers, NLP paper (Zhang et al. 2020) Architecture This is a standard encoder/decoder architecture with a special pre-training task suited for summarization of text.
Parameter count Base = 223M Large = 568M Bibliography Jingqing Zhang, Yao Zhao, Mohammad Saleh, Peter J. Liu. July 10, 2020. "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization". arXiv. http://arxiv.org/abs/1912.08777.</description></item><item><title>PaLM</title><link>https://hugocisneros.com/notes/palm/</link><pubDate>Wed, 27 Jul 2022 10:43:00 +0200</pubDate><guid>https://hugocisneros.com/notes/palm/</guid><description>tags Transformers, NLP paper (Chowdhery et al. 2022) Architecture This is a standard decoder-only architecture with some specific extensions:
SwiGLU activation functions Parallel layers Multi-query attention RoPE embeddings Shared input-output embeddings No biaises A 256k SentencePiece vocabulary generated from the training data Parameter count 540B
Bibliography Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al.. April 19, 2022. "Palm: Scaling Language Modeling with Pathways"</description></item><item><title>OPT: Open Pre-trained Transformer</title><link>https://hugocisneros.com/notes/opt/</link><pubDate>Wed, 27 Jul 2022 10:40:00 +0200</pubDate><guid>https://hugocisneros.com/notes/opt/</guid><description> tags Transformers, GPT, NLP paper (Zhang et al. 2022) Architecture It is the same architecture as GPT-3 but with some training improvements from Megatron.
Parameter count 175B
Bibliography Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, et al.. June 21, 2022. "OPT: Open Pre-trained Transformer Language Models". arXiv. http://arxiv.org/abs/2205.01068.</description></item><item><title>Minerva</title><link>https://hugocisneros.com/notes/minerva/</link><pubDate>Tue, 26 Jul 2022 15:21:00 +0200</pubDate><guid>https://hugocisneros.com/notes/minerva/</guid><description> tags Transformers, Mathematics, PaLM paper (Lewkowycz et al. 2022) Architecture This model is PaLM fine-tuned on mathematical datasets.
Parameter count 540B
Bibliography Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, et al.. June 30, 2022. "Solving Quantitative Reasoning Problems with Language Models". arXiv. http://arxiv.org/abs/2206.14858.</description></item><item><title>Megatron</title><link>https://hugocisneros.com/notes/megatron/</link><pubDate>Tue, 26 Jul 2022 15:18:00 +0200</pubDate><guid>https://hugocisneros.com/notes/megatron/</guid><description> tags Transformers, GPT, BERT, T5 paper (Shoeybi et al. 2020) Architecture The principle of Megatron is to extend existing architectures by using model parallelism. It has a number of parameters that depends on the base model used.
Bibliography Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro. March 13, 2020. "Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism". arXiv. DOI.</description></item><item><title>mBART</title><link>https://hugocisneros.com/notes/mbart/</link><pubDate>Tue, 26 Jul 2022 15:15:00 +0200</pubDate><guid>https://hugocisneros.com/notes/mbart/</guid><description> tags Transformers, NLP, BART paper (Liu et al. 2020) Architecture It&amp;rsquo;s an encoder-decoder architecture based on BART
Bibliography Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer. January 23, 2020. "Multilingual Denoising Pre-training for Neural Machine Translation". arXiv. DOI.</description></item><item><title>LAMDA</title><link>https://hugocisneros.com/notes/lamda/</link><pubDate>Tue, 26 Jul 2022 11:53:00 +0200</pubDate><guid>https://hugocisneros.com/notes/lamda/</guid><description> tags Transformers, NLP paper (Thoppilan et al. 2022) Parameter count 137B
Bibliography Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, et al.. February 10, 2022. "Lamda: Language Models for Dialog Applications". arXiv. http://arxiv.org/abs/2201.08239.</description></item><item><title>Jurassic-1</title><link>https://hugocisneros.com/notes/jurassic_1/</link><pubDate>Tue, 26 Jul 2022 11:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/jurassic_1/</guid><description> tags Transformers, GPT, NLP blog post AI21Labs blog Architecture This model is similar to GPT-3 with an improved tokenizer that increases the learning efficiency. It also has more parameters.
Parameter count 178B
Bibliography</description></item><item><title>Imagen</title><link>https://hugocisneros.com/notes/imagen/</link><pubDate>Tue, 26 Jul 2022 11:41:00 +0200</pubDate><guid>https://hugocisneros.com/notes/imagen/</guid><description>tags Transformers, Diffusion models, Computer vision, NLP, T5, CLIP paper (Saharia et al. 2022) Architecture This is based on the U-net diffusion architecture with a few extensions. T5 or CLIP or BERT is used as a frozen text encoder.
Parameter count 2B
Bibliography Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, et al.. May 23, 2022. "Photorealistic Text-to-image Diffusion Models with Deep Language Understanding"</description></item><item><title>GPTInstruct</title><link>https://hugocisneros.com/notes/gptinstruct/</link><pubDate>Tue, 26 Jul 2022 11:10:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gptinstruct/</guid><description> tags Transformers, GPT, NLP paper (Ouyang et al. 2022) Architecture This model starts off from a pretrained GPT-3. Reward modeling is added with Reinforcement learning.
Parameter count 175B
Bibliography Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al.. March 4, 2022. "Training Language Models to Follow Instructions with Human Feedback". arXiv. DOI.</description></item><item><title>GPT-Neo</title><link>https://hugocisneros.com/notes/gpt_neo/</link><pubDate>Tue, 26 Jul 2022 11:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_neo/</guid><description> tags Transformers, GPT, NLP software &amp;lt;&amp;amp;gpt-neo&amp;gt; Architecture This model is very similar to GPT-2, with the addition of local attention every other layer and a window size of 256 tokens.
Parameter count 1.5B, 2.7B (XL)
Bibliography</description></item><item><title>Global context ViT</title><link>https://hugocisneros.com/notes/global_context_vit/</link><pubDate>Tue, 26 Jul 2022 11:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/global_context_vit/</guid><description> tags Transformers, Computer vision, ViT paper (Hatamizadeh et al. 2022) Architecture This is a hierarchical version of ViT with both local and global attention.
Parameter count 90M
Bibliography Ali Hatamizadeh, Hongxu Yin, Jan Kautz, Pavlo Molchanov. June 20, 2022. "Global Context Vision Transformers". arXiv. DOI.</description></item><item><title>GPT-3</title><link>https://hugocisneros.com/notes/gpt_3/</link><pubDate>Tue, 26 Jul 2022 10:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_3/</guid><description> tags Transformers, NLP, GPT paper (Brown et al. 2020) Architecture Like GPT-2, with the addition of locally banded sparse attention.
Parameter count 175B
Bibliography Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al.. June 4, 2020. "Language Models Are Few-shot Learners". Arxiv:2005.14165 [cs]. http://arxiv.org/abs/2005.14165.</description></item><item><title>GPT-2</title><link>https://hugocisneros.com/notes/gpt_2/</link><pubDate>Tue, 26 Jul 2022 10:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_2/</guid><description> tags Transformers, GPT paper (Radford et al. 2019) Architecture Some minor changes from GPT, like a larger context and some order change of normalization.
Parameter count 1.5B
Bibliography Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. 2019. "Language Models Are Unsupervised Multitask Learners". Openai Blog 1 (8):9.</description></item><item><title>GLaM</title><link>https://hugocisneros.com/notes/glam/</link><pubDate>Tue, 26 Jul 2022 10:01:00 +0200</pubDate><guid>https://hugocisneros.com/notes/glam/</guid><description>tags Transformers, NLP paper (Du et al. 2021) Architecture The model is a mixture of 64 expert decoder-only transformer architectures. Two experts are activated per token, making the model relatively efficient for its number of parameters
Parameter count 1.2T total, 96B active per token.
Bibliography Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, et al.. December 13, 2021. "Glam: Efficient Scaling of Language Models with Mixture-of-experts"</description></item><item><title>Flamingo</title><link>https://hugocisneros.com/notes/flamingo/</link><pubDate>Tue, 26 Jul 2022 09:56:00 +0200</pubDate><guid>https://hugocisneros.com/notes/flamingo/</guid><description> tags Transformers, Computer vision, NLP, Chinchilla paper (Alayrac et al. 2022) Architecture Uses a frozen language model (e.g. Chinchilla) that is conditioned on a visual representation given from a normalizer-free ResNet.
Parameter count 80B
Bibliography Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al.. April 29, 2022. "Flamingo: A Visual Language Model for Few-shot Learning". arXiv. http://arxiv.org/abs/2204.14198.</description></item><item><title>ERNIE</title><link>https://hugocisneros.com/notes/ernie/</link><pubDate>Tue, 26 Jul 2022 09:51:00 +0200</pubDate><guid>https://hugocisneros.com/notes/ernie/</guid><description> tags Transformers, BERT, NLP paper (Zhang et al. 2019) Architecture This transformer uses two stacked BERT for encoding: one for the text, one for the entities in a knowledge graph.
Parameter count 114M
Bibliography Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu. June 4, 2019. "ERNIE: Enhanced Language Representation with Informative Entities". arXiv. DOI.</description></item><item><title>ELECTRA</title><link>https://hugocisneros.com/notes/electra/</link><pubDate>Tue, 26 Jul 2022 09:08:00 +0200</pubDate><guid>https://hugocisneros.com/notes/electra/</guid><description> tags Transformers, NLP paper (Clark et al. 2020) Paramter count Base = 110M Large = 330M Bibliography Kevin Clark, Minh-Thang Luong, Quoc V. Le, Christopher D. Manning. 2020. "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators". In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. https://openreview.net/forum?id=r1xMH1BtvB.</description></item><item><title>DQ-BART</title><link>https://hugocisneros.com/notes/dq_bart/</link><pubDate>Tue, 26 Jul 2022 09:05:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dq_bart/</guid><description> tags Transformers, BART, NLP paper (Li et al. 2022) Architecture It is a distilled and quantized version of BART. It improves performance as well as the model size.
Bibliography Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing Xiang, Dan Roth. March 21, 2022. "DQ-BART: Efficient Sequence-to-sequence Model via Joint Distillation and Quantization". arXiv. DOI.</description></item><item><title>DistillBERT</title><link>https://hugocisneros.com/notes/distillbert/</link><pubDate>Tue, 26 Jul 2022 08:43:00 +0200</pubDate><guid>https://hugocisneros.com/notes/distillbert/</guid><description> tags Transformers, BERT, NLP paper (Sanh et al. 2020) Architecture It is a distilled version of BERT that is much more efficient.
Parameter count 66M
Bibliography Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. February 29, 2020. "Distilbert, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter". arXiv. DOI.</description></item><item><title>DialoGPT</title><link>https://hugocisneros.com/notes/dialogpt/</link><pubDate>Fri, 22 Jul 2022 13:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dialogpt/</guid><description> tags GPT, Transformers, NLP paper (Zhang et al. 2020) Architecture It is exactly like a GPT-2 architecture but trained on dialog data.
Parameter count 1.5B
Bibliography Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan. May 2, 2020. "Dialogpt: Large-scale Generative Pre-training for Conversational Response Generation". arXiv. DOI.</description></item><item><title>Decision transformer</title><link>https://hugocisneros.com/notes/decision_transformer/</link><pubDate>Fri, 22 Jul 2022 13:03:00 +0200</pubDate><guid>https://hugocisneros.com/notes/decision_transformer/</guid><description> tags Transformers, GPT, Reinforcement learning paper (Chen et al. 2021) Architecture This is a decoder model that uses a GPT-like model to encode and predict trajectories for Reinforcement learning tasks. It has essentially the same characteristics as GPT.
Bibliography Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch. June 24, 2021. "Decision Transformer: Reinforcement Learning via Sequence Modeling". arXiv. DOI.</description></item><item><title>ALBERT</title><link>https://hugocisneros.com/notes/albert/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/albert/</guid><description>tags Transformers, BERT, NLP paper (Lan et al. 2020) Architecture It is an encoder-only architecture. It extends BERT by using parameter-sharing and is more efficient than BERT with the same number of parameters.
Parameter count Base = 12M Large = 18M XLarge = 60M Bibliography Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. February 8, 2020. "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"</description></item><item><title>BERT</title><link>https://hugocisneros.com/notes/bert/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/bert/</guid><description> tags Transformers, NLP paper (Devlin et al. 2019) Parameter count Base = 110M Large = 340M Bibliography Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. May 24, 2019. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding". arXiv. DOI.</description></item><item><title>BLOOM</title><link>https://hugocisneros.com/notes/bloom/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/bloom/</guid><description> tags Transformers, GPT, NLP blog post BLOOM announcement blog post Architecture It is similar to the architecture of GPT-3, using full attention instead of sparse attention.
Parameter count 176B
Bibliography</description></item><item><title>CTRL</title><link>https://hugocisneros.com/notes/ctrl/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/ctrl/</guid><description>tags Transformers, NLP paper (Keskar et al. 2019) Architecture This is a model that can generate text conditioned on control codes that specify the domain, style, topics, dates, entities, relationships between entities, plot points, and task-related behavior of the text.
Parameter count 1.63B
Bibliography Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, Richard Socher. September 20, 2019. "CTRL: A Conditional Transformer Language Model for Controllable Generation". arXiv. DOI.</description></item><item><title>Big bird</title><link>https://hugocisneros.com/notes/big_bird/</link><pubDate>Fri, 22 Jul 2022 13:01:00 +0200</pubDate><guid>https://hugocisneros.com/notes/big_bird/</guid><description>tags Transformers, NLP paper (Zaheer et al. 2021) Architecture Big bird can be used as both an encoder-only and an encoder/decoder architecture.
It extends the likes of BERT by implementing a sparse attention mechanism, making the attention computational complexity less than quadratic.
Bibliography Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, et al.. January 8, 2021. "Big Bird: Transformers for Longer Sequences". arXiv. DOI.</description></item><item><title>DALL-E-2</title><link>https://hugocisneros.com/notes/dall_e_2/</link><pubDate>Fri, 22 Jul 2022 13:00:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dall_e_2/</guid><description> tags Transformers, Diffusion models, CLIP paper (Ramesh et al. 2022) Architecture This is the successor of DALL-E, it is an encoder/decoder model that uses a combination of CLIP and Diffusion models to generate images from text. The diffusion decoder is similar to GLIDE.
Parameter count 3.5B
Bibliography Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. April 12, 2022. "Hierarchical Text-conditional Image Generation with CLIP Latents". arXiv. DOI.</description></item><item><title>DALL-E</title><link>https://hugocisneros.com/notes/dall_e/</link><pubDate>Fri, 22 Jul 2022 12:53:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dall_e/</guid><description> tags Transformers, GPT paper (Ramesh et al. 2021) Architecture It is a decoder architecture with a Variational autoencoders and a variant of GPT-3 to convert text to images.
Parameter count 12B
Bibliography Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. February 26, 2021. "Zero-shot Text-to-image Generation". arXiv. DOI.</description></item><item><title>CLIP</title><link>https://hugocisneros.com/notes/clip/</link><pubDate>Fri, 22 Jul 2022 12:29:00 +0200</pubDate><guid>https://hugocisneros.com/notes/clip/</guid><description> tags Transformers, NLP, Computer vision paper (Radford et al. 2021) Architecture It is an encoder-only model which combines ViT and ResNet to encode images and a transformer for the text encoding.
Bibliography Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al.. February 26, 2021. "Learning Transferable Visual Models from Natural Language Supervision". arXiv. DOI.</description></item><item><title>Chinchilla</title><link>https://hugocisneros.com/notes/chinchilla/</link><pubDate>Fri, 22 Jul 2022 12:27:00 +0200</pubDate><guid>https://hugocisneros.com/notes/chinchilla/</guid><description> tags Transformers, GPT, NLP paper (Hoffmann et al. 2022) Architecture This model is very similar to Gopher, with some improvements to make the model smaller and more efficient.
Parameter count 70B
Bibliography Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al.. March 29, 2022. "Training Compute-optimal Large Language Models". arXiv. DOI.</description></item><item><title>Positional encoding</title><link>https://hugocisneros.com/notes/positional_encoding/</link><pubDate>Fri, 22 Jul 2022 11:57:00 +0200</pubDate><guid>https://hugocisneros.com/notes/positional_encoding/</guid><description> tags Transformers, Attention</description></item><item><title>BART</title><link>https://hugocisneros.com/notes/bart/</link><pubDate>Fri, 22 Jul 2022 10:11:00 +0200</pubDate><guid>https://hugocisneros.com/notes/bart/</guid><description> tags Transformers paper (Lewis et al. 2019) Architecture It is an encoder/decoder architecture. The encoder is based on BERT and the decoder is based on GPT. It generalizes the two models into a single one.
Bibliography Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. October 29, 2019. "BART: Denoising Sequence-to-sequence Pre-training for Natural Language Generation, Translation, and Comprehension". arXiv. DOI.</description></item><item><title>Notes on: Memorizing Transformers by Wu, Y., Rabe, M. N., Hutchins, D., &amp; Szegedy, C. (2022)</title><link>https://hugocisneros.com/notes/wumemorizingtransformers2022/</link><pubDate>Wed, 25 May 2022 13:26:00 +0200</pubDate><guid>https://hugocisneros.com/notes/wumemorizingtransformers2022/</guid><description>source (Wu et al. 2022) tags Transformers, Memory in neural networks Summary This paper introduces a method to extend the classical Transformer neural network model with an addressable memory that can be queried and updated at inference time.
This memory is addressed using an attention mechanism. It is a set of cached attention (key, value) vector pairs. At some arbitrary depth of the attention &amp;ldquo;stack&amp;rdquo; the memory mechanism is inserted.</description></item><item><title>Notes on: Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention by Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., &amp; Singh, V. (2021)</title><link>https://hugocisneros.com/notes/xiongnystromformernystr2021/</link><pubDate>Thu, 02 Sep 2021 12:52:00 +0200</pubDate><guid>https://hugocisneros.com/notes/xiongnystromformernystr2021/</guid><description>tags Transformers source (Xiong et al. 2021) Summary This paper describes a way of applying the Nyström method for approximating matrix multiplication to transformers. More precisely, the approximation is used in the self-attention mechanism&amp;rsquo;s softmax calculation.
This approximation adresses one of the biggest downside of attention: its computational complexity. The authors claim that their method reduces it from \(O(n^2)\) to \(O(n)\).
The goal of the method is to efficiently approximate the matrix</description></item><item><title>Notes on: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention by Katharopoulos, A., Vyas, A., Pappas, N., &amp; Fleuret, F. (2020)</title><link>https://hugocisneros.com/notes/katharopoulostransformersarernns2020/</link><pubDate>Thu, 02 Sep 2021 12:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/katharopoulostransformersarernns2020/</guid><description>tags Transformers, RNN source (Katharopoulos et al. 2020) Summary Transformers have traditionally been described as different models from RNNs. This is because instead of processing the sequence one token at a time, Transformers use attention to process all elements simultaneously.
The paper introduces an interesting new formulation, replacing the softmax attention with a feature map-based dot product.
This new formulation yields better time and memory complexity as well as a model that is casual and autoregressive (similar to RNNs).</description></item><item><title>Notes on: Pretrained Transformers as Universal Computation Engines by Lu, K., Grover, A., Abbeel, P., &amp; Mordatch, I. (2021)</title><link>https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/</link><pubDate>Wed, 25 Aug 2021 15:19:00 +0200</pubDate><guid>https://hugocisneros.com/notes/lupretrainedtransformersuniversal2021/</guid><description>source (Lu et al. 2021) tags Transformers Summary Different types of neural network architecture encode different kinds of biases. For example, convolutional neural networks perform local, translation-invariant operations and recurrent neural networks operate on sequential data.
One can use these biases in randomly initialized networks as a basis for interesting computations. This is on of the motivation for reservoir computing with echo-state networks, which uses fixed random recurrent neural network and a simple trainable linear transformation to perform complex computations.</description></item><item><title>Notes on: Information-Theoretic Probing with Minimum Description Length by Voita, E., &amp; Titov, I. (2020)</title><link>https://hugocisneros.com/notes/voitainformationtheoreticprobingminimum2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/voitainformationtheoreticprobingminimum2020/</guid><description> tags Evaluating NLP, Transformers, Minimum description length source (Voita, Titov 2020) Summary Comments Bibliography Elena Voita, Ivan Titov. March 27, 2020. "Information-theoretic Probing with Minimum Description Length". Arxiv:2003.12298 [cs]. http://arxiv.org/abs/2003.12298.</description></item></channel></rss>