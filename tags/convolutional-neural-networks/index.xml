<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Convolutional neural networks on Hugo Cisneros</title><link>https://hugocisneros.com/tags/convolutional-neural-networks/</link><description>Recent content in Convolutional neural networks on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 22 Jul 2022 12:28:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/convolutional-neural-networks/index.xml" rel="self" type="application/rss+xml"/><item><title>Residual neural networks</title><link>https://hugocisneros.com/notes/residual_networks/</link><pubDate>Fri, 22 Jul 2022 12:28:00 +0200</pubDate><guid>https://hugocisneros.com/notes/residual_networks/</guid><description>tags Neural networks, Convolutional neural networks, Computer vision resources (He et al. 2016) Residual neural networks are neural networks with skip-connections (or shortcuts, residual connections) that will bypass some of the networks operations in depth.
Highway networks (Srivastava et al. 2015)
DenseNets (&amp;lt;cite itemprop=&amp;ldquo;citation&amp;rdquo; itemscope=&amp;ldquo;&amp;ldquo;Huang, Liu ,n.d.)
Bibliography Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. June 2016. "Deep Residual Learning for Image Recognition". In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78.</description></item><item><title>Notes on: Network Deconvolution by Ye, C., Evanusa, M., He, H., Mitrokhin, A., Goldstein, T., Yorke, J. A., Fermuller, Cornelia, … (2020)</title><link>https://hugocisneros.com/notes/yenetworkdeconvolution2020/</link><pubDate>Thu, 25 Mar 2021 09:58:00 +0100</pubDate><guid>https://hugocisneros.com/notes/yenetworkdeconvolution2020/</guid><description>tags Convolutional neural networks, Neural network training source (Ye et al. 2020) Summary This paper introduces so-called Network Deconvolution, advertised as a way to remove pixel-wise and channel-wise correlation in deep neural networks.
The authors base their new operator on the optimal configuration for \(L_2\) linear regression, where gradient descent converges in one single step if and only if:
\[ \frac{1}{N}X^t X = I \] where \(X\) is the feature matrix and \(N\) the number of samples.</description></item><item><title>Graph convolutional networks</title><link>https://hugocisneros.com/notes/graph_convolutional_networks/</link><pubDate>Tue, 01 Dec 2020 15:34:00 +0100</pubDate><guid>https://hugocisneros.com/notes/graph_convolutional_networks/</guid><description> tags Convolutional neural networks, Graph neural networks</description></item></channel></rss>