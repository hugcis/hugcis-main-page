<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>T5 on Hugo Cisneros</title><link>https://hugocisneros.com/tags/t5/</link><description>Recent content in T5 on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 11:06:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/t5/index.xml" rel="self" type="application/rss+xml"/><item><title>Switch transformer</title><link>https://hugocisneros.com/notes/switch_transformer/</link><pubDate>Wed, 27 Jul 2022 11:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/switch_transformer/</guid><description> tags Transformers, T5, NLP paper (Fedus et al. 2022) Architecture This model increases the parameter count of T5-like architecture while allowing efficient routing through different experts in a mixture of experts.
Parameter count 1T
Bibliography William Fedus, Barret Zoph, Noam Shazeer. June 16, 2022. "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity". arXiv. DOI.</description></item><item><title>Megatron</title><link>https://hugocisneros.com/notes/megatron/</link><pubDate>Tue, 26 Jul 2022 15:18:00 +0200</pubDate><guid>https://hugocisneros.com/notes/megatron/</guid><description> tags Transformers, GPT, BERT, T5 paper (Shoeybi et al. 2020) Architecture The principle of Megatron is to extend existing architectures by using model parallelism. It has a number of parameters that depends on the base model used.
Bibliography Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro. March 13, 2020. "Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism". arXiv. DOI.</description></item><item><title>Imagen</title><link>https://hugocisneros.com/notes/imagen/</link><pubDate>Tue, 26 Jul 2022 11:41:00 +0200</pubDate><guid>https://hugocisneros.com/notes/imagen/</guid><description>tags Transformers, Diffusion models, Computer vision, NLP, T5, CLIP paper (Saharia et al. 2022) Architecture This is based on the U-net diffusion architecture with a few extensions. T5 or CLIP or BERT is used as a frozen text encoder.
Parameter count 2B
Bibliography Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, et al.. May 23, 2022. "Photorealistic Text-to-image Diffusion Models with Deep Language Understanding"</description></item></channel></rss>