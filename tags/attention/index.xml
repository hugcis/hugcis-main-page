<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Attention on Hugo Cisneros</title><link>https://hugocisneros.com/tags/attention/</link><description>Recent content in Attention on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 05 May 2021 16:26:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/attention/index.xml" rel="self" type="application/rss+xml"/><item><title>Notes on: Hopfield Networks is All You Need by Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., Holzleitner, M., … (2020)</title><link>https://hugocisneros.com/notes/ramsauerhopfieldnetworksall2020/</link><pubDate>Wed, 05 May 2021 16:26:00 +0200</pubDate><guid>https://hugocisneros.com/notes/ramsauerhopfieldnetworksall2020/</guid><description> tags Hopfield Networks, Attention source (Ramsauer et al. 2020) resources Blog post Summary This quote summarizes the paper well: &amp;ldquo;In order to integrate modern Hopfield networks into deep learning architectures, we have to make them continuous&amp;rdquo;.
Comments Bibliography Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Holzleitner, et al.. July 16, 2020. "Hopfield Networks Is All You Need". Arxiv:2008.02217 [cs, Stat]. http://arxiv.org/abs/2008.02217.</description></item><item><title>Attention graph networks</title><link>https://hugocisneros.com/notes/attention_graph_networks/</link><pubDate>Wed, 24 Feb 2021 11:32:00 +0100</pubDate><guid>https://hugocisneros.com/notes/attention_graph_networks/</guid><description> tags Graph neural networks, Attention</description></item></channel></rss>