<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Recurrent neural networks on Hugo Cisneros</title>
    <link>https://hugocisneros.com/tags/recurrent-neural-networks/</link>
    <description>Recent content in Recurrent neural networks on Hugo Cisneros</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Aug 2021 09:25:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/recurrent-neural-networks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Notes on: Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks by Voelker, A., Kajić, I., &amp; Eliasmith, C. (2019)</title>
      <link>https://hugocisneros.com/notes/voelkerlegendrememoryunits2019/</link>
      <pubDate>Tue, 10 Aug 2021 09:25:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/voelkerlegendrememoryunits2019/</guid>
      <description>tags Recurrent neural networks source (Voelker, Kajićand Eliasmith 2019)  Summary This paper introduces the LMU recurrent cell. This cell is based on a similar-ish idea to LSTM to maintain a memory hidden state. The main idea of the paper is to make this memory satisfy a set of first order ordinary differential equations.
\[
\begin{equation} \theta \dot{m}(t) = Am(t) + Bu(t) \end{equation}
\]
This system has a solution which represents sliding windows of \(u\) via Legendre polynomials.</description>
    </item>
    
    <item>
      <title>Notes on: Modeling systems with internal state using evolino by Wierstra, D., Gomez, F. J., &amp; Schmidhuber, J. (2005)</title>
      <link>https://hugocisneros.com/notes/wierstramodelingsystemsinternal2005/</link>
      <pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate>
      
      <guid>https://hugocisneros.com/notes/wierstramodelingsystemsinternal2005/</guid>
      <description> tags Genetic algorithms, Recurrent neural networks source (Wierstra, Gomezand Schmidhuber 2005)  Summary Comments Bibliography Wierstra, Daan, Faustino J. Gomezand Jürgen Schmidhuber. 2005. &#34;Modeling Systems with Internal State Using Evolino&#34;. In Proceedings of the 2005 Conference on Genetic and Evolutionary Computation - GECCO &#39;05, 1795. Washington DC, USA: ACM Press. DOI.  </description>
    </item>
    
    <item>
      <title>Echo-state networks</title>
      <link>https://hugocisneros.com/notes/echo_state_networks/</link>
      <pubDate>Fri, 02 Oct 2020 16:17:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/echo_state_networks/</guid>
      <description>tags Recurrent neural networks, Unsupervised learning resources Scholarpedia  Principle An echo state network is usually a standard RNN with fixed random weights. The output from this RNN is used as a high dimensional feature map to be fed into a machine learning system.
(Jaeger 2004; Jaeger, Maassand Principe 2007; Jaeger 2012)
Bibliography Jaeger, H.. April 2, 2004. &#34;Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication&#34;. Science 304 (5667):78–80.</description>
    </item>
    
    <item>
      <title>Backward RNN</title>
      <link>https://hugocisneros.com/notes/backward_rnn/</link>
      <pubDate>Tue, 14 Jul 2020 08:33:00 +0200</pubDate>
      
      <guid>https://hugocisneros.com/notes/backward_rnn/</guid>
      <description>tags Recurrent neural networks  Regular RNNs process input in sequence. When applied to a language modeling task, one tries to predict a word given the previous ones. For example, with the sentence The quick brown fox jumps over the lazy, a classical RNN will initialize and internal state \(s_0\) and process each word in sequence, starting from The and updating its internal state with each new word in order to make a final prediction.</description>
    </item>
    
  </channel>
</rss>
