<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>BART on Hugo Cisneros</title><link>https://hugocisneros.com/tags/bart/</link><description>Recent content in BART on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 26 Jul 2022 15:15:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/bart/index.xml" rel="self" type="application/rss+xml"/><item><title>mBART</title><link>https://hugocisneros.com/notes/mbart/</link><pubDate>Tue, 26 Jul 2022 15:15:00 +0200</pubDate><guid>https://hugocisneros.com/notes/mbart/</guid><description> tags Transformers, NLP, BART paper (Liu et al. 2020) Architecture It&amp;rsquo;s an encoder-decoder architecture based on BART
Bibliography Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer. January 23, 2020. "Multilingual Denoising Pre-training for Neural Machine Translation". arXiv. DOI.</description></item><item><title>DQ-BART</title><link>https://hugocisneros.com/notes/dq_bart/</link><pubDate>Tue, 26 Jul 2022 09:05:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dq_bart/</guid><description> tags Transformers, BART, NLP paper (Li et al. 2022) Architecture It is a distilled and quantized version of BART. It improves performance as well as the model size.
Bibliography Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing Xiang, Dan Roth. March 21, 2022. "DQ-BART: Efficient Sequence-to-sequence Model via Joint Distillation and Quantization". arXiv. DOI.</description></item></channel></rss>