<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPT on Hugo Cisneros</title><link>https://hugocisneros.com/tags/gpt/</link><description>Recent content in GPT on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 11:48:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/gpt/index.xml" rel="self" type="application/rss+xml"/><item><title>Turing-NLG</title><link>https://hugocisneros.com/notes/turing_nlg/</link><pubDate>Wed, 27 Jul 2022 11:48:00 +0200</pubDate><guid>https://hugocisneros.com/notes/turing_nlg/</guid><description>tags Transformers, GPT, NLP website Microsoft Project Turing Architecture The architecture is similar to GPT-2 and GPT-3 with some parameter optimization and software/hardware platform to improve training.
Parameter count 17B originally, now up to 530B.</description></item><item><title>Trajectory transformer</title><link>https://hugocisneros.com/notes/trajectory_transformer/</link><pubDate>Wed, 27 Jul 2022 11:41:00 +0200</pubDate><guid>https://hugocisneros.com/notes/trajectory_transformer/</guid><description> tags Transformers, Reinforcement learning, GPT paper (Janner et al. 2021) Architecture It is a similar model to Decision transformer, with some added techniques to encode a trajectory.
Bibliography Michael Janner, Qiyang Li, Sergey Levine. November 28, 2021. "Offline Reinforcement Learning as One Big Sequence Modeling Problem". arXiv. DOI.</description></item><item><title>SeeKer</title><link>https://hugocisneros.com/notes/seeker/</link><pubDate>Wed, 27 Jul 2022 11:01:00 +0200</pubDate><guid>https://hugocisneros.com/notes/seeker/</guid><description>tags Transformers, GPT paper (Shuster et al. 2022) Architecture This is an extension that can be applied to any Transformer model by introducing “search”, “knowledge”, and “response” modules during pre-training of the model. It has the same applications as the base model it extends.
Parameter count Depends on the base model being extended.
Bibliography Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, Jason Weston. March 29, 2022. "</description></item><item><title>OPT: Open Pre-trained Transformer</title><link>https://hugocisneros.com/notes/opt/</link><pubDate>Wed, 27 Jul 2022 10:40:00 +0200</pubDate><guid>https://hugocisneros.com/notes/opt/</guid><description> tags Transformers, GPT, NLP paper (Zhang et al. 2022) Architecture It is the same architecture as GPT-3 but with some training improvements from Megatron.
Parameter count 175B
Bibliography Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, et al.. June 21, 2022. "OPT: Open Pre-trained Transformer Language Models". arXiv. http://arxiv.org/abs/2205.01068.</description></item><item><title>Megatron</title><link>https://hugocisneros.com/notes/megatron/</link><pubDate>Tue, 26 Jul 2022 15:18:00 +0200</pubDate><guid>https://hugocisneros.com/notes/megatron/</guid><description> tags Transformers, GPT, BERT, T5 paper (Shoeybi et al. 2020) Architecture The principle of Megatron is to extend existing architectures by using model parallelism. It has a number of parameters that depends on the base model used.
Bibliography Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro. March 13, 2020. "Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism". arXiv. DOI.</description></item><item><title>Jurassic-1</title><link>https://hugocisneros.com/notes/jurassic_1/</link><pubDate>Tue, 26 Jul 2022 11:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/jurassic_1/</guid><description> tags Transformers, GPT, NLP blog post AI21Labs blog Architecture This model is similar to GPT-3 with an improved tokenizer that increases the learning efficiency. It also has more parameters.
Parameter count 178B
Bibliography</description></item><item><title>GPTInstruct</title><link>https://hugocisneros.com/notes/gptinstruct/</link><pubDate>Tue, 26 Jul 2022 11:10:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gptinstruct/</guid><description> tags Transformers, GPT, NLP paper (Ouyang et al. 2022) Architecture This model starts off from a pretrained GPT-3. Reward modeling is added with Reinforcement learning.
Parameter count 175B
Bibliography Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al.. March 4, 2022. "Training Language Models to Follow Instructions with Human Feedback". arXiv. DOI.</description></item><item><title>GPT-Neo</title><link>https://hugocisneros.com/notes/gpt_neo/</link><pubDate>Tue, 26 Jul 2022 11:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_neo/</guid><description> tags Transformers, GPT, NLP software &amp;lt;&amp;amp;gpt-neo&amp;gt; Architecture This model is very similar to GPT-2, with the addition of local attention every other layer and a window size of 256 tokens.
Parameter count 1.5B, 2.7B (XL)
Bibliography</description></item><item><title>GPT-3</title><link>https://hugocisneros.com/notes/gpt_3/</link><pubDate>Tue, 26 Jul 2022 10:06:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_3/</guid><description> tags Transformers, NLP, GPT paper (Brown et al. 2020) Architecture Like GPT-2, with the addition of locally banded sparse attention.
Parameter count 175B
Bibliography Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al.. June 4, 2020. "Language Models Are Few-shot Learners". Arxiv:2005.14165 [cs]. http://arxiv.org/abs/2005.14165.</description></item><item><title>GPT-2</title><link>https://hugocisneros.com/notes/gpt_2/</link><pubDate>Tue, 26 Jul 2022 10:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gpt_2/</guid><description> tags Transformers, GPT paper (Radford et al. 2019) Architecture Some minor changes from GPT, like a larger context and some order change of normalization.
Parameter count 1.5B
Bibliography Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever. 2019. "Language Models Are Unsupervised Multitask Learners". Openai Blog 1 (8):9.</description></item><item><title>DialoGPT</title><link>https://hugocisneros.com/notes/dialogpt/</link><pubDate>Fri, 22 Jul 2022 13:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dialogpt/</guid><description> tags GPT, Transformers, NLP paper (Zhang et al. 2020) Architecture It is exactly like a GPT-2 architecture but trained on dialog data.
Parameter count 1.5B
Bibliography Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan. May 2, 2020. "Dialogpt: Large-scale Generative Pre-training for Conversational Response Generation". arXiv. DOI.</description></item><item><title>Decision transformer</title><link>https://hugocisneros.com/notes/decision_transformer/</link><pubDate>Fri, 22 Jul 2022 13:03:00 +0200</pubDate><guid>https://hugocisneros.com/notes/decision_transformer/</guid><description> tags Transformers, GPT, Reinforcement learning paper (Chen et al. 2021) Architecture This is a decoder model that uses a GPT-like model to encode and predict trajectories for Reinforcement learning tasks. It has essentially the same characteristics as GPT.
Bibliography Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch. June 24, 2021. "Decision Transformer: Reinforcement Learning via Sequence Modeling". arXiv. DOI.</description></item><item><title>BLOOM</title><link>https://hugocisneros.com/notes/bloom/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/bloom/</guid><description> tags Transformers, GPT, NLP blog post BLOOM announcement blog post Architecture It is similar to the architecture of GPT-3, using full attention instead of sparse attention.
Parameter count 176B
Bibliography</description></item><item><title>DALL-E</title><link>https://hugocisneros.com/notes/dall_e/</link><pubDate>Fri, 22 Jul 2022 12:53:00 +0200</pubDate><guid>https://hugocisneros.com/notes/dall_e/</guid><description> tags Transformers, GPT paper (Ramesh et al. 2021) Architecture It is a decoder architecture with a Variational autoencoders and a variant of GPT-3 to convert text to images.
Parameter count 12B
Bibliography Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever. February 26, 2021. "Zero-shot Text-to-image Generation". arXiv. DOI.</description></item><item><title>Chinchilla</title><link>https://hugocisneros.com/notes/chinchilla/</link><pubDate>Fri, 22 Jul 2022 12:27:00 +0200</pubDate><guid>https://hugocisneros.com/notes/chinchilla/</guid><description> tags Transformers, GPT, NLP paper (Hoffmann et al. 2022) Architecture This model is very similar to Gopher, with some improvements to make the model smaller and more efficient.
Parameter count 70B
Bibliography Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al.. March 29, 2022. "Training Compute-optimal Large Language Models". arXiv. DOI.</description></item><item><title>Gopher</title><link>https://hugocisneros.com/notes/gopher/</link><pubDate>Fri, 22 Jul 2022 12:24:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gopher/</guid><description> tags Transformers, GPT paper (Rae et al. 2022) Architecture This model is very similar to GPT-2 but uses RSNorm instead of LayerNorm and relative positional encoding rather than absolute positional encoding.
Parameter count 280B
Bibliography Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, et al.. January 21, 2022. "Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher". arXiv. DOI.</description></item></channel></rss>