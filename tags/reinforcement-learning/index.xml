<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement learning on Hugo Cisneros</title><link>https://hugocisneros.com/tags/reinforcement-learning/</link><description>Recent content in Reinforcement learning on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 13 Feb 2023 13:23:00 +0100</lastBuildDate><atom:link href="https://hugocisneros.com/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Adversarial examples in reinforcement learning</title><link>https://hugocisneros.com/projects/adversarial_rl/</link><pubDate>Wed, 30 Jan 2019 15:10:00 +0200</pubDate><guid>https://hugocisneros.com/projects/adversarial_rl/</guid><description>Links Project slides Project report Description The goal of this project was to write a review of adversarial examples in RL as of January 2019. It was also made into a talk for which the slides can be found above.
We begin with a brief review of adversarial examples in Deep learning. These examples are input values specifically crafted to fool a classifier into assigning it the wrong class. For example, an image could be made to look like a cat but optimized so that a object recognition neural network classifies it as a dog.</description></item><item><title>Reinforcement learning with human feedback</title><link>https://hugocisneros.com/notes/reinforcement_learning_with_human_feedback/</link><pubDate>Mon, 13 Feb 2023 13:23:00 +0100</pubDate><guid>https://hugocisneros.com/notes/reinforcement_learning_with_human_feedback/</guid><description> tags Reinforcement learning, NLP</description></item><item><title>Gato</title><link>https://hugocisneros.com/notes/gato/</link><pubDate>Wed, 27 Jul 2022 12:12:00 +0200</pubDate><guid>https://hugocisneros.com/notes/gato/</guid><description> tags Transformers, Reinforcement learning paper (Reed et al. 2022) Architecture A standard decoder-only transformer is preceded by an embedding layer that embeds text and images with positional encoding and spatial information if available.
Parameter count 1.2B
Bibliography Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al.. May 12, 2022. "A Generalist Agent". https://arxiv.org/abs/2205.06175v2.</description></item><item><title>Trajectory transformer</title><link>https://hugocisneros.com/notes/trajectory_transformer/</link><pubDate>Wed, 27 Jul 2022 11:41:00 +0200</pubDate><guid>https://hugocisneros.com/notes/trajectory_transformer/</guid><description> tags Transformers, Reinforcement learning, GPT paper (Janner et al. 2021) Architecture It is a similar model to Decision transformer, with some added techniques to encode a trajectory.
Bibliography Michael Janner, Qiyang Li, Sergey Levine. November 28, 2021. "Offline Reinforcement Learning as One Big Sequence Modeling Problem". arXiv. DOI.</description></item><item><title>Decision transformer</title><link>https://hugocisneros.com/notes/decision_transformer/</link><pubDate>Fri, 22 Jul 2022 13:03:00 +0200</pubDate><guid>https://hugocisneros.com/notes/decision_transformer/</guid><description> tags Transformers, GPT, Reinforcement learning paper (Chen et al. 2021) Architecture This is a decoder model that uses a GPT-like model to encode and predict trajectories for Reinforcement learning tasks. It has essentially the same characteristics as GPT.
Bibliography Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, Igor Mordatch. June 24, 2021. "Decision Transformer: Reinforcement Learning via Sequence Modeling". arXiv. DOI.</description></item><item><title>Time to threshold</title><link>https://hugocisneros.com/notes/time_to_threshold/</link><pubDate>Sun, 24 Apr 2022 13:20:00 +0200</pubDate><guid>https://hugocisneros.com/notes/time_to_threshold/</guid><description>tags Transfer learning, Reinforcement learning This is a simple metric first mentioned in (Taylor et al. 2007; Taylor, Stone 2007). In the paper by Taylor Stone and Liu, it is defined as:
Time-to-Threshold: Measure the time needed to reach a performance threshold in the target task.
In other words, this metric measures the time spent to reach a target performance for a given learning system.
To write down this metric, we use a</description></item><item><title>Intrinsic motivation</title><link>https://hugocisneros.com/notes/intrinsic_motivation/</link><pubDate>Mon, 17 Jan 2022 13:44:00 +0100</pubDate><guid>https://hugocisneros.com/notes/intrinsic_motivation/</guid><description>tags Reinforcement learning, Robotics references :
According to (Ryan, Deci 2000) (pp. 56),
Intrinsic motivation is defined as the doing of an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards.
It is defined by contrast with extrinsic motivation
Extrinsic motivation is a construct that pertains whenever an activity is done in order to attain some separable outcome.</description></item><item><title>Quality diversity</title><link>https://hugocisneros.com/notes/quality_diversity/</link><pubDate>Thu, 25 Mar 2021 09:58:00 +0100</pubDate><guid>https://hugocisneros.com/notes/quality_diversity/</guid><description>tags Evolution, Reinforcement learning, Search algorithms papers (Pugh et al. 2016; Cully, Demiris 2017) QD is about creating algorithms that favor diversity in searching the space. In QD, one needs to both:
Measure the quality of a solution Have a way to describe the effect of a solution Solutions in QD have to be good in the two above ways.
QD is also a form of novelty search.
Bibliography Justin K.</description></item><item><title>Notes on: Curiosity-Driven Exploration by Self-Supervised Prediction by Pathak, D., Agrawal, P., Efros, A. A., &amp; Darrell, T. (2017)</title><link>https://hugocisneros.com/notes/pathakcuriositydrivenexplorationselfsupervised2017/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/pathakcuriositydrivenexplorationselfsupervised2017/</guid><description>tags Reinforcement learning source (Pathak et al. 2017) Summary This paper presents a curiosity-based method for training RL agents. These agents are given a reward \(r_t\) which is the sum of an intrinsic and an extrinsic rewards. The latter is mostly (if not always) 0, while the former is constructed progressively during exploration by an Intrisic Curiosity Module (ICM).
The module is illustrated below (figure from the paper).
The left part of the figure represents a standard RL setup where actions are taken according to a policy and they affect the state of the agent.</description></item><item><title>Notes on: PCGRL: Procedural Content Generation via Reinforcement Learning by Khalifa, A., Bontrager, P., Earle, S., &amp; Togelius, J. (2020)</title><link>https://hugocisneros.com/notes/khalifapcgrlproceduralcontent2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/khalifapcgrlproceduralcontent2020/</guid><description> tags Reinforcement learning source (Khalifa et al. 2020) Summary Comments Bibliography Ahmed Khalifa, Philip Bontrager, Sam Earle, Julian Togelius. January 24, 2020. "PCGRL: Procedural Content Generation via Reinforcement Learning". Arxiv:2001.09212 [cs, Stat]. http://arxiv.org/abs/2001.09212.</description></item><item><title>Notes on: Regenerating Soft Robots through Neural Cellular Automata by Horibe, K., Walker, K., &amp; Risi, S. (2021)</title><link>https://hugocisneros.com/notes/horiberegeneratingsoftrobots2021/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/horiberegeneratingsoftrobots2021/</guid><description> tags Cellular automata, Reinforcement learning source (Horibe et al. 2021) Summary The authors explore neural cellular automata (Mordvintsev et al. 2020) as a framework for growing soft robots.
Comments Bibliography Kazuya Horibe, Kathryn Walker, Sebastian Risi. February 7, 2021. "Regenerating Soft Robots Through Neural Cellular Automata". Arxiv:2102.02579 [cs, Q-bio]. http://arxiv.org/abs/2102.02579. Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin. February 11, 2020. "Growing Neural Cellular Automata". Distill 5 (2):e23. DOI.&amp;nbsp;See notes</description></item><item><title>MAP-Elites</title><link>https://hugocisneros.com/notes/map_elites/</link><pubDate>Mon, 27 Jul 2020 13:24:00 +0200</pubDate><guid>https://hugocisneros.com/notes/map_elites/</guid><description>tags Quality diversity, Reinforcement learning papers (Mouret, Clune 2015; Cully et al. 2015) MAP-Elites are an example of QD algorithm. The behavior space is discretized in cells and during exploration, only the best &amp;ldquo;elite&amp;rdquo; for each cell is kept.
Individuals are added to the grid if they:
fill an empty space are better than an existing elite Bibliography Jean-Baptiste Mouret, Jeff Clune. April 19, 2015. "Illuminating Search Spaces by Mapping Elites"</description></item></channel></rss>