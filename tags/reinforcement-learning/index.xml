<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement learning on Hugo Cisneros</title><link>https://hugocisneros.com/tags/reinforcement-learning/</link><description>Recent content in Reinforcement learning on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 24 Apr 2022 13:20:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Adversarial examples in reinforcement learning</title><link>https://hugocisneros.com/projects/adversarial_rl/</link><pubDate>Wed, 30 Jan 2019 15:10:00 +0200</pubDate><guid>https://hugocisneros.com/projects/adversarial_rl/</guid><description>Links Project slides Project report Description The goal of this project was to write a review of adversarial examples in RL as of January 2019. It was also made into a talk for which the slides can be found above.
This project was a collaboration with Cl√©ment Acher, part of a reinforcement learning course taught by Alessandro Lazaric and Matteo Pirotta.</description></item><item><title>Time to threshold</title><link>https://hugocisneros.com/notes/time_to_threshold/</link><pubDate>Sun, 24 Apr 2022 13:20:00 +0200</pubDate><guid>https://hugocisneros.com/notes/time_to_threshold/</guid><description>tags Transfer learning, Reinforcement learning This is a simple metric first mentioned in (Taylor et al. 2007; Taylor, Stone 2007). In the paper by Taylor Stone and Liu, it is defined as:
Time-to-Threshold: Measure the time needed to reach a performance threshold in the target task.
In other words, this metric measures the time spent to reach a target performance for a given learning system.
To write down this metric, we use a</description></item><item><title>Intrinsic motivation</title><link>https://hugocisneros.com/notes/intrinsic_motivation/</link><pubDate>Mon, 17 Jan 2022 13:44:00 +0100</pubDate><guid>https://hugocisneros.com/notes/intrinsic_motivation/</guid><description>tags Reinforcement learning, Robotics references :
According to (Ryan, Deci 2000) (pp. 56),
Intrinsic motivation is defined as the doing of an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards.
It is defined by contrast with extrinsic motivation
Extrinsic motivation is a construct that pertains whenever an activity is done in order to attain some separable outcome.</description></item><item><title>Quality diversity</title><link>https://hugocisneros.com/notes/quality_diversity/</link><pubDate>Thu, 25 Mar 2021 09:58:00 +0100</pubDate><guid>https://hugocisneros.com/notes/quality_diversity/</guid><description>tags Evolution, Reinforcement learning, Search algorithms papers (Pugh et al. 2016; Cully, Demiris 2017) QD is about creating algorithms that favor diversity in searching the space. In QD, one needs to both:
Measure the quality of a solution Have a way to describe the effect of a solution Solutions in QD have to be good in the two above ways.
QD is also a form of novelty search.
Bibliography Justin K.</description></item><item><title>Notes on: Curiosity-Driven Exploration by Self-Supervised Prediction by Pathak, D., Agrawal, P., Efros, A. A., &amp; Darrell, T. (2017)</title><link>https://hugocisneros.com/notes/pathakcuriositydrivenexplorationselfsupervised2017/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/pathakcuriositydrivenexplorationselfsupervised2017/</guid><description>tags Reinforcement learning source (Pathak et al. 2017) Summary This paper presents a curiosity-based method for training RL agents. These agents are given a reward \(r_t\) which is the sum of an intrinsic and an extrinsic rewards. The latter is mostly (if not always) 0, while the former is constructed progressively during exploration by an Intrisic Curiosity Module (ICM).
The module is illustrated below (figure from the paper).
The left part of the figure represents a standard RL setup where actions are taken according to a policy and they affect the state of the agent.</description></item><item><title>Notes on: PCGRL: Procedural Content Generation via Reinforcement Learning by Khalifa, A., Bontrager, P., Earle, S., &amp; Togelius, J. (2020)</title><link>https://hugocisneros.com/notes/khalifapcgrlproceduralcontent2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/khalifapcgrlproceduralcontent2020/</guid><description> tags Reinforcement learning source (Khalifa et al. 2020) Summary Comments Bibliography Ahmed Khalifa, Philip Bontrager, Sam Earle, Julian Togelius. January 24, 2020. "PCGRL: Procedural Content Generation via Reinforcement Learning". Arxiv:2001.09212 [cs, Stat]. http://arxiv.org/abs/2001.09212.</description></item><item><title>Notes on: Regenerating Soft Robots through Neural Cellular Automata by Horibe, K., Walker, K., &amp; Risi, S. (2021)</title><link>https://hugocisneros.com/notes/horiberegeneratingsoftrobots2021/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/horiberegeneratingsoftrobots2021/</guid><description> tags Cellular automata, Reinforcement learning source (Horibe et al. 2021) Summary The authors explore neural cellular automata (Mordvintsev et al. 2020) as a framework for growing soft robots.
Comments Bibliography Kazuya Horibe, Kathryn Walker, Sebastian Risi. February 7, 2021. "Regenerating Soft Robots Through Neural Cellular Automata". Arxiv:2102.02579 [cs, Q-bio]. http://arxiv.org/abs/2102.02579. Alexander Mordvintsev, Ettore Randazzo, Eyvind Niklasson, Michael Levin. February 11, 2020. "Growing Neural Cellular Automata". Distill 5 (2):e23. DOI.&amp;nbsp;See notes</description></item><item><title>MAP-Elites</title><link>https://hugocisneros.com/notes/map_elites/</link><pubDate>Mon, 27 Jul 2020 13:24:00 +0200</pubDate><guid>https://hugocisneros.com/notes/map_elites/</guid><description>tags Quality diversity, Reinforcement learning papers (Mouret, Clune 2015; Cully et al. 2015) MAP-Elites are an example of QD algorithm. The behavior space is discretized in cells and during exploration, only the best &amp;ldquo;elite&amp;rdquo; for each cell is kept.
Individuals are added to the grid if they:
fill an empty space are better than an existing elite Bibliography Jean-Baptiste Mouret, Jeff Clune. April 19, 2015. "Illuminating Search Spaces by Mapping Elites"</description></item></channel></rss>