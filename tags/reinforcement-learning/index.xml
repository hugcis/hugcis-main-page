<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement learning on Hugo Cisneros</title><link>https://hugocisneros.com/tags/reinforcement-learning/</link><description>Recent content in Reinforcement learning on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 17 Jan 2022 13:44:00 +0100</lastBuildDate><atom:link href="https://hugocisneros.com/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Adversarial examples in reinforcement learning</title><link>https://hugocisneros.com/projects/adversarial_rl/</link><pubDate>Wed, 30 Jan 2019 15:10:00 +0200</pubDate><guid>https://hugocisneros.com/projects/adversarial_rl/</guid><description>Links Project slides Project report Description The goal of this project was to write a review of adversarial examples in RL as of January 2019. It was also made into a talk for which the slides can be found above.
This project was a collaboration with Cl√©ment Acher, part of a reinforcement learning course taught by Alessandro Lazaric and Matteo Pirotta.</description></item><item><title>Intrinsic motivation</title><link>https://hugocisneros.com/notes/intrinsic_motivation/</link><pubDate>Mon, 17 Jan 2022 13:44:00 +0100</pubDate><guid>https://hugocisneros.com/notes/intrinsic_motivation/</guid><description>tags Reinforcement learning, Robotics references :
According to (Ryan and Deci 2000) (pp. 56),
Intrinsic motivation is defined as the doing of an activity for its inherent satisfaction rather than for some separable consequence. When intrinsically motivated, a person is moved to act for the fun or challenge entailed rather than because of external products, pressures, or rewards.
It is defined by contrast with extrinsic motivation</description></item><item><title>Quality diversity</title><link>https://hugocisneros.com/notes/quality_diversity/</link><pubDate>Thu, 25 Mar 2021 09:58:00 +0100</pubDate><guid>https://hugocisneros.com/notes/quality_diversity/</guid><description>tags Evolution, Reinforcement learning, Search algorithms papers (Pugh, Sorosand Stanley 2016; Cully and Demiris 2017) QD is about creating algorithms that favor diversity in searching the space. In QD, one needs to both:
Measure the quality of a solution Have a way to describe the effect of a solution Solutions in QD have to be good in the two above ways.
QD is also a form of novelty search.</description></item><item><title>Notes on: Curiosity-Driven Exploration by Self-Supervised Prediction by Pathak, D., Agrawal, P., Efros, A. A., &amp; Darrell, T. (2017)</title><link>https://hugocisneros.com/notes/pathakcuriositydrivenexplorationselfsupervised2017/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/pathakcuriositydrivenexplorationselfsupervised2017/</guid><description>tags Reinforcement learning source (Pathak et al. 2017) Summary This paper presents a curiosity-based method for training RL agents. These agents are given a reward \(r_t\) which is the sum of an intrinsic and an extrinsic rewards. The latter is mostly (if not always) 0, while the former is constructed progressively during exploration by an Intrisic Curiosity Module (ICM).
The module is illustrated below (figure from the paper).</description></item><item><title>Notes on: PCGRL: Procedural Content Generation via Reinforcement Learning by Khalifa, A., Bontrager, P., Earle, S., &amp; Togelius, J. (2020)</title><link>https://hugocisneros.com/notes/khalifapcgrlproceduralcontent2020/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/khalifapcgrlproceduralcontent2020/</guid><description> tags Reinforcement learning source (Khalifa et al. 2020) Summary Comments Bibliography Khalifa, Ahmed, Philip Bontrager, Sam Earleand Julian Togelius. January 24, 2020. "PCGRL: Procedural Content Generation via Reinforcement Learning". Arxiv:2001.09212 [cs, Stat]. http://arxiv.org/abs/2001.09212.</description></item><item><title>Notes on: Regenerating Soft Robots through Neural Cellular Automata by Horibe, K., Walker, K., &amp; Risi, S. (2021)</title><link>https://hugocisneros.com/notes/horiberegeneratingsoftrobots2021/</link><pubDate>Thu, 25 Mar 2021 09:57:00 +0100</pubDate><guid>https://hugocisneros.com/notes/horiberegeneratingsoftrobots2021/</guid><description>tags Cellular automata, Reinforcement learning source (Horibe, Walkerand Risi 2021) Summary The authors explore neural cellular automata (Mordvintsev et al. 2020) as a framework for growing soft robots.
Comments Bibliography Horibe, Kazuya, Kathryn Walkerand Sebastian Risi. February 7, 2021. "Regenerating Soft Robots Through Neural Cellular Automata". Arxiv:2102.02579 [cs, Q-bio]. http://arxiv.org/abs/2102.02579. Mordvintsev, Alexander, Ettore Randazzo, Eyvind Niklassonand Michael Levin. February 11, 2020. "Growing Neural Cellular Automata". Distill 5 (2):e23. DOI.</description></item><item><title>MAP-Elites</title><link>https://hugocisneros.com/notes/map_elites/</link><pubDate>Mon, 27 Jul 2020 13:24:00 +0200</pubDate><guid>https://hugocisneros.com/notes/map_elites/</guid><description>tags Quality diversity, Reinforcement learning papers (Mouret and Clune 2015; Cully et al. 2015) MAP-Elites are an example of QD algorithm. The behavior space is discretized in cells and during exploration, only the best &amp;ldquo;elite&amp;rdquo; for each cell is kept.
Individuals are added to the grid if they:
fill an empty space are better than an existing elite Bibliography Mouret, Jean-Baptisteand Jeff Clune. April 19, 2015. "</description></item></channel></rss>