<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>RoBERTa on Hugo Cisneros</title><link>https://hugocisneros.com/tags/roberta/</link><description>Recent content in RoBERTa on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 12:04:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/roberta/index.xml" rel="self" type="application/rss+xml"/><item><title>XLM-RoBERTa</title><link>https://hugocisneros.com/notes/xlm_roberta/</link><pubDate>Wed, 27 Jul 2022 12:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/xlm_roberta/</guid><description> tags Transformers, RoBERTa, NLP paper (Conneau et al. 2020) Architecture The model is an extension of RoBERTa that introduces small parameter tuning insights in the context of multilingual applications.
Parameter count Base = 270M Large = 550M Bibliography Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm√°n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. April 7, 2020. "Unsupervised Cross-lingual Representation Learning at Scale". arXiv. DOI.</description></item></channel></rss>