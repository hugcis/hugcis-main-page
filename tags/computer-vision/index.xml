<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer vision on Hugo Cisneros</title><link>https://hugocisneros.com/tags/computer-vision/</link><description>Recent content in Computer vision on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 11:46:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/computer-vision/index.xml" rel="self" type="application/rss+xml"/><item><title>Vision transformer</title><link>https://hugocisneros.com/notes/vision_transformer/</link><pubDate>Wed, 27 Jul 2022 11:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/vision_transformer/</guid><description> tags Transformers, Computer vision, BERT paper (Dosovitskiy et al. 2021) Architecture It is an extension of the BERT architecture that can be trained on patches of images.
Parameter count 86M to 632M
Bibliography Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.. June 3, 2021. "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale". arXiv. DOI.</description></item><item><title>Swin Transformer</title><link>https://hugocisneros.com/notes/swin_transformer/</link><pubDate>Wed, 27 Jul 2022 11:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/swin_transformer/</guid><description> tags Transformers, ViT, Computer vision paper (Liu et al. 2021) Architecture This model extends ViT by replace the multi-head self-attention with a &amp;ldquo;shifted windows&amp;rdquo; module allowing ViT to work with higher resolution images.
Parameter count 29M - 197M
Bibliography Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo. August 17, 2021. "Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows". arXiv. DOI.</description></item><item><title>Imagen</title><link>https://hugocisneros.com/notes/imagen/</link><pubDate>Tue, 26 Jul 2022 11:41:00 +0200</pubDate><guid>https://hugocisneros.com/notes/imagen/</guid><description>tags Transformers, Diffusion models, Computer vision, NLP, T5, CLIP paper (Saharia et al. 2022) Architecture This is based on the U-net diffusion architecture with a few extensions. T5 or CLIP or BERT is used as a frozen text encoder.
Parameter count 2B
Bibliography Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, et al.. May 23, 2022. "Photorealistic Text-to-image Diffusion Models with Deep Language Understanding"</description></item><item><title>Global context ViT</title><link>https://hugocisneros.com/notes/global_context_vit/</link><pubDate>Tue, 26 Jul 2022 11:04:00 +0200</pubDate><guid>https://hugocisneros.com/notes/global_context_vit/</guid><description> tags Transformers, Computer vision, ViT paper (Hatamizadeh et al. 2022) Architecture This is a hierarchical version of ViT with both local and global attention.
Parameter count 90M
Bibliography Ali Hatamizadeh, Hongxu Yin, Jan Kautz, Pavlo Molchanov. June 20, 2022. "Global Context Vision Transformers". arXiv. DOI.</description></item><item><title>GLIDE</title><link>https://hugocisneros.com/notes/glide/</link><pubDate>Tue, 26 Jul 2022 11:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/glide/</guid><description> tags Diffusion models, NLP, Computer vision paper (Nichol et al. 2022) Architecture This model uses joint textual and visual embedding diffusion model followed by some upsampling.
Parameter count 3.5B
Bibliography Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen. March 8, 2022. "GLIDE: Towards Photorealistic Image Generation and Editing with Text-guided Diffusion Models". arXiv. DOI.</description></item><item><title>Flamingo</title><link>https://hugocisneros.com/notes/flamingo/</link><pubDate>Tue, 26 Jul 2022 09:56:00 +0200</pubDate><guid>https://hugocisneros.com/notes/flamingo/</guid><description> tags Transformers, Computer vision, NLP, Chinchilla paper (Alayrac et al. 2022) Architecture Uses a frozen language model (e.g. Chinchilla) that is conditioned on a visual representation given from a normalizer-free ResNet.
Parameter count 80B
Bibliography Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al.. April 29, 2022. "Flamingo: A Visual Language Model for Few-shot Learning". arXiv. http://arxiv.org/abs/2204.14198.</description></item><item><title>CLIP</title><link>https://hugocisneros.com/notes/clip/</link><pubDate>Fri, 22 Jul 2022 12:29:00 +0200</pubDate><guid>https://hugocisneros.com/notes/clip/</guid><description> tags Transformers, NLP, Computer vision paper (Radford et al. 2021) Architecture It is an encoder-only model which combines ViT and ResNet to encode images and a transformer for the text encoding.
Bibliography Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al.. February 26, 2021. "Learning Transferable Visual Models from Natural Language Supervision". arXiv. DOI.</description></item><item><title>Residual neural networks</title><link>https://hugocisneros.com/notes/residual_networks/</link><pubDate>Fri, 22 Jul 2022 12:28:00 +0200</pubDate><guid>https://hugocisneros.com/notes/residual_networks/</guid><description>tags Neural networks, Convolutional neural networks, Computer vision resources (He et al. 2016) Residual neural networks are neural networks with skip-connections (or shortcuts, residual connections) that will bypass some of the networks operations in depth.
Highway networks (Srivastava et al. 2015)
DenseNets (&amp;lt;cite itemprop=&amp;ldquo;citation&amp;rdquo; itemscope=&amp;ldquo;&amp;ldquo;Huang, Liu ,n.d.)
Bibliography Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. June 2016. "Deep Residual Learning for Image Recognition". In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78.</description></item><item><title>Style transfer</title><link>https://hugocisneros.com/notes/style_transfer/</link><pubDate>Wed, 01 Sep 2021 17:30:00 +0200</pubDate><guid>https://hugocisneros.com/notes/style_transfer/</guid><description>tags Computer vision Style transfer is the process of transferring some visual features from one image to another image while preserving the latter&amp;rsquo;s content information. Since both these notions may be considered subjective, the problem of style transfer is not well defined and may be approached in many ways.
Style transfer with CNNs This is an early example of style transfer with convolutional neural networks: (Gatys et al. 2016)</description></item><item><title>Style transfer with generative adversarial neural networks</title><link>https://hugocisneros.com/projects/style_transfer_gans/</link><pubDate>Tue, 15 Jan 2019 15:14:57 +0200</pubDate><guid>https://hugocisneros.com/projects/style_transfer_gans/</guid><description>Links Project slides Description This project&amp;rsquo;s goal was to implement style transfer techniques based on GANs. We applied several algorithms on our own datasets of landscape pictures scraped from Flickr and frames from anime extracted automatically.
This project was a collaboration with Clément Acher and was part of the course Object recognition and computer vision taught by Jean Ponce, Ivan Laptev, Cordelia Schmid and Josef Sivic.</description></item></channel></rss>