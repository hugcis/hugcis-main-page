<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transfer learning on Hugo Cisneros</title><link>https://hugocisneros.com/tags/transfer-learning/</link><description>Recent content in Transfer learning on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 20 Apr 2022 13:54:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/transfer-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Time to threshold</title><link>https://hugocisneros.com/notes/time_to_threshold/</link><pubDate>Wed, 20 Apr 2022 13:54:00 +0200</pubDate><guid>https://hugocisneros.com/notes/time_to_threshold/</guid><description> tags Transfer learning, Reinforcement learning This is a simple metric mentioned in (Taylor et al. 2007). In the paper it is defined as
Time-to-Threshold: Measure the time needed to reach a performance threshold in the target task.
Bibliography Matthew E. Taylor, Peter Stone, Yaxin Liu. 2007. "Transfer Learning via Inter-task Mappings for Temporal Difference Learning". J. Mach. Learn. Res. 8:2125â€“67. http://dl.acm.org/citation.cfm?id=1314569.</description></item><item><title>Few-shot learning</title><link>https://hugocisneros.com/notes/few_shot_learning/</link><pubDate>Tue, 19 Apr 2022 13:31:00 +0200</pubDate><guid>https://hugocisneros.com/notes/few_shot_learning/</guid><description>tags Machine learning, Transfer learning resources AI Multiple post Few-shot learning (FSL) can be considered as a kind of meta-learning problem where the model learns how to learn to solve different problems.
FSL tasks are referred to as N-way K-shots, where N corresponds to the number of examples in each training classes and K is the number of separate training tasks for the model meta-training. A test time, the model will only see N examples of each of the classes it has to learn.</description></item><item><title>Distillation</title><link>https://hugocisneros.com/notes/distillation/</link><pubDate>Mon, 14 Jun 2021 11:54:00 +0200</pubDate><guid>https://hugocisneros.com/notes/distillation/</guid><description>tags Machine learning, Neural networks, Transfer learning Distillation is used to describe the process of transferring performances from a large trained teacher neural network to a untrained student network.
Instead of training the target network to score best according the task&amp;rsquo;s loss function, distillation optimizes for the target network to match the output distribution or neuron activation patterns of the teacher network.
A review: (Beyer et al. 2021).</description></item></channel></rss>