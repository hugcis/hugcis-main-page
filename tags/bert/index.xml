<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>BERT on Hugo Cisneros</title><link>https://hugocisneros.com/tags/bert/</link><description>Recent content in BERT on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Jul 2022 11:46:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/bert/index.xml" rel="self" type="application/rss+xml"/><item><title>Vision transformer</title><link>https://hugocisneros.com/notes/vision_transformer/</link><pubDate>Wed, 27 Jul 2022 11:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/vision_transformer/</guid><description> tags Transformers, Computer vision, BERT paper (Dosovitskiy et al. 2021) Architecture It is an extension of the BERT architecture that can be trained on patches of images.
Parameter count 86M to 632M
Bibliography Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al.. June 3, 2021. "An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale". arXiv. DOI.</description></item><item><title>RoBERTa</title><link>https://hugocisneros.com/notes/roberta/</link><pubDate>Wed, 27 Jul 2022 10:46:00 +0200</pubDate><guid>https://hugocisneros.com/notes/roberta/</guid><description> tags Transformers, BERT, NLP paper (Liu et al. 2019) Architecture This is an extension of BERT with more data and a better optimized training procedure.
Parameter count 356M
Bibliography Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. July 26, 2019. "Roberta: A Robustly Optimized BERT Pretraining Approach". arXiv. http://arxiv.org/abs/1907.11692.</description></item><item><title>Megatron</title><link>https://hugocisneros.com/notes/megatron/</link><pubDate>Tue, 26 Jul 2022 15:18:00 +0200</pubDate><guid>https://hugocisneros.com/notes/megatron/</guid><description> tags Transformers, GPT, BERT, T5 paper (Shoeybi et al. 2020) Architecture The principle of Megatron is to extend existing architectures by using model parallelism. It has a number of parameters that depends on the base model used.
Bibliography Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, Bryan Catanzaro. March 13, 2020. "Megatron-lm: Training Multi-billion Parameter Language Models Using Model Parallelism". arXiv. DOI.</description></item><item><title>ERNIE</title><link>https://hugocisneros.com/notes/ernie/</link><pubDate>Tue, 26 Jul 2022 09:51:00 +0200</pubDate><guid>https://hugocisneros.com/notes/ernie/</guid><description> tags Transformers, BERT, NLP paper (Zhang et al. 2019) Architecture This transformer uses two stacked BERT for encoding: one for the text, one for the entities in a knowledge graph.
Parameter count 114M
Bibliography Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu. June 4, 2019. "ERNIE: Enhanced Language Representation with Informative Entities". arXiv. DOI.</description></item><item><title>DistillBERT</title><link>https://hugocisneros.com/notes/distillbert/</link><pubDate>Tue, 26 Jul 2022 08:43:00 +0200</pubDate><guid>https://hugocisneros.com/notes/distillbert/</guid><description> tags Transformers, BERT, NLP paper (Sanh et al. 2020) Architecture It is a distilled version of BERT that is much more efficient.
Parameter count 66M
Bibliography Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf. February 29, 2020. "Distilbert, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter". arXiv. DOI.</description></item><item><title>ALBERT</title><link>https://hugocisneros.com/notes/albert/</link><pubDate>Fri, 22 Jul 2022 13:02:00 +0200</pubDate><guid>https://hugocisneros.com/notes/albert/</guid><description>tags Transformers, BERT, NLP paper (Lan et al. 2020) Architecture It is an encoder-only architecture. It extends BERT by using parameter-sharing and is more efficient than BERT with the same number of parameters.
Parameter count Base = 12M Large = 18M XLarge = 60M Bibliography Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. February 8, 2020. "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"</description></item></channel></rss>