<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Neural architecture search on Hugo Cisneros</title><link>https://hugocisneros.com/tags/neural-architecture-search/</link><description>Recent content in Neural architecture search on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 03 May 2022 11:27:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/neural-architecture-search/index.xml" rel="self" type="application/rss+xml"/><item><title>NEAT</title><link>https://hugocisneros.com/notes/neat/</link><pubDate>Tue, 03 May 2022 11:27:00 +0200</pubDate><guid>https://hugocisneros.com/notes/neat/</guid><description> tags Neural architecture search, Evolutionary strategies, Neural networks papers (Stanley, Miikkulainen 2002) Bibliography Kenneth O. Stanley, Risto Miikkulainen. June 2002. "Evolving Neural Networks Through Augmenting Topologies". Evolutionary Computation 10 (2):99â€“127. DOI.&amp;nbsp;See notes</description></item><item><title>Notes on: Efficient Neural Architecture Search via Parameter Sharing by Pham, H., Guan, M. Y., Zoph, B., Le, Q. V., &amp; Dean, J. (2018)</title><link>https://hugocisneros.com/notes/phamefficientneuralarchitecture2018/</link><pubDate>Sun, 17 Apr 2022 13:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/phamefficientneuralarchitecture2018/</guid><description>tags Neural architecture search source (Pham et al. 2018) Summary Like other papers, the controller is a RNN that generates each part of the architecture in sequence. The main contribution of this paper is to introduce parameter sharing in child models. For, this, it represents all possible architectures in a single DAG of operations and share weights between same operations. They explain how to design a RNN cell with their model, a convolutional network (and convolutional cell to build a CNN) and how to train.</description></item></channel></rss>