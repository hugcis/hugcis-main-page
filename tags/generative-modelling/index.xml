<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Generative modelling on Hugo Cisneros</title><link>https://hugocisneros.com/tags/generative-modelling/</link><description>Recent content in Generative modelling on Hugo Cisneros</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 19 Oct 2022 15:07:00 +0200</lastBuildDate><atom:link href="https://hugocisneros.com/tags/generative-modelling/index.xml" rel="self" type="application/rss+xml"/><item><title>Diffusion models</title><link>https://hugocisneros.com/notes/diffusion_models/</link><pubDate>Wed, 19 Oct 2022 15:07:00 +0200</pubDate><guid>https://hugocisneros.com/notes/diffusion_models/</guid><description>tags Generative modelling papers (Sohl-Dickstein et al. 2015), (Ho et al. 2020) Principle of diffusion Forward diffusion An image of size \(N\) by \(N\) \(x_0\), which is a vector in \(\mathbb{R}^{N \times N \times c}\) is diffused at each timestep \(t\) to become \(x_t\). The forward diffusion step is defined as follows: \[ q(\boldsymbol{x}_t | \boldsymbol{x}_{t-1}) = \mathcal{N}(\boldsymbol{x}_t; \sqrt{1 - \beta_t} \boldsymbol{x}_{ t - 1 }, \beta_t I) \] The probability of a sequence of images \(x_1, \ldots, x_T\) is then \[ q(\boldsymbol{x}_1, \ldots, \boldsymbol{x}_T | \boldsymbol{x}_0) = \prod_{t=1}^T q(\boldsymbol{x}_t|\boldsymbol{x}_{t -1}) \]</description></item><item><title>Generative adversarial networks</title><link>https://hugocisneros.com/notes/generative_adversarial_networks/</link><pubDate>Wed, 19 Jan 2022 12:14:00 +0100</pubDate><guid>https://hugocisneros.com/notes/generative_adversarial_networks/</guid><description>tags Neural networks, Generative modelling Generative adversarial networks are a type of generative model. It is close in spirit to Variational autoencoders, but has key differences. The main one is the way the model is trained, which uses an adversarial equilibrium between training a generator and training a discriminator.
Are GANs glorified PCA? (Richardson, Weiss 2020) This paper seems to show that image-to-image translation models are ill-posed and imply the image transformation should always be very local.</description></item></channel></rss>